#!/usr/bin/env python3
"""
generate_notebook_v14_fixed.py

A fixed generator that writes `hindi_emoknob_demo_v14.ipynb`. This is a safe replacement
for the corrupted `generate_notebook_v14.py` and includes a safer
`get_xtts_speaker_latent` helper which tries to call speaker encoders with
`l2_norm=False` and falls back to the legacy API.
"""
import json
from pathlib import Path
import nbformat
from nbformat.v4 import new_notebook, new_code_cell, new_markdown_cell

PROJECT_ROOT = Path(r"D:\Downloads\Bengali_EmoKnob")
NOTEBOOK_PATH = PROJECT_ROOT / "hindi_emoknob_demo_v14.ipynb"

def make_cells():
    cells = []

    # Title
    cells.append(new_markdown_cell(
        "# Hindi EmoKnob — Demo (v14)\n\n"
        "Safe, local-model-first notebook. Fixes XTTS wrapper mismatch and adds robust helpers.\n\n"
        "Run cells in order (Environment → XTTS download/load → Indic load → Helpers → GUI)."
    ))

    # Environment
    cell_env = (
        "# Environment & paths (run first)\n"
        "import os, sys, shutil, traceback\n"
        "from pathlib import Path\n"
        "import torch\n\n"
        f"PROJECT_ROOT = Path(r\"{PROJECT_ROOT}\")\n"
        "MODELS_DIR = PROJECT_ROOT / \"models\"\n"
        "XTTS_LOCAL_DIR = MODELS_DIR / \"xtts_v2\"   # local XTTS folder\n"
        "INDIC_LOCAL_DIR = MODELS_DIR / \"ai4bharat_indicwav2vec_hindi\"  # local Indic wav2vec\n\n"
        "for p in [MODELS_DIR, XTTS_LOCAL_DIR, INDIC_LOCAL_DIR]:\n"
        "    p.mkdir(parents=True, exist_ok=True)\n\n"
        "try:\n"
        "    torch.set_num_threads(2)\n"
        "except Exception as e:\n"
        "    print(\"Warning: torch.set_num_threads failed:\", e)\n\n"
        "SR_XTTS = 22050\n"
        "SR_INDIC = 16000\n\n"
        "print('PROJECT_ROOT:', PROJECT_ROOT)\n"
        "print('MODELS_DIR:', MODELS_DIR)\n"
        "print('XTTS_LOCAL_DIR:', XTTS_LOCAL_DIR)\n"
        "print('INDIC_LOCAL_DIR:', INDIC_LOCAL_DIR)\n"
        "print('SR_XTTS:', SR_XTTS, 'SR_INDIC:', SR_INDIC)\n"
    )
    cells.append(new_code_cell(cell_env))

    # Utils
    cell_utils = (
        "# Utilities: ffmpeg conversion + preprocess + unique_path\n"
        "import subprocess\n"
        "import librosa, soundfile as sf\n"
        "from pathlib import Path\n"
        "import numpy as np\n\n"
        "def run_ffmpeg_convert_to_wav(in_path, out_path, sr):\n"
        "    in_path = str(in_path)\n"
        "    out_path = str(out_path)\n"
        "    cmd = [\"ffmpeg\", \"-y\", \"-i\", in_path, \"-ac\", \"1\", \"-ar\", str(sr), \"-vn\",\n"
        "           \"-hide_banner\", \"-loglevel\", \"error\", out_path]\n"
        "    subprocess.run(cmd, check=True)\n\n"
        "def preprocess_audio(in_path, out_wav_path, sr, normalize=True):\n"
        "    in_path = Path(in_path)\n"
        "    out_wav_path = Path(out_wav_path)\n"
        "    out_wav_path.parent.mkdir(parents=True, exist_ok=True)\n"
        "    tmp = out_wav_path.with_suffix('.tmp.wav')\n"
        "    run_ffmpeg_convert_to_wav(in_path, tmp, sr)\n"
        "    y, _ = librosa.load(str(tmp), sr=sr, mono=True)\n"
        "    if normalize:\n"
        "        peak = max(1e-9, max(abs(float(y.max())), abs(float(y.min()))))\n"
        "        y = y / peak * 0.95\n"
        "    sf.write(str(out_wav_path), y.astype(np.float32), sr)\n"
        "    try:\n"
        "        tmp.unlink()\n"
        "    except:\n"
        "        pass\n"
        "    return out_wav_path\n\n"
        "def unique_path(path: Path):\n"
        "    path = Path(path)\n"
        "    if not path.exists():\n"
        "        return path\n"
        "    base = path.stem\n"
        "    suf = path.suffix\n"
        "    parent = path.parent\n"
        "    i = 1\n"
        "    while True:\n"
        "        candidate = parent / f\"{base}_{i}{suf}\"\n"
        "        if not candidate.exists():\n"
        "            return candidate\n"
        "        i += 1\n"
    )
    cells.append(new_code_cell(cell_utils))

    # XTTS load cell
    cell_xtts = (
        "# XTTS local download & loader (local-first)\n"
        "from TTS.api import TTS\n"
        "from huggingface_hub import snapshot_download\n"
        "import traceback\n\n"
        "def ensure_xtts_local(target_dir: Path):\n"
        "    target_dir.mkdir(parents=True, exist_ok=True)\n"
        "    ck = target_dir / 'model.pth'\n"
        "    cfg = target_dir / 'config.json'\n"
        "    if ck.exists() and cfg.exists():\n"
        "        print('XTTS local present:', target_dir)\n"
        "        return True\n"
        "    try:\n"
        "        tmp = snapshot_download(repo_id='coqui/xtts-v2', cache_dir=str(target_dir), repo_type='model', allow_patterns=['*'])\n"
        "        print('snapshot_download result:', tmp)\n"
        "    except Exception as e:\n"
        "        print('snapshot_download failed (this is OK if huggingface auth required). Error:', e)\n"
        "        traceback.print_exc()\n"
        "    ck = target_dir / 'model.pth'\n"
        "    cfg = target_dir / 'config.json'\n"
        "    if ck.exists() and cfg.exists():\n"
        "        return True\n"
        "    return False\n\n"
        "def load_xtts_local_or_remote(gpu=False):\n"
        "    ok = ensure_xtts_local(Path('models') / 'xtts_v2')\n"
        "    try:\n"
        "        if ok:\n"
        "            t = TTS(model_path=str(Path('models') / 'xtts_v2' / 'model.pth'), config_path=str(Path('models') / 'xtts_v2' / 'config.json'), gpu=gpu)\n"
        "            return t\n"
        "    except Exception as e:\n"
        "        traceback.print_exc()\n"
        "    t = TTS(model_name='tts_models/multilingual/multi-dataset/xtts_v2', gpu=gpu)\n"
        "    return t\n\n"
        "XTTS = None\n"
        "try:\n"
        "    XTTS = load_xtts_local_or_remote(gpu=False)\n"
        "except Exception as e:\n"
        "    traceback.print_exc()\n"
    )
    cells.append(new_code_cell(cell_xtts))

    # Indic load cell
    cell_indic = (
        "# Load ai4bharat/indicwav2vec-hindi from local models folder\n"
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n"
        "import torch\n\n"
        "INDIC_LOCAL = Path('models') / 'ai4bharat_indicwav2vec_hindi'\n"
        "if not INDIC_LOCAL.exists():\n"
        "    print('Warning: Indic model folder not found at:', INDIC_LOCAL)\n"
        "else:\n"
        "    processor = Wav2Vec2Processor.from_pretrained(str(INDIC_LOCAL))\n"
        "    indic_enc = Wav2Vec2Model.from_pretrained(str(INDIC_LOCAL))\n"
        "    indic_enc.eval()\n"
    )
    cells.append(new_code_cell(cell_indic))

    # Helpers cell (safe speaker latent extraction)
    cell_helpers = (
        "# Embedding helpers: safe XTTS resolver and embedding extraction\n"
        "import numpy as np\n"
        "import torch\n"
        "import librosa\n"
        "from pathlib import Path\n\n"
        "def resolve_xtts_internal_model(tts_obj):\n"
        "    if tts_obj is None:\n"
        "        raise RuntimeError('Provided tts_obj is None')\n"
        "    if hasattr(tts_obj, 'synthesizer') and hasattr(tts_obj.synthesizer, 'tts_model'):\n"
        "        return tts_obj.synthesizer.tts_model\n"
        "    if hasattr(tts_obj, 'tts_model'):\n"
        "        return tts_obj.tts_model\n"
        "    raise RuntimeError('Could not resolve internal XTTS model.')\n\n"
        "def get_indic_embedding(wav_path, sr_source=SR_XTTS, sr_indic=SR_INDIC):\n"
        "    global processor, indic_enc\n"
        "    if 'processor' not in globals() or 'indic_enc' not in globals():\n"
        "        raise RuntimeError('Indic encoder not loaded. Run the Indic load cell.')\n"
        "    y, sr = librosa.load(str(wav_path), sr=sr_source, mono=True)\n"
        "    if sr != sr_indic:\n"
        "        y = librosa.resample(y, orig_sr=sr, target_sr=sr_indic)\n"
        "    inp = processor(y, sampling_rate=sr_indic, return_tensors='pt', padding=True)\n"
        "    with torch.no_grad():\n"
        "        out = indic_enc(**inp).last_hidden_state\n"
        "    emb = out.mean(dim=1).squeeze().detach().cpu().numpy()\n"
        "    return emb\n\n"
        "def get_xtts_speaker_latent(tts_obj, wav_path, load_sr=SR_XTTS):\n"
        "    model = resolve_xtts_internal_model(tts_obj)\n"
        "    # Try to load audio into a torch tensor (batch, samples)\n"
        "    try:\n"
        "        wav, sr = librosa.load(str(wav_path), sr=load_sr, mono=True)\n"
        "        audio_t = torch.tensor(wav).unsqueeze(0).float()\n"
        "    except Exception:\n"
        "        audio_t = None\n\n"
        "    def _walk_attr(obj, path_list):\n"
        "        cur = obj\n"
        "        for p in path_list:\n"
        "            if hasattr(cur, p):\n"
        "                cur = getattr(cur, p)\n"
        "            else:\n"
        "                return None\n"
        "        return cur\n\n"
        "    encoder_paths = [\n"
        "        ['hifigan_decoder', 'speaker_encoder'],\n"
        "        ['speaker_manager', 'encoder'],\n"
        "        ['speaker_encoder'],\n"
        "        ['synthesizer', 'speaker_encoder'],\n"
        "        ['encoder', 'speaker_encoder'],\n"
        "    ]\n\n"
        "    for p in encoder_paths:\n"
        "        enc = _walk_attr(model, p)\n"
        "        if enc is None:\n"
        "            continue\n"
        "        try:\n"
        "            with torch.no_grad():\n"
        "                out = None\n"
        "                if audio_t is not None:\n"
        "                    try:\n"
        "                        out = enc.forward(audio_t, l2_norm=False)\n"
        "                    except TypeError:\n"
        "                        try:\n"
        "                            out = enc.forward(audio_t)\n"
        "                        except Exception:\n"
        "                            out = None\n"
        "                if out is None:\n"
        "                    try:\n"
        "                        out = enc(str(wav_path))\n"
        "                    except Exception:\n"
        "                        out = None\n"
        "                if out is not None:\n"
        "                    sp = out.squeeze()\n"
        "                    return sp.detach().cpu().numpy() if hasattr(sp, 'detach') else np.array(sp)\n"
        "        except Exception:\n"
        "            continue\n\n"
        "    # Fallback to get_conditioning_latents\n"
        "    try:\n"
        "        res = model.get_conditioning_latents(str(wav_path), load_sr=load_sr)\n"
        "    except TypeError:\n"
        "        res = model.get_conditioning_latents(str(wav_path))\n"
        "    if isinstance(res, (list, tuple)) and len(res) >= 2:\n"
        "        speaker_lat = res[1]\n"
        "    else:\n"
        "        speaker_lat = res\n"
        "    sp = speaker_lat.squeeze()\n"
        "    return sp.detach().cpu().numpy() if hasattr(sp, 'detach') else np.array(sp)\n\n"
        "print('Helpers ready: resolve_xtts_internal_model, get_indic_embedding, get_xtts_speaker_latent')\n"
    )
    cells.append(new_code_cell(cell_helpers))

    # Projection methods
    cell_projections = (
        "# Projection methods: CCA and PLS mapping utilities\n"
        "import numpy as np\n"
        "from sklearn.cross_decomposition import CCA, PLSRegression\n\n"
        "def fit_cca_or_pls(X_indic, Y_xtts, method='cca', n_comp=32):\n"
        "    if method == 'cca':\n"
        "        cca = CCA(n_components=n_comp, max_iter=500)\n"
        "        cca.fit(X_indic, Y_xtts)\n"
        "        return cca\n"
        "    elif method == 'pls':\n"
        "        pls = PLSRegression(n_components=n_comp, max_iter=500)\n"
        "        pls.fit(X_indic, Y_xtts)\n"
        "        return pls\n"
        "    else:\n"
        "        raise ValueError('Unknown mapping method: ' + str(method))\n\n"
        "def map_indic_vector_to_xtts(model, v_indic):\n"
        "    v_indic = np.asarray(v_indic).reshape(1, -1)\n"
        "    if isinstance(model, CCA):\n"
        "        u, v = model.transform(v_indic, np.zeros((1, model.n_components)))\n"
        "        return v.ravel()\n"
        "    elif isinstance(model, PLSRegression):\n"
        "        out = model.predict(v_indic)\n"
        "        return out.ravel()\n"
        "    else:\n"
        "        return v_indic.ravel()\n"
    )
    cells.append(new_code_cell(cell_projections))

    # Emotion vector extraction
    cell_emovec = (
        "# Compute emotion vectors from emotion_samples folder (multi-sample average or single)\n"
        "import numpy as np\n"
        "from pathlib import Path\n\n"
        "EMOTION_SAMPLES_DIR = PROJECT_ROOT / 'data' / 'emotion_samples'\n"
        "OUTPUT_SINGLE_DIR = PROJECT_ROOT / 'data' / 'outputs' / 'emotion_vectors' / 'single'\n"
        "OUTPUT_AVG_DIR = PROJECT_ROOT / 'data' / 'outputs' / 'emotion_vectors' / 'average'\n"
        "for p in [OUTPUT_SINGLE_DIR, OUTPUT_AVG_DIR]:\n"
        "    Path(p).mkdir(parents=True, exist_ok=True)\n\n"
        "def compute_emotion_vector_xtts_multi(emotion_dir, method='cca', n_comp=32, mode='average', sample_id=1,\n"
        "                                      save_single_dir=None, save_avg_dir=None):\n"
        "    emotion_dir = Path(emotion_dir)\n"
        "    sample_dirs = [d for d in sorted(emotion_dir.iterdir()) if d.is_dir()]\n"
        "    if len(sample_dirs) == 0:\n"
        "        raise ValueError('No sample subfolders found in: ' + str(emotion_dir))\n\n"
        "    X = []\n"
        "    Y = []\n"
        "    single_vectors = []\n\n"
        "    for sd in sample_dirs:\n"
        "        wavs = [f for f in sorted(sd.iterdir()) if f.suffix.lower() in ['.wav','.mp3','.m4a','.flac']]\n"
        "        if len(wavs) < 2:\n"
        "            print('Skipping sample (not enough files):', sd)\n"
        "            continue\n"
        "        neutral = wavs[0]; emot = wavs[1]\n\n"
        "        n_clean = sd / (neutral.stem + '_clean.wav')\n"
        "        e_clean = sd / (emot.stem + '_clean.wav')\n"
        "        if not n_clean.exists():\n"
        "            preprocess_audio(neutral, n_clean, sr=SR_XTTS)\n"
        "        if not e_clean.exists():\n"
        "            preprocess_audio(emot, e_clean, sr=SR_XTTS)\n\n"
        "        xi = get_indic_embedding(n_clean, sr_source=SR_XTTS, sr_indic=SR_INDIC)\n"
        "        xe = get_indic_embedding(e_clean, sr_source=SR_XTTS, sr_indic=SR_INDIC)\n"
        "        yi = get_xtts_speaker_latent(XTTS, n_clean, load_sr=SR_XTTS)\n"
        "        ye = get_xtts_speaker_latent(XTTS, e_clean, load_sr=SR_XTTS)\n\n"
        "        X.append(xe - xi)\n"
        "        Y.append(ye - yi)\n"
        "        single_vectors.append((sd.name, xe - xi, ye - yi))\n\n"
        "        if save_single_dir:\n"
        "            Path(save_single_dir).mkdir(parents=True, exist_ok=True)\n"
        "            np.save(Path(save_single_dir) / f\"{emotion_dir.name}_{sd.name}_indic.npy\", xe - xi)\n"
        "            np.save(Path(save_single_dir) / f\"{emotion_dir.name}_{sd.name}_xtts.npy\", ye - yi)\n\n"
        "    if len(X) == 0:\n"
        "        raise ValueError('No matched pairs extracted for emotion: ' + str(emotion_dir))\n\n"
        "    X = np.stack(X)\n"
        "    Y = np.stack(Y)\n\n"
        "    if method == 'xtts_native':\n"
        "        if mode == 'single':\n"
        "            idx = sample_id - 1\n"
        "            return single_vectors[idx][2] / (np.linalg.norm(single_vectors[idx][2]) + 1e-12)\n"
        "        avg = np.mean([v for (_,_,v) in single_vectors], axis=0)\n"
        "        avg = avg / (np.linalg.norm(avg) + 1e-12)\n"
        "        if save_avg_dir:\n"
        "            Path(save_avg_dir).mkdir(parents=True, exist_ok=True)\n"
        "            np.save(Path(save_avg_dir) / f\"{emotion_dir.name}_avg_xtts.npy\", avg)\n"
        "        return avg\n\n"
        "    mapper = fit_cca_or_pls(X, Y, method=method, n_comp=min(n_comp, X.shape[1], Y.shape[1]))\n\n"
        "    if mode == 'single':\n"
        "        idx = sample_id - 1\n"
        "        v_indic = single_vectors[idx][1]\n"
        "    else:\n"
        "        v_indic = np.mean([xi for (_,xi,_) in single_vectors], axis=0)\n\n"
        "    v_indic = v_indic / (np.linalg.norm(v_indic) + 1e-12)\n"
        "    mapped = map_indic_vector_to_xtts(mapper, v_indic)\n"
        "    mapped = mapped / (np.linalg.norm(mapped) + 1e-12)\n\n"
        "    if save_avg_dir:\n"
        "        Path(save_avg_dir).mkdir(parents=True, exist_ok=True)\n"
        "        np.save(Path(save_avg_dir) / f\"{emotion_dir.name}_avg_mapped.npy\", mapped)\n\n"
        "    return mapped\n"
    )
    cells.append(new_code_cell(cell_emovec))

    # Synthesis cell
    cell_synth = (
        "# Apply emotion vector and synthesize via XTTS\n"
        "import numpy as np\n"
        "import torch\n"
        "from pathlib import Path\n"
        "import soundfile as sf\n\n"
        "OUTPUT_GEN_DIR = PROJECT_ROOT / 'data' / 'outputs' / 'generated'\n"
        "OUTPUT_GEN_DIR.mkdir(parents=True, exist_ok=True)\n\n"
        "def apply_emotion_and_synthesize(text, speaker_wav, emotion_vec, alpha=0.1, out_path=None, language='hi', scale_to_speaker=True):\n"
        "    '''Apply emotion vector to speaker embedding and synthesize speech.\n"
        "    \n"
        "    Args:\n"
        "        text: Hindi text to synthesize\n"
        "        speaker_wav: Path to speaker reference audio (or cleaned wav)\n"
        "        emotion_vec: Emotion direction vector (from compute_emotion_vector_xtts_multi)\n"
        "        alpha: Blending intensity (0.0-1.0). Recommended 0.05-0.3 for perceptible emotion changes\n"
        "        out_path: Output wav file path\n"
        "        language: Language code (default: 'hi' for Hindi)\n"
        "        scale_to_speaker: If True, scale emotion_vec by speaker embedding norm to compensate for XTTS normalization\n"
        "    \n"
        "    Returns:\n"
        "        Path to generated audio file\n"
        "    '''\n"
        "    if out_path is None:\n"
        "        out_path = OUTPUT_GEN_DIR / 'test_hindi_emotional.wav'\n"
        "    out_path = unique_path(Path(out_path))\n\n"
        "    # Get speaker latent from reference audio\n"
        "    sp = get_xtts_speaker_latent(XTTS, speaker_wav, load_sr=SR_XTTS)\n"
        "    sp = np.asarray(sp).astype(np.float32)\n"
        "    ev = np.asarray(emotion_vec).astype(np.float32)\n\n"
        "    # Shape mismatch handling\n"
        "    if ev.shape[0] != sp.shape[0]:\n"
        "        if ev.shape[0] > sp.shape[0]:\n"
        "            ev = ev[: sp.shape[0]]\n"
        "        else:\n"
        "            ev = np.pad(ev, (0, sp.shape[0]-ev.shape[0]))\n\n"
        "    # CRITICAL FIX: XTTS returns L2-NORMALIZED speaker embeddings (norm=1.0)\n"
        "    # When we compute ye - yi (difference of two unit vectors), result has norm ~0.6-0.8\n"
        "    # This is too small to have perceptible effect with alpha in [0.01, 0.3]\n"
        "    # Solution: Scale emotion_vec proportionally to compensate\n"
        "    if scale_to_speaker:\n"
        "        sp_norm = np.linalg.norm(sp)\n"
        "        ev_norm = np.linalg.norm(ev)\n"
        "        if sp_norm > 1e-6 and ev_norm > 1e-6:\n"
        "            # Scale emotion to have similar magnitude as typical speaker embedding perturbations\n"
        "            # Speaker norm = 1.0, emotion norm ≈ 0.6-0.8 → scale up by ~1.3-1.6\n"
        "            scaling_factor = sp_norm / (ev_norm + 1e-12)\n"
        "            ev = ev * scaling_factor\n"
        "            print(f'[Emotion Scaling] sp_norm={sp_norm:.4f}, ev_norm={ev_norm:.4f}, scale={scaling_factor:.4f}')\n\n"
        "    # Apply emotion: new_embedding = speaker_embedding + alpha * scaled_emotion_delta\n"
        "    new_sp = sp + alpha * ev\n"
        "    sp_tensor = torch.tensor(new_sp).unsqueeze(0).float()\n\n"
        "    try:\n"
        "        # Get both conditioning latents from original speaker\n"
        "        res = XTTS.synthesizer.tts_model.get_conditioning_latents(str(speaker_wav), load_sr=SR_XTTS)\n"
        "        if isinstance(res, (list, tuple)) and len(res) >= 2:\n"
        "            gpt_cond_latent = res[0]  # gpt_cond_latent\n"
        "            # Use modified speaker embedding with original gpt_cond_latent\n"
        "        else:\n"
        "            raise RuntimeError('Could not extract both conditioning latents')\n"
        "        \n"
        "        # Use low-level inference with both required parameters\n"
        "        with torch.no_grad():\n"
        "            res_wav = XTTS.synthesizer.tts_model.inference(\n"
        "                text=text, \n"
        "                language=language, \n"
        "                gpt_cond_latent=gpt_cond_latent,\n"
        "                speaker_embedding=sp_tensor\n"
        "            )\n"
        "        \n"
        "        if isinstance(res_wav, dict) and 'wav' in res_wav:\n"
        "            wav = res_wav['wav']\n"
        "        else:\n"
        "            wav = res_wav\n"
        "        \n"
        "        # Ensure output is float32 and save\n"
        "        if isinstance(wav, torch.Tensor):\n"
        "            wav = wav.cpu().numpy()\n"
        "        wav = np.asarray(wav, dtype=np.float32)\n"
        "        sf.write(str(out_path), wav, SR_XTTS)\n"
        "        print(f'✓ Synthesis complete -> {out_path}')\n"
        "    except Exception as e:\n"
        "        print(f'Synthesis failed: {e}')\n"
        "        raise RuntimeError(f'Could not synthesize with custom speaker embedding: {str(e)}')\n\n"
        "    return out_path\n"
    )
    cells.append(new_code_cell(cell_synth))

    # GUI cell
    cell_gui = (
        "# Simple GUI (ipywidgets)\n"
        "import ipywidgets as widgets\n"
        "from IPython.display import display, clear_output\n\n"
        "def list_speakers():\n"
        "    sp_dir = PROJECT_ROOT / 'data' / 'speakers'\n"
        "    sp_dir.mkdir(parents=True, exist_ok=True)\n"
        "    items = [p.name for p in sp_dir.iterdir() if p.suffix.lower() in ['.wav','.mp3','.m4a','.flac']]\n"
        "    return items if items else ['(no speakers found)']\n\n"
        "def list_emotions():\n"
        "    ed = PROJECT_ROOT / 'data' / 'emotion_samples'\n"
        "    ed.mkdir(parents=True, exist_ok=True)\n"
        "    return [p.name for p in ed.iterdir() if p.is_dir()] or ['(no emotions found)']\n\n"
        "emotion_dropdown = widgets.Dropdown(options=list_emotions(), description='Emotion:')\n"
        "method_dropdown = widgets.Dropdown(options=['cca','pls','xtts_native'], description='Method:')\n"
        "mode_dropdown = widgets.Dropdown(options=['average','single'], description='Mode:')\n"
        "sample_id_widget = widgets.IntText(value=1, description='Sample id:')\n"
        "alpha_slider = widgets.FloatSlider(value=0.7, min=0.0, max=1.0, step=0.01, description='alpha:')\n"
        "text_in = widgets.Text(value='मैं आज बहुत खुश हूँ।', description='Text:')\n"
        "speaker_dropdown = widgets.Dropdown(options=list_speakers(), description='Speaker:')\n"
        "run_button = widgets.Button(description='Run', button_style='success')\n"
        "log_out = widgets.Output(layout={'border': '1px solid black'})\n\n"
        "display(widgets.VBox([emotion_dropdown, method_dropdown, mode_dropdown, sample_id_widget, alpha_slider, speaker_dropdown, text_in, run_button, log_out]))\n\n"
        "def on_run_clicked(b):\n"
        "    with log_out:\n"
        "        clear_output()\n"
        "        print('[RUN] Starting pipeline...')\n"
        "        emotion = emotion_dropdown.value\n"
        "        method = method_dropdown.value\n"
        "        mode = mode_dropdown.value\n"
        "        sid = int(sample_id_widget.value)\n"
        "        alpha = float(alpha_slider.value)\n"
        "        sp = speaker_dropdown.value\n"
        "        txt = text_in.value\n"
        "        print('Options -> emotion:', emotion, 'method:', method, 'mode:', mode, 'sample:', sid, 'alpha:', alpha, 'speaker:', sp)\n"
        "        try:\n"
        "            emotion_dir = PROJECT_ROOT / 'data' / 'emotion_samples' / emotion\n"
        "            print('Computing emotion vector... (this may take a few seconds)')\n"
        "            vec = compute_emotion_vector_xtts_multi(emotion_dir, method=method, n_comp=32, mode=mode, sample_id=sid,\n"
        "                                                   save_single_dir=PROJECT_ROOT/'data'/'outputs'/'emotion_vectors'/'single'/emotion,\n"
        "                                                   save_avg_dir=PROJECT_ROOT/'data'/'outputs'/'emotion_vectors'/'average')\n"
        "            print('Emotion vector shape:', getattr(vec, 'shape', None))\n"
        "            speaker_wav = PROJECT_ROOT / 'data' / 'speakers' / sp\n"
        "            print('Synthesizing...')\n"
        "            outp = apply_emotion_and_synthesize(txt, speaker_wav, vec, alpha=alpha)\n"
        "            print('Done. Output saved at:', outp)\n"
        "        except Exception as e:\n"
        "            import traceback\n"
        "            traceback.print_exc()\n\n"
        "run_button.on_click(on_run_clicked)\n"
        "print('GUI ready. Select options and press Run.')\n"
    )
    cells.append(new_code_cell(cell_gui))

    # Notes
    cells.append(new_markdown_cell(
        "## Usage notes\n\n"
        "- Run the **Environment** cell first.\n"
        "- Run **XTTS** load cell (cell 3). If XTTS downloads to cache, copy the model folder into `models/xtts_v2` to avoid repeated downloads.\n"
        "- Make sure `ffmpeg` is installed and in PATH.\n"
        "- Populate `data/speakers` and `data/emotion_samples/<emotion>/sample_xx/` with your audio files.\n"
        "- The GUI will list available speakers and emotions automatically."
    ))

    return cells

def write_notebook(path: Path):
    nb = new_notebook()
    nb['cells'] = make_cells()
    nb['metadata'] = {
        "kernelspec": {"display_name": "Python 3 (venv)", "language": "python", "name": "python3"},
        "language_info": {"name": "python"}
    }
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf8") as f:
        nbformat.write(nb, f)
    print("Notebook written to:", path)

if __name__ == "__main__":
    write_notebook(NOTEBOOK_PATH)
    print("Generation complete.")
