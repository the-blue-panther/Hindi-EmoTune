{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ca370a",
   "metadata": {},
   "source": [
    "# Hindi EmoKnob ‚Äî Demo (v14)\n",
    "\n",
    "Safe, local-model-first notebook. Fixes XTTS wrapper mismatch and adds robust helpers.\n",
    "\n",
    "Run cells in order (Environment ‚Üí XTTS download/load ‚Üí Indic load ‚Üí Helpers ‚Üí GUI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b0ff3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: D:\\Downloads\\Bengali_EmoKnob\n",
      "MODELS_DIR: D:\\Downloads\\Bengali_EmoKnob\\models\n",
      "XTTS_LOCAL_DIR: D:\\Downloads\\Bengali_EmoKnob\\models\\xtts_v2\n",
      "INDIC_LOCAL_DIR: D:\\Downloads\\Bengali_EmoKnob\\models\\ai4bharat_indicwav2vec_hindi\n",
      "SR_XTTS: 22050 SR_INDIC: 16000\n"
     ]
    }
   ],
   "source": [
    "# Environment & paths (run first)\n",
    "import os, sys, shutil, traceback\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "PROJECT_ROOT = Path(r\"D:\\Downloads\\Bengali_EmoKnob\")\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "XTTS_LOCAL_DIR = MODELS_DIR / \"xtts_v2\"   # local XTTS folder\n",
    "INDIC_LOCAL_DIR = MODELS_DIR / \"ai4bharat_indicwav2vec_hindi\"  # local Indic wav2vec\n",
    "\n",
    "for p in [MODELS_DIR, XTTS_LOCAL_DIR, INDIC_LOCAL_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Safe thread config: try set_num_threads, avoid set_num_interop_threads after parallel work started\n",
    "try:\n",
    "    torch.set_num_threads(2)\n",
    "except Exception as e:\n",
    "    print(\"Warning: torch.set_num_threads failed:\", e)\n",
    "\n",
    "# Sampling rates\n",
    "SR_XTTS = 22050   # XTTS default sample rate used by get_conditioning_latents\n",
    "SR_INDIC = 16000  # ai4bharat/indicwav2vec-hindi expects 16k\n",
    "\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "print('MODELS_DIR:', MODELS_DIR)\n",
    "print('XTTS_LOCAL_DIR:', XTTS_LOCAL_DIR)\n",
    "print('INDIC_LOCAL_DIR:', INDIC_LOCAL_DIR)\n",
    "print('SR_XTTS:', SR_XTTS, 'SR_INDIC:', SR_INDIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9bb592b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities: ffmpeg-based conversion + normalization + unique path\n",
    "import subprocess\n",
    "import librosa, soundfile as sf\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def run_ffmpeg_convert_to_wav(in_path, out_path, sr):\n",
    "    in_path = str(in_path)\n",
    "    out_path = str(out_path)\n",
    "    cmd = [\"ffmpeg\", \"-y\", \"-i\", in_path, \"-ac\", \"1\", \"-ar\", str(sr), \"-vn\",\n",
    "           \"-hide_banner\", \"-loglevel\", \"error\", out_path]\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"ffmpeg conversion failed for {in_path}: {e}\")\n",
    "\n",
    "def preprocess_audio(in_path, out_wav_path, sr, normalize=True):\n",
    "    '''Convert any audio file to mono WAV with sample-rate 'sr', do simple amplitude normalization.'''\n",
    "    in_path = Path(in_path)\n",
    "    out_wav_path = Path(out_wav_path)\n",
    "    out_wav_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = out_wav_path.with_suffix('.tmp.wav')\n",
    "    run_ffmpeg_convert_to_wav(in_path, tmp, sr)\n",
    "    y, _ = librosa.load(str(tmp), sr=sr, mono=True)\n",
    "    if normalize:\n",
    "        peak = max(1e-9, max(abs(float(y.max())), abs(float(y.min()))))\n",
    "        y = y / peak * 0.95\n",
    "    sf.write(str(out_wav_path), y.astype(np.float32), sr)\n",
    "    try:\n",
    "        tmp.unlink()\n",
    "    except:\n",
    "        pass\n",
    "    return out_wav_path\n",
    "\n",
    "def unique_path(path: Path):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        return path\n",
    "    base = path.stem\n",
    "    suf = path.suffix\n",
    "    parent = path.parent\n",
    "    i = 1\n",
    "    while True:\n",
    "        candidate = parent / f\"{base}_{i}{suf}\"\n",
    "        if not candidate.exists():\n",
    "            return candidate\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "111e93a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTTS local present: models\\xtts_v2\n",
      "Trying to load XTTS from local models/xtts_v2 ...\n",
      " > Using model: xtts\n",
      "Failed to load local XTTS (will try model_name). Error: [WinError 3] The system cannot find the path specified: 'd:/Downloads/Bengali_EmoKnob/models/xtts_v2/model.pth/model.pth'\n",
      "Loading XTTS via model_name (this will download to user cache if not present)...\n",
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      "Failed to load local XTTS (will try model_name). Error: [WinError 3] The system cannot find the path specified: 'd:/Downloads/Bengali_EmoKnob/models/xtts_v2/model.pth/model.pth'\n",
      "Loading XTTS via model_name (this will download to user cache if not present)...\n",
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_7316\\2513200961.py\", line 33, in load_xtts_local_or_remote\n",
      "    t = TTS(model_path=str(Path('models') / 'xtts_v2' / 'model.pth'),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Downloads\\Bengali_EmoKnob\\.venv\\Lib\\site-packages\\TTS\\api.py\", line 81, in __init__\n",
      "    self.load_tts_model_by_path(\n",
      "  File \"d:\\Downloads\\Bengali_EmoKnob\\.venv\\Lib\\site-packages\\TTS\\api.py\", line 203, in load_tts_model_by_path\n",
      "    self.synthesizer = Synthesizer(\n",
      "                       ^^^^^^^^^^^^\n",
      "  File \"d:\\Downloads\\Bengali_EmoKnob\\.venv\\Lib\\site-packages\\TTS\\utils\\synthesizer.py\", line 93, in __init__\n",
      "    self._load_tts(tts_checkpoint, tts_config_path, use_cuda)\n",
      "  File \"d:\\Downloads\\Bengali_EmoKnob\\.venv\\Lib\\site-packages\\TTS\\utils\\synthesizer.py\", line 192, in _load_tts\n",
      "    self.tts_model.load_checkpoint(self.tts_config, tts_checkpoint, eval=True)\n",
      "  File \"d:\\Downloads\\Bengali_EmoKnob\\.venv\\Lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 771, in load_checkpoint\n",
      "    checkpoint = self.get_compatible_checkpoint_state_dict(model_path)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Downloads\\Bengali_EmoKnob\\.venv\\Lib\\site-packages\\TTS\\tts\\models\\xtts.py\", line 714, in get_compatible_checkpoint_state_dict\n",
      "    checkpoint = load_fsspec(model_path, map_location=torch.device(\"cpu\"))[\"model\"]\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Downloads\\Bengali_EmoKnob\\.venv\\Lib\\site-packages\\TTS\\utils\\io.py\", line 46, in load_fsspec\n",
      "    with fsspec.open(\n",
      "  File \"d:\\Downloads\\Bengali_EmoKnob\\.venv\\Lib\\site-packages\\fsspec\\core.py\", line 105, in __enter__\n",
      "    f = self.fs.open(self.path, mode=mode)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Downloads\\Bengali_EmoKnob\\.venv\\Lib\\site-packages\\fsspec\\implementations\\cached.py\", line 475, in <lambda>\n",
      "    return lambda *args, **kw: getattr(type(self), item).__get__(self)(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Downloads\\Bengali_EmoKnob\\.venv\\Lib\\site-packages\\fsspec\\spec.py\", line 1349, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"d:\\Downloads\\Bengali_EmoKnob\\.venv\\Lib\\site-packages\\fsspec\\implementations\\cached.py\", line 475, in <lambda>\n",
      "    return lambda *args, **kw: getattr(type(self), item).__get__(self)(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Downloads\\Bengali_EmoKnob\\.venv\\Lib\\site-packages\\fsspec\\implementations\\cached.py\", line 713, in _open\n",
      "    fn = self._make_local_details(path)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Downloads\\Bengali_EmoKnob\\.venv\\Lib\\site-packages\\fsspec\\implementations\\cached.py\", line 475, in <lambda>\n",
      "    return lambda *args, **kw: getattr(type(self), item).__get__(self)(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Downloads\\Bengali_EmoKnob\\.venv\\Lib\\site-packages\\fsspec\\implementations\\cached.py\", line 629, in _make_local_details\n",
      "    \"uid\": self.fs.ukey(path),\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Downloads\\Bengali_EmoKnob\\.venv\\Lib\\site-packages\\fsspec\\spec.py\", line 1388, in ukey\n",
      "    return sha256(str(self.info(path)).encode()).hexdigest()\n",
      "                      ^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Downloads\\Bengali_EmoKnob\\.venv\\Lib\\site-packages\\fsspec\\implementations\\local.py\", line 101, in info\n",
      "    out = os.stat(path, follow_symlinks=False)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [WinError 3] The system cannot find the path specified: 'd:/Downloads/Bengali_EmoKnob/models/xtts_v2/model.pth/model.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using model: xtts\n",
      "XTTS loaded via model_name.\n",
      "XTTS loaded via model_name.\n"
     ]
    }
   ],
   "source": [
    "# XTTS local download & loader (local-first)\n",
    "from TTS.api import TTS\n",
    "from huggingface_hub import snapshot_download\n",
    "import shutil, os, traceback\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_xtts_local(target_dir: Path):\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ck = target_dir / 'model.pth'\n",
    "    cfg = target_dir / 'config.json'\n",
    "    if ck.exists() and cfg.exists():\n",
    "        print('XTTS local present:', target_dir)\n",
    "        return True\n",
    "    print('Attempting snapshot_download of coqui/xtts-v2 into models folder (best-effort)...')\n",
    "    try:\n",
    "        tmp = snapshot_download(repo_id='coqui/xtts-v2', cache_dir=str(target_dir), repo_type='model', allow_patterns=['*'])\n",
    "        print('snapshot_download result:', tmp)\n",
    "    except Exception as e:\n",
    "        print('snapshot_download failed (this is OK if huggingface auth required). Error:', e)\n",
    "        traceback.print_exc()\n",
    "    ck = target_dir / 'model.pth'\n",
    "    cfg = target_dir / 'config.json'\n",
    "    if ck.exists() and cfg.exists():\n",
    "        return True\n",
    "    print('XTTS not available locally. You can allow TTS to download to cache once, then move folder to models/xtts_v2.')\n",
    "    return False\n",
    "\n",
    "def load_xtts_local_or_remote(gpu=False):\n",
    "    ok = ensure_xtts_local(Path('models') / 'xtts_v2')\n",
    "    try:\n",
    "        if ok:\n",
    "            print('Trying to load XTTS from local models/xtts_v2 ...')\n",
    "            t = TTS(model_path=str(Path('models') / 'xtts_v2' / 'model.pth'),\n",
    "                    config_path=str(Path('models') / 'xtts_v2' / 'config.json'),\n",
    "                    gpu=gpu)\n",
    "            print('Loaded XTTS from local files.')\n",
    "            return t\n",
    "    except Exception as e:\n",
    "        print('Failed to load local XTTS (will try model_name). Error:', e)\n",
    "        traceback.print_exc()\n",
    "    print('Loading XTTS via model_name (this will download to user cache if not present)...')\n",
    "    t = TTS(model_name='tts_models/multilingual/multi-dataset/xtts_v2', gpu=gpu)\n",
    "    print('XTTS loaded via model_name.')\n",
    "    return t\n",
    "\n",
    "# Load XTTS (CPU first)\n",
    "XTTS = None\n",
    "try:\n",
    "    XTTS = load_xtts_local_or_remote(gpu=False)\n",
    "except Exception as e:\n",
    "    print('XTTS load error:', e)\n",
    "    import traceback; traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6cb66798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Indic wav2vec from: models\\ai4bharat_indicwav2vec_hindi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at models\\ai4bharat_indicwav2vec_hindi and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indic encoder loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load ai4bharat/indicwav2vec-hindi from local models folder\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "INDIC_LOCAL = Path('models') / 'ai4bharat_indicwav2vec_hindi'\n",
    "if not INDIC_LOCAL.exists():\n",
    "    print('Warning: Indic model folder not found at:', INDIC_LOCAL)\n",
    "else:\n",
    "    print('Loading Indic wav2vec from:', INDIC_LOCAL)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(str(INDIC_LOCAL))\n",
    "    indic_enc = Wav2Vec2Model.from_pretrained(str(INDIC_LOCAL))\n",
    "    indic_enc.eval()\n",
    "    print('Indic encoder loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "04271c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers ready: resolve_xtts_internal_model, get_indic_embedding, get_xtts_speaker_latent\n"
     ]
    }
   ],
   "source": [
    "# Embedding helpers: safe XTTS resolver and embedding extraction\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_xtts_internal_model(tts_obj):\n",
    "    '''Return the internal XTTS model used by TTS wrapper.'''\n",
    "    if tts_obj is None:\n",
    "        raise RuntimeError('Provided tts_obj is None')\n",
    "    if hasattr(tts_obj, 'synthesizer') and hasattr(tts_obj.synthesizer, 'tts_model'):\n",
    "        return tts_obj.synthesizer.tts_model\n",
    "    if hasattr(tts_obj, 'tts_model'):\n",
    "        return tts_obj.tts_model\n",
    "    raise RuntimeError('Could not resolve internal XTTS model. Ensure you loaded native XTTS-v2 via TTS API.')\n",
    "\n",
    "def get_indic_embedding(wav_path, sr_source=SR_XTTS, sr_indic=SR_INDIC):\n",
    "    '''Load wav, resample to sr_indic if needed, and return 1D numpy embedding (mean of last_hidden_state).'''\n",
    "    global processor, indic_enc\n",
    "    if 'processor' not in globals() or 'indic_enc' not in globals():\n",
    "        raise RuntimeError('Indic encoder not loaded. Run the Indic load cell.')\n",
    "    y, sr = librosa.load(str(wav_path), sr=sr_source, mono=True)\n",
    "    if sr != sr_indic:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=sr_indic)\n",
    "    inp = processor(y, sampling_rate=sr_indic, return_tensors='pt', padding=True)\n",
    "    with torch.no_grad():\n",
    "        out = indic_enc(**inp).last_hidden_state\n",
    "    emb = out.mean(dim=1).squeeze().detach().cpu().numpy()\n",
    "    return emb\n",
    "\n",
    "def get_xtts_speaker_latent(tts_obj, wav_path, load_sr=SR_XTTS):\n",
    "    '''Extract speaker latent from XTTS by calling internal.get_conditioning_latents(audio_path, load_sr=...).\n",
    "    Returns 1D numpy array.'''\n",
    "    model = resolve_xtts_internal_model(tts_obj)\n",
    "    try:\n",
    "        res = model.get_conditioning_latents(str(wav_path), load_sr=load_sr)\n",
    "    except TypeError:\n",
    "        res = model.get_conditioning_latents(str(wav_path))\n",
    "    if isinstance(res, (list, tuple)) and len(res) >= 2:\n",
    "        speaker_lat = res[1]\n",
    "    else:\n",
    "        speaker_lat = res\n",
    "    try:\n",
    "        sp = speaker_lat.squeeze()\n",
    "        return sp.detach().cpu().numpy() if hasattr(sp, 'detach') else np.array(sp)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError('Failed to convert speaker latent to numpy: ' + str(e))\n",
    "\n",
    "print('Helpers ready: resolve_xtts_internal_model, get_indic_embedding, get_xtts_speaker_latent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e12d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection methods: CCA and PLS mapping utilities\n",
    "import numpy as np\n",
    "from sklearn.cross_decomposition import CCA, PLSRegression\n",
    "\n",
    "def fit_cca_or_pls(X_indic, Y_xtts, method='cca', n_comp=32):\n",
    "    if method == 'cca':\n",
    "        cca = CCA(n_components=n_comp, max_iter=500)\n",
    "        cca.fit(X_indic, Y_xtts)\n",
    "        return cca\n",
    "    elif method == 'pls':\n",
    "        pls = PLSRegression(n_components=n_comp, max_iter=500)\n",
    "        pls.fit(X_indic, Y_xtts)\n",
    "        return pls\n",
    "    else:\n",
    "        raise ValueError('Unknown mapping method: ' + str(method))\n",
    "\n",
    "def map_indic_vector_to_xtts(model, v_indic):\n",
    "    v_indic = np.asarray(v_indic).reshape(1, -1)\n",
    "    if isinstance(model, CCA):\n",
    "        u, v = model.transform(v_indic, np.zeros((1, model.n_components)))\n",
    "        return v.ravel()\n",
    "    elif isinstance(model, PLSRegression):\n",
    "        out = model.predict(v_indic)\n",
    "        return out.ravel()\n",
    "    else:\n",
    "        return v_indic.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3afeb58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì compute_emotion_vector_xtts_multi() defined (xtts_native returns UN-NORMALIZED deltas)\n"
     ]
    }
   ],
   "source": [
    "# Compute emotion vectors from emotion_samples folder (multi-sample average or single)\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "EMOTION_SAMPLES_DIR = PROJECT_ROOT / 'data' / 'emotion_samples'\n",
    "OUTPUT_SINGLE_DIR = PROJECT_ROOT / 'data' / 'outputs' / 'emotion_vectors' / 'single'\n",
    "OUTPUT_AVG_DIR = PROJECT_ROOT / 'data' / 'outputs' / 'emotion_vectors' / 'average'\n",
    "for p in [OUTPUT_SINGLE_DIR, OUTPUT_AVG_DIR]:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def compute_emotion_vector_xtts_multi(emotion_dir, method='cca', n_comp=32, mode='average', sample_id=1,\n",
    "                                      save_single_dir=None, save_avg_dir=None):\n",
    "    emotion_dir = Path(emotion_dir)\n",
    "    sample_dirs = [d for d in sorted(emotion_dir.iterdir()) if d.is_dir()]\n",
    "    if len(sample_dirs) == 0:\n",
    "        raise ValueError('No sample subfolders found in: ' + str(emotion_dir))\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    single_vectors = []\n",
    "\n",
    "    for sd in sample_dirs:\n",
    "        emotion_name = emotion_dir.name  # e.g., 'happy', 'sad', 'angry', 'singing'\n",
    "        \n",
    "        # Explicitly look for neutral_clean.wav and {emotion_name}_clean.wav\n",
    "        n_clean = sd / 'neutral_clean.wav'\n",
    "        e_clean = sd / f'{emotion_name}_clean.wav'\n",
    "        \n",
    "        # If both clean files exist, use them directly\n",
    "        if n_clean.exists() and e_clean.exists():\n",
    "            print(f'[{sd.name}] Found existing clean files: {n_clean.name}, {e_clean.name}')\n",
    "        else:\n",
    "            # Clean files don't exist; find and preprocess raw audio files (any format: wav, mp3, m4a, flac)\n",
    "            raw_files = [f for f in sorted(sd.iterdir()) \n",
    "                        if f.suffix.lower() in ['.wav', '.mp3', '.m4a', '.flac']]\n",
    "            \n",
    "            if len(raw_files) < 2:\n",
    "                print(f'[{sd.name}] Skipping (need ‚â•2 raw audio files, found {len(raw_files)})')\n",
    "                continue\n",
    "            \n",
    "            # Match files: look for 'neutral' in stem, and emotion name in stem\n",
    "            neutral_raw = None\n",
    "            emotion_raw = None\n",
    "            \n",
    "            for f in raw_files:\n",
    "                f_lower = f.stem.lower()\n",
    "                if 'neutral' in f_lower:\n",
    "                    neutral_raw = f\n",
    "                elif emotion_name.lower() in f_lower:\n",
    "                    emotion_raw = f\n",
    "            \n",
    "            # Fallback: if explicit matching failed, use first two alphabetically\n",
    "            if neutral_raw is None or emotion_raw is None:\n",
    "                neutral_raw = raw_files[0]\n",
    "                emotion_raw = raw_files[1]\n",
    "                print(f'[{sd.name}] No explicit neutral/emotion match; using: {neutral_raw.name}, {emotion_raw.name}')\n",
    "            \n",
    "            # Preprocess neutral_raw -> neutral_clean.wav (only if not already done)\n",
    "            if not n_clean.exists():\n",
    "                print(f'[{sd.name}] Preprocessing {neutral_raw.name} ‚Üí neutral_clean.wav')\n",
    "                preprocess_audio(neutral_raw, n_clean, sr=SR_XTTS)\n",
    "            else:\n",
    "                print(f'[{sd.name}] Skipping neutral_clean.wav (already exists)')\n",
    "            \n",
    "            # Preprocess emotion_raw -> {emotion_name}_clean.wav (only if not already done)\n",
    "            if not e_clean.exists():\n",
    "                print(f'[{sd.name}] Preprocessing {emotion_raw.name} ‚Üí {emotion_name}_clean.wav')\n",
    "                preprocess_audio(emotion_raw, e_clean, sr=SR_XTTS)\n",
    "            else:\n",
    "                print(f'[{sd.name}] Skipping {emotion_name}_clean.wav (already exists)')\n",
    "\n",
    "        xi = get_indic_embedding(n_clean, sr_source=SR_XTTS, sr_indic=SR_INDIC)\n",
    "        xe = get_indic_embedding(e_clean, sr_source=SR_XTTS, sr_indic=SR_INDIC)\n",
    "        yi = get_xtts_speaker_latent(XTTS, n_clean, load_sr=SR_XTTS)\n",
    "        ye = get_xtts_speaker_latent(XTTS, e_clean, load_sr=SR_XTTS)\n",
    "\n",
    "        X.append(xe - xi)\n",
    "        Y.append(ye - yi)\n",
    "        single_vectors.append((sd.name, xe - xi, ye - yi))\n",
    "\n",
    "        if save_single_dir:\n",
    "            Path(save_single_dir).mkdir(parents=True, exist_ok=True)\n",
    "            np.save(Path(save_single_dir) / f\"{emotion_dir.name}_{sd.name}_indic.npy\", xe - xi)\n",
    "            np.save(Path(save_single_dir) / f\"{emotion_dir.name}_{sd.name}_xtts.npy\", ye - yi)\n",
    "\n",
    "    if len(X) == 0:\n",
    "        raise ValueError('No matched pairs extracted for emotion: ' + str(emotion_dir))\n",
    "\n",
    "    X = np.stack(X)\n",
    "    Y = np.stack(Y)\n",
    "\n",
    "    # **IMPORTANT**: With fewer than ~5 samples, CCA/PLS cannot fit reliably.\n",
    "    # Automatically switch to native XTTS emotion deltas for small sample sizes.\n",
    "    if len(X) < 5 and method != 'xtts_native':\n",
    "        print(f'‚ö†Ô∏è Only {len(X)} samples; CCA/PLS unreliable with <5 samples.')\n",
    "        print(f'   Switching from \"{method}\" ‚Üí \"xtts_native\" for stable results.')\n",
    "        method = 'xtts_native'\n",
    "\n",
    "    if method == 'xtts_native':\n",
    "        if mode == 'single':\n",
    "            idx = sample_id - 1\n",
    "            # Return raw emotion delta WITHOUT normalization (EmoKnob-style)\n",
    "            raw_delta = single_vectors[idx][2]\n",
    "            if save_avg_dir:\n",
    "                Path(save_avg_dir).mkdir(parents=True, exist_ok=True)\n",
    "                np.save(Path(save_avg_dir) / f\"{emotion_dir.name}_single{sample_id:03d}_xtts_raw.npy\", raw_delta)\n",
    "            return raw_delta\n",
    "        \n",
    "        # Average raw emotion deltas (don't normalize)\n",
    "        avg = np.mean([v for (_,_,v) in single_vectors], axis=0)\n",
    "        \n",
    "        if save_avg_dir:\n",
    "            Path(save_avg_dir).mkdir(parents=True, exist_ok=True)\n",
    "            np.save(Path(save_avg_dir) / f\"{emotion_dir.name}_avg_xtts_raw.npy\", avg)\n",
    "        \n",
    "        avg_norm = np.linalg.norm(avg)\n",
    "        print(f\"\\n  ‚úì Emotion delta computed from {len(single_vectors)} samples\")\n",
    "        print(f\"    Shape: {avg.shape}\")\n",
    "        print(f\"    Norm (difference of normalized embeddings): {avg_norm:.6f}\")\n",
    "        print(f\"    ‚ö†Ô∏è NOTE: XTTS returns L2-normalized embeddings (norm=1.0)\")\n",
    "        print(f\"    The emotion vector will be scaled up in apply_emotion_and_synthesize()\")\n",
    "        print(f\"    to compensate for normalized embeddings (scale factor ‚âà {1.0 / (avg_norm + 1e-12):.2f}x)\")\n",
    "        \n",
    "        return avg  # Return un-normalized raw delta\n",
    "\n",
    "    # For CCA/PLS with >=5 samples: dynamically cap n_comp\n",
    "    max_comp = min(X.shape[0], X.shape[1], Y.shape[1])\n",
    "    actual_n_comp = min(n_comp, max_comp)\n",
    "    print(f'Using n_components={actual_n_comp} (requested {n_comp}, max available {max_comp})')\n",
    "    \n",
    "    mapper = fit_cca_or_pls(X, Y, method=method, n_comp=actual_n_comp)\n",
    "\n",
    "    if mode == 'single':\n",
    "        idx = sample_id - 1\n",
    "        v_indic = single_vectors[idx][1]\n",
    "    else:\n",
    "        v_indic = np.mean([xi for (_,xi,_) in single_vectors], axis=0)\n",
    "\n",
    "    v_indic = v_indic / (np.linalg.norm(v_indic) + 1e-12)\n",
    "    mapped = map_indic_vector_to_xtts(mapper, v_indic)\n",
    "    mapped = mapped / (np.linalg.norm(mapped) + 1e-12)\n",
    "\n",
    "    if save_avg_dir:\n",
    "        Path(save_avg_dir).mkdir(parents=True, exist_ok=True)\n",
    "        np.save(Path(save_avg_dir) / f\"{emotion_dir.name}_avg_mapped.npy\", mapped)\n",
    "\n",
    "    return mapped\n",
    "\n",
    "print(\"‚úì compute_emotion_vector_xtts_multi() defined (xtts_native returns UN-NORMALIZED deltas)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f151ed09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì apply_emotion_and_synthesize() ready\n"
     ]
    }
   ],
   "source": [
    "# Apply emotion vector and synthesize via XTTS\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "\n",
    "OUTPUT_GEN_DIR = PROJECT_ROOT / 'data' / 'outputs' / 'generated'\n",
    "OUTPUT_GEN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def apply_emotion_and_synthesize(text, speaker_wav, emotion_vec, alpha=0.1, out_path=None, language='hi', scale_to_speaker=True):\n",
    "    \"\"\"Apply emotion vector to speaker embedding and synthesize speech.\n",
    "    \n",
    "    Args:\n",
    "        text: Hindi text to synthesize\n",
    "        speaker_wav: Path to speaker reference audio (or cleaned wav)\n",
    "        emotion_vec: Emotion direction vector (from compute_emotion_vector_xtts_multi)\n",
    "        alpha: Blending intensity (0.0-1.0). Recommended 0.05-0.3 for perceptible emotion changes\n",
    "        out_path: Output wav file path\n",
    "        language: Language code (default: 'hi' for Hindi)\n",
    "        scale_to_speaker: If True, scale emotion_vec by speaker embedding norm to compensate for XTTS normalization\n",
    "    \n",
    "    Returns:\n",
    "        Path to generated audio file\n",
    "    \"\"\"\n",
    "    if out_path is None:\n",
    "        out_path = OUTPUT_GEN_DIR / 'test_hindi_emotional.wav'\n",
    "    out_path = unique_path(Path(out_path))\n",
    "\n",
    "    # Get speaker latent from reference audio\n",
    "    sp = get_xtts_speaker_latent(XTTS, speaker_wav, load_sr=SR_XTTS)\n",
    "    sp = np.asarray(sp).astype(np.float32)\n",
    "    ev = np.asarray(emotion_vec).astype(np.float32)\n",
    "\n",
    "    # Shape mismatch handling\n",
    "    if ev.shape[0] != sp.shape[0]:\n",
    "        if ev.shape[0] > sp.shape[0]:\n",
    "            ev = ev[: sp.shape[0]]\n",
    "        else:\n",
    "            ev = np.pad(ev, (0, sp.shape[0]-ev.shape[0]))\n",
    "\n",
    "    # CRITICAL FIX: XTTS returns L2-NORMALIZED speaker embeddings (norm=1.0)\n",
    "    # When we compute ye - yi (difference of two unit vectors), result has norm ~0.6-0.8\n",
    "    # This is too small to have perceptible effect with alpha in [0.01, 0.3]\n",
    "    # Solution: Scale emotion_vec proportionally to compensate\n",
    "    if scale_to_speaker:\n",
    "        sp_norm = np.linalg.norm(sp)\n",
    "        ev_norm = np.linalg.norm(ev)\n",
    "        if sp_norm > 1e-6 and ev_norm > 1e-6:\n",
    "            # Scale emotion to have similar magnitude as typical speaker embedding perturbations\n",
    "            # Speaker norm = 1.0, emotion norm ‚âà 0.6-0.8 ‚Üí scale up by ~1.3-1.6\n",
    "            scaling_factor = sp_norm / (ev_norm + 1e-12)\n",
    "            ev = ev * scaling_factor\n",
    "            print(f\"[Emotion Scaling] sp_norm={sp_norm:.4f}, ev_norm={ev_norm:.4f}, scale={scaling_factor:.4f}\")\n",
    "\n",
    "    # Apply emotion: new_embedding = speaker_embedding + alpha * scaled_emotion_delta\n",
    "    new_sp = sp + alpha * ev\n",
    "    \n",
    "    # Create a temporary audio file with the modified embedding\n",
    "    # by monkey-patching get_conditioning_latents temporarily\n",
    "    model = XTTS.synthesizer.tts_model\n",
    "    original_get_cond = model.get_conditioning_latents\n",
    "    \n",
    "    def patched_get_cond(*args, **kwargs):\n",
    "        \"\"\"Return modified speaker embedding with original gpt_cond_latent\"\"\"\n",
    "        res = original_get_cond(*args, **kwargs)\n",
    "        if isinstance(res, (list, tuple)) and len(res) >= 2:\n",
    "            gpt_cond_latent, orig_speaker_emb = res[0], res[1]\n",
    "            # Return original gpt_cond but modified speaker embedding\n",
    "            # Ensure speaker embedding shape is [1, channels, 1] (as returned by get_conditioning_latents)\n",
    "            return (gpt_cond_latent, torch.tensor(new_sp).unsqueeze(0).unsqueeze(-1).float())\n",
    "        return res\n",
    "    \n",
    "    try:\n",
    "        # Temporarily patch the model\n",
    "        model.get_conditioning_latents = patched_get_cond\n",
    "        \n",
    "        # Use standard tts_to_file which will call our patched get_conditioning_latents\n",
    "        XTTS.tts_to_file(text=text, speaker_wav=str(speaker_wav), language=language, file_path=str(out_path))\n",
    "        print(f'‚úì Synthesis complete -> {out_path}')\n",
    "        return out_path\n",
    "    except Exception as e:\n",
    "        print(f'Synthesis failed: {e}')\n",
    "        raise RuntimeError(f'Could not synthesize with custom speaker embedding: {str(e)}')\n",
    "    finally:\n",
    "        # Restore original method\n",
    "        model.get_conditioning_latents = original_get_cond\n",
    "\n",
    "print('‚úì apply_emotion_and_synthesize() ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade7f77",
   "metadata": {},
   "source": [
    "## Samples Batch Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7efc1b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing functions ready: preprocess_all(emotion_names=[...])\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: Batch clean all emotion and speaker samples\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "AUDIO_EXTS = ['.wav', '.mp3', '.m4a', '.flac']\n",
    "\n",
    "def clean_emotion_samples(emotion_dir, sr=SR_XTTS):\n",
    "    \"\"\"Clean all audio files in emotion sample folders.\n",
    "    \n",
    "    For each sample folder under emotion_dir:\n",
    "    - Finds raw audio files (neutral and emotion)\n",
    "    - Creates *_clean.wav versions using preprocess_audio\n",
    "    - Skips if cleaned version already exists\n",
    "    \n",
    "    Returns count of cleaned files.\n",
    "    \"\"\"\n",
    "    emotion_dir = Path(emotion_dir)\n",
    "    emotion_name = emotion_dir.name\n",
    "    sample_dirs = sorted([d for d in emotion_dir.iterdir() if d.is_dir()])\n",
    "    \n",
    "    if not sample_dirs:\n",
    "        print(f'No sample folders found in {emotion_dir}')\n",
    "        return 0\n",
    "    \n",
    "    cleaned_count = 0\n",
    "    print(f'\\nüéµ Cleaning emotion samples for \"{emotion_name}\" ({len(sample_dirs)} samples)')\n",
    "    print('‚îÄ' * 70)\n",
    "    \n",
    "    for sd in sample_dirs:\n",
    "        # Find raw audio files\n",
    "        raw_files = sorted([f for f in sd.iterdir() if f.suffix.lower() in AUDIO_EXTS])\n",
    "        if not raw_files:\n",
    "            print(f'  {sd.name}: ‚ö†Ô∏è No audio files found')\n",
    "            continue\n",
    "        \n",
    "        # Match neutral and emotion files\n",
    "        neutral_raw = None\n",
    "        emotion_raw = None\n",
    "        \n",
    "        for f in raw_files:\n",
    "            f_lower = f.stem.lower()\n",
    "            if 'neutral' in f_lower:\n",
    "                neutral_raw = f\n",
    "            elif emotion_name.lower() in f_lower:\n",
    "                emotion_raw = f\n",
    "        \n",
    "        # Fallback to first two files if no match\n",
    "        if neutral_raw is None or emotion_raw is None:\n",
    "            neutral_raw = raw_files[0]\n",
    "            emotion_raw = raw_files[1] if len(raw_files) > 1 else raw_files[0]\n",
    "        \n",
    "        # Clean neutral file\n",
    "        n_clean = sd / 'neutral_clean.wav'\n",
    "        if not n_clean.exists():\n",
    "            try:\n",
    "                preprocess_audio(neutral_raw, n_clean, sr=sr)\n",
    "                cleaned_count += 1\n",
    "                print(f'  {sd.name}: ‚úì neutral_clean.wav')\n",
    "            except Exception as e:\n",
    "                print(f'  {sd.name}: ‚úó neutral failed - {str(e)[:40]}')\n",
    "        else:\n",
    "            print(f'  {sd.name}: ‚äò neutral_clean.wav (exists)')\n",
    "        \n",
    "        # Clean emotion file\n",
    "        e_clean = sd / f'{emotion_name}_clean.wav'\n",
    "        if not e_clean.exists():\n",
    "            try:\n",
    "                preprocess_audio(emotion_raw, e_clean, sr=sr)\n",
    "                cleaned_count += 1\n",
    "                print(f'  {sd.name}: ‚úì {emotion_name}_clean.wav')\n",
    "            except Exception as e:\n",
    "                print(f'  {sd.name}: ‚úó {emotion_name} failed - {str(e)[:40]}')\n",
    "        else:\n",
    "            print(f'  {sd.name}: ‚äò {emotion_name}_clean.wav (exists)')\n",
    "    \n",
    "    print('‚îÄ' * 70)\n",
    "    print(f'‚úì Emotion cleaning complete: {cleaned_count} new files created\\n')\n",
    "    return cleaned_count\n",
    "\n",
    "\n",
    "def clean_speaker_samples(sr=SR_XTTS):\n",
    "    \"\"\"Clean all speaker audio files in data/speakers.\n",
    "    \n",
    "    For each supported audio file:\n",
    "    - Creates {stem}_clean.wav if it doesn't exist\n",
    "    - Skips if already cleaned\n",
    "    \n",
    "    Returns count of cleaned files.\n",
    "    \"\"\"\n",
    "    sp_dir = PROJECT_ROOT / 'data' / 'speakers'\n",
    "    sp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    raw_files = sorted([f for f in sp_dir.iterdir() if f.suffix.lower() in AUDIO_EXTS and f.is_file()])\n",
    "    \n",
    "    if not raw_files:\n",
    "        print('No speaker files found in data/speakers')\n",
    "        return 0\n",
    "    \n",
    "    cleaned_count = 0\n",
    "    print(f'\\nüé§ Cleaning speaker samples ({len(raw_files)} files)')\n",
    "    print('‚îÄ' * 70)\n",
    "    \n",
    "    for f in raw_files:\n",
    "        # Skip if already a clean file\n",
    "        if f.stem.endswith('_clean'):\n",
    "            print(f'  {f.name}: ‚äò (already clean)')\n",
    "            continue\n",
    "        \n",
    "        clean_name = f.with_name(f.stem + '_clean.wav')\n",
    "        if clean_name.exists():\n",
    "            print(f'  {f.name}: ‚äò {clean_name.name} (exists)')\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            preprocess_audio(f, clean_name, sr=sr)\n",
    "            cleaned_count += 1\n",
    "            print(f'  {f.name}: ‚úì {clean_name.name}')\n",
    "        except Exception as e:\n",
    "            print(f'  {f.name}: ‚úó failed - {str(e)[:40]}')\n",
    "    \n",
    "    print('‚îÄ' * 70)\n",
    "    print(f'‚úì Speaker cleaning complete: {cleaned_count} new files created\\n')\n",
    "    return cleaned_count\n",
    "\n",
    "\n",
    "def preprocess_all(emotion_names=None):\n",
    "    \"\"\"Batch clean all emotion and speaker samples.\n",
    "    \n",
    "    Args:\n",
    "        emotion_names: list of emotion folder names to clean. If None, clean all.\n",
    "    \n",
    "    Returns: dict with cleaning stats.\n",
    "    \"\"\"\n",
    "    emotion_dir = PROJECT_ROOT / 'data' / 'emotion_samples'\n",
    "    emotion_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if emotion_names is None:\n",
    "        emotion_names = [d.name for d in emotion_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    stats = {'emotion': {}, 'speaker': 0}\n",
    "    \n",
    "    print('\\n' + '=' * 70)\n",
    "    print('BATCH PREPROCESSING: EMOTION + SPEAKER SAMPLES')\n",
    "    print('=' * 70)\n",
    "    \n",
    "    for emotion in emotion_names:\n",
    "        ed = emotion_dir / emotion\n",
    "        if ed.exists():\n",
    "            count = clean_emotion_samples(ed)\n",
    "            stats['emotion'][emotion] = count\n",
    "    \n",
    "    stats['speaker'] = clean_speaker_samples()\n",
    "    \n",
    "    print('=' * 70)\n",
    "    print('SUMMARY:')\n",
    "    for emotion, count in stats['emotion'].items():\n",
    "        print(f'  {emotion}: {count} files cleaned')\n",
    "    print(f'  speakers: {stats[\"speaker\"]} files cleaned')\n",
    "    print('=' * 70 + '\\n')\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "print('Preprocessing functions ready: preprocess_all(emotion_names=[...])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe45fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7dadad099846e5b006ded8e9cd1d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<b>Select Emotions to Clean:</b>'), Checkbox(value=True, description‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Preprocessing GUI ready. Use buttons above to clean samples.\n"
     ]
    }
   ],
   "source": [
    "# Interactive Preprocessing GUI\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Get available emotions\n",
    "emotion_dir = PROJECT_ROOT / 'data' / 'emotion_samples'\n",
    "available_emotions = sorted([d.name for d in emotion_dir.iterdir() if d.is_dir()])\n",
    "\n",
    "# Create checkboxes for emotion selection\n",
    "emotion_checkboxes = {\n",
    "    emotion: widgets.Checkbox(value=True, description=emotion, indent=False)\n",
    "    for emotion in available_emotions\n",
    "}\n",
    "\n",
    "# Output area\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_clean_emotions(button):\n",
    "    \"\"\"Clean selected emotion samples.\"\"\"\n",
    "    with output:\n",
    "        clear_output()\n",
    "        selected = [e for e, cb in emotion_checkboxes.items() if cb.value]\n",
    "        if not selected:\n",
    "            print('‚ö†Ô∏è No emotions selected')\n",
    "            return\n",
    "        preprocess_all(emotion_names=selected)\n",
    "\n",
    "def on_clean_speakers(button):\n",
    "    \"\"\"Clean speaker samples.\"\"\"\n",
    "    with output:\n",
    "        clear_output()\n",
    "        clean_speaker_samples()\n",
    "\n",
    "def on_clean_all(button):\n",
    "    \"\"\"Clean all emotion and speaker samples.\"\"\"\n",
    "    with output:\n",
    "        clear_output()\n",
    "        preprocess_all()\n",
    "\n",
    "# Create buttons\n",
    "btn_emotions = widgets.Button(description='üéµ Clean Selected Emotions', button_style='info', tooltip='Clean checked emotion samples')\n",
    "btn_speakers = widgets.Button(description='üé§ Clean Speaker Samples', button_style='warning', tooltip='Clean speaker audio files')\n",
    "btn_all = widgets.Button(description='üîÑ Clean All Samples', button_style='danger', tooltip='Clean all emotions and speakers')\n",
    "\n",
    "# Attach callbacks\n",
    "btn_emotions.on_click(on_clean_emotions)\n",
    "btn_speakers.on_click(on_clean_speakers)\n",
    "btn_all.on_click(on_clean_all)\n",
    "\n",
    "# Layout\n",
    "emotion_box = widgets.VBox(\n",
    "    [widgets.HTML('<b>Select Emotions to Clean:</b>')] + \n",
    "    [emotion_checkboxes[e] for e in available_emotions],\n",
    "    layout=widgets.Layout(border='1px solid #ccc', padding='10px', margin='10px 0')\n",
    ")\n",
    "\n",
    "button_box = widgets.HBox([btn_emotions, btn_speakers, btn_all], layout=widgets.Layout(margin='10px 0'))\n",
    "\n",
    "panel = widgets.VBox([emotion_box, button_box, output])\n",
    "display(panel)\n",
    "\n",
    "print('‚úì Preprocessing GUI ready. Use buttons above to clean samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a6c93326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä CLEANED SAMPLES INVENTORY\n",
      "======================================================================\n",
      "\n",
      "‚úó ANGRY:\n",
      "   Ready for visualization: 0 samples (with both neutral + emotion clean files)\n",
      "\n",
      "‚úì HAPPY:\n",
      "   Ready for visualization: 4 samples (with both neutral + emotion clean files)\n",
      "\n",
      "‚úó SAD:\n",
      "   Ready for visualization: 0 samples (with both neutral + emotion clean files)\n",
      "\n",
      "‚úì SINGING:\n",
      "   Ready for visualization: 1 samples (with both neutral + emotion clean files)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üé§ SPEAKER SAMPLES:\n",
      "   Total files: 3\n",
      "   Cleaned files: 3\n",
      "      ‚úì amir_khan_clean.wav (687.4 KB)\n",
      "      ‚úì character_1_clean.wav (281.4 KB)\n",
      "      ‚úì lata_mangeskar_clean.wav (626.6 KB)\n",
      "\n",
      "======================================================================\n",
      "Summary: 5 emotion samples ready\n",
      "======================================================================\n",
      "\n",
      "Tip: Run \"Clean All Samples\" or \"Clean Selected Emotions\" above to prepare files.\n"
     ]
    }
   ],
   "source": [
    "# Sample Stats: View cleaned files and their properties\n",
    "from pathlib import Path\n",
    "\n",
    "def show_sample_stats():\n",
    "    \"\"\"Display stats about cleaned samples and speakers.\"\"\"\n",
    "    emotion_dir = PROJECT_ROOT / 'data' / 'emotion_samples'\n",
    "    speaker_dir = PROJECT_ROOT / 'data' / 'speakers'\n",
    "    \n",
    "    print('\\n' + '=' * 70)\n",
    "    print('üìä CLEANED SAMPLES INVENTORY')\n",
    "    print('=' * 70)\n",
    "    \n",
    "    # Check emotions\n",
    "    total_emotion_samples = 0\n",
    "    total_emotion_files = 0\n",
    "    for emotion_folder in sorted(emotion_dir.iterdir()):\n",
    "        if not emotion_folder.is_dir():\n",
    "            continue\n",
    "        emotion_name = emotion_folder.name\n",
    "        samples_with_both = 0\n",
    "        samples_with_partial = 0\n",
    "        \n",
    "        for sample_folder in sorted(emotion_folder.iterdir()):\n",
    "            if not sample_folder.is_dir():\n",
    "                continue\n",
    "            n_clean = sample_folder / 'neutral_clean.wav'\n",
    "            e_clean = sample_folder / f'{emotion_name}_clean.wav'\n",
    "            \n",
    "            if n_clean.exists() and e_clean.exists():\n",
    "                samples_with_both += 1\n",
    "                total_emotion_files += 2\n",
    "            elif n_clean.exists() or e_clean.exists():\n",
    "                samples_with_partial += 1\n",
    "        \n",
    "        total_emotion_samples += samples_with_both\n",
    "        \n",
    "        status = '‚úì' if samples_with_both > 0 else '‚úó'\n",
    "        print(f'\\n{status} {emotion_name.upper()}:')\n",
    "        print(f'   Ready for visualization: {samples_with_both} samples (with both neutral + emotion clean files)')\n",
    "        if samples_with_partial > 0:\n",
    "            print(f'   Partial: {samples_with_partial} samples (only one file cleaned)')\n",
    "    \n",
    "    # Check speakers\n",
    "    print(f'\\n{\"‚îÄ\" * 70}')\n",
    "    print('\\nüé§ SPEAKER SAMPLES:')\n",
    "    if speaker_dir.exists():\n",
    "        all_files = list(speaker_dir.glob('*.wav')) + list(speaker_dir.glob('*.mp3')) + list(speaker_dir.glob('*.m4a')) + list(speaker_dir.glob('*.flac'))\n",
    "        clean_files = list(speaker_dir.glob('*_clean.wav'))\n",
    "        if all_files:\n",
    "            print(f'   Total files: {len(all_files)}')\n",
    "            print(f'   Cleaned files: {len(clean_files)}')\n",
    "            for cf in sorted(clean_files):\n",
    "                size_kb = cf.stat().st_size / 1024\n",
    "                print(f'      ‚úì {cf.name} ({size_kb:.1f} KB)')\n",
    "        else:\n",
    "            print('   No speaker files found')\n",
    "    else:\n",
    "        print('   Directory not found')\n",
    "    \n",
    "    print('\\n' + '=' * 70)\n",
    "    print(f'Summary: {total_emotion_samples} emotion samples ready')\n",
    "    print('=' * 70 + '\\n')\n",
    "\n",
    "\n",
    "# Display stats\n",
    "show_sample_stats()\n",
    "print('Tip: Run \"Clean All Samples\" or \"Clean Selected Emotions\" above to prepare files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f02992b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5826d34238ec4095b270686cd2a5c0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Select emotion and click Plot:</b>'), HBox(children=(Dropdown(description='Emoti‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Interactive emotion sample plotter ready\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_emotion_samples_pca(emotion_name):\n",
    "    \"\"\"Plot emotion samples in 2D PCA space showing neutral vs emotion shifts.\n",
    "    \n",
    "    Args:\n",
    "        emotion_name: name of emotion folder (e.g., 'happy', 'sad')\n",
    "    \"\"\"\n",
    "    emotion_dir = PROJECT_ROOT / 'data' / 'emotion_samples' / emotion_name\n",
    "    \n",
    "    if not emotion_dir.exists():\n",
    "        print(f'Emotion folder not found: {emotion_dir}')\n",
    "        return\n",
    "    \n",
    "    neutral_vecs = []\n",
    "    emotion_vecs = []\n",
    "    sample_names = []\n",
    "    \n",
    "    sample_dirs = sorted([d for d in emotion_dir.iterdir() if d.is_dir()])\n",
    "    \n",
    "    print(f'üéµ Loading {emotion_name} samples...')\n",
    "    \n",
    "    for sd in sample_dirs:\n",
    "        n_clean = sd / 'neutral_clean.wav'\n",
    "        e_clean = sd / f'{emotion_name}_clean.wav'\n",
    "        \n",
    "        # Skip if clean files don't exist\n",
    "        if not (n_clean.exists() and e_clean.exists()):\n",
    "            print(f'  ‚ö†Ô∏è {sd.name}: missing clean files, skipping')\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Extract speaker latents\n",
    "            n_emb = get_xtts_speaker_latent(XTTS, n_clean, load_sr=SR_XTTS)\n",
    "            e_emb = get_xtts_speaker_latent(XTTS, e_clean, load_sr=SR_XTTS)\n",
    "            \n",
    "            neutral_vecs.append(n_emb)\n",
    "            emotion_vecs.append(e_emb)\n",
    "            sample_names.append(sd.name)\n",
    "            print(f'  ‚úì {sd.name}')\n",
    "        except Exception as e:\n",
    "            print(f'  ‚úó {sd.name}: {str(e)[:40]}')\n",
    "    \n",
    "    if len(neutral_vecs) == 0:\n",
    "        print(f'No valid samples found in {emotion_dir}')\n",
    "        return\n",
    "    \n",
    "    neutral_vecs = np.array(neutral_vecs)  # shape: (n_samples, 512)\n",
    "    emotion_vecs = np.array(emotion_vecs)  # shape: (n_samples, 512)\n",
    "    \n",
    "    # Compute averages\n",
    "    neutral_avg = neutral_vecs.mean(axis=0)  # (512,)\n",
    "    emotion_avg = emotion_vecs.mean(axis=0)  # (512,)\n",
    "    \n",
    "    # Stack all vectors for PCA\n",
    "    all_vecs = np.vstack([neutral_vecs, emotion_vecs, neutral_avg.reshape(1, -1), emotion_avg.reshape(1, -1)])\n",
    "    \n",
    "    # Apply PCA to 2D\n",
    "    pca = PCA(n_components=2)\n",
    "    vecs_2d = pca.fit_transform(all_vecs)\n",
    "    \n",
    "    # Split back\n",
    "    n_samples = len(neutral_vecs)\n",
    "    neutral_2d = vecs_2d[:n_samples]\n",
    "    emotion_2d = vecs_2d[n_samples:2*n_samples]\n",
    "    neutral_avg_2d = vecs_2d[2*n_samples]\n",
    "    emotion_avg_2d = vecs_2d[2*n_samples + 1]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot individual samples\n",
    "    plt.scatter(neutral_2d[:, 0], neutral_2d[:, 1], c='blue', s=100, alpha=0.6, \n",
    "               label='Neutral samples', edgecolors='darkblue', linewidth=1.5)\n",
    "    plt.scatter(emotion_2d[:, 0], emotion_2d[:, 1], c='red', s=100, alpha=0.6, \n",
    "               label=f'{emotion_name.capitalize()} samples', edgecolors='darkred', linewidth=1.5)\n",
    "    \n",
    "    # Annotate sample names\n",
    "    for i, name in enumerate(sample_names):\n",
    "        plt.annotate(name, (neutral_2d[i, 0], neutral_2d[i, 1]), \n",
    "                    fontsize=8, alpha=0.7, xytext=(5, 5), textcoords='offset points')\n",
    "        plt.annotate(name, (emotion_2d[i, 0], emotion_2d[i, 1]), \n",
    "                    fontsize=8, alpha=0.7, xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    # Plot averages (larger markers)\n",
    "    plt.scatter(neutral_avg_2d[0], neutral_avg_2d[1], c='blue', s=400, marker='X', \n",
    "               edgecolors='darkblue', linewidth=2, label='Neutral avg', zorder=10)\n",
    "    plt.scatter(emotion_avg_2d[0], emotion_avg_2d[1], c='red', s=400, marker='X', \n",
    "               edgecolors='darkred', linewidth=2, label=f'{emotion_name.capitalize()} avg', zorder=10)\n",
    "    \n",
    "    # Draw arrow from neutral to emotion average (emotion shift)\n",
    "    plt.arrow(neutral_avg_2d[0], neutral_avg_2d[1], \n",
    "             emotion_avg_2d[0] - neutral_avg_2d[0], \n",
    "             emotion_avg_2d[1] - neutral_avg_2d[1],\n",
    "             head_width=0.2, head_length=0.15, fc='green', ec='green', alpha=0.7, linewidth=2.5, zorder=5)\n",
    "    \n",
    "    # Labels and formatting\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    cumsum_var = np.cumsum(explained_var)\n",
    "    \n",
    "    plt.xlabel(f'PC1 ({explained_var[0]:.1%})', fontsize=12)\n",
    "    plt.ylabel(f'PC2 ({explained_var[1]:.1%})', fontsize=12)\n",
    "    plt.title(f'Emotion Vectors: {emotion_name.capitalize()}\\nPCA 2D Projection (Cumulative: {cumsum_var[1]:.1%})',\n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11, loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f'\\nüìä PCA Statistics:')\n",
    "    print(f'   PC1 variance: {explained_var[0]:.2%}')\n",
    "    print(f'   PC2 variance: {explained_var[1]:.2%}')\n",
    "    print(f'   Cumulative: {cumsum_var[1]:.2%}')\n",
    "    \n",
    "    print(f'\\nüìà Vector Statistics ({n_samples} samples):')\n",
    "    print(f'   Neutral avg norm: {np.linalg.norm(neutral_avg):.3f}')\n",
    "    print(f'   {emotion_name.capitalize()} avg norm: {np.linalg.norm(emotion_avg):.3f}')\n",
    "    emotion_diff = emotion_avg - neutral_avg\n",
    "    print(f'   Difference norm: {np.linalg.norm(emotion_diff):.3f}')\n",
    "    cosine_sim = np.dot(neutral_avg, emotion_avg) / (np.linalg.norm(neutral_avg) * np.linalg.norm(emotion_avg) + 1e-12)\n",
    "    print(f'   Cosine similarity: {cosine_sim:.3f}')\n",
    "\n",
    "\n",
    "# Interactive dropdown to select emotion and plot\n",
    "emotion_plot_dropdown = widgets.Dropdown(\n",
    "    options=list_emotions(),\n",
    "    description='Emotion:',\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "plot_button = widgets.Button(description='Plot Samples', button_style='info')\n",
    "plot_output = widgets.Output()\n",
    "\n",
    "def on_plot_clicked(b):\n",
    "    with plot_output:\n",
    "        clear_output()\n",
    "        emotion = emotion_plot_dropdown.value\n",
    "        if emotion == '(no emotions found)':\n",
    "            print('No emotions available. Run preprocessing first.')\n",
    "            return\n",
    "        plot_emotion_samples_pca(emotion)\n",
    "\n",
    "plot_button.on_click(on_plot_clicked)\n",
    "\n",
    "plot_panel = widgets.VBox([\n",
    "    widgets.HTML('<b>Select emotion and click Plot:</b>'),\n",
    "    widgets.HBox([emotion_plot_dropdown, plot_button]),\n",
    "    plot_output\n",
    "])\n",
    "\n",
    "display(plot_panel)\n",
    "print('‚úì Interactive emotion sample plotter ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "41ebed6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce1095d39874cc9932638a8266cf195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Speaker:', options=('amir_khan_clean.wav', 'character_1_cl‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive vector plot ready. Choose speaker, emotion and alpha, then click Plot Vectors.\n"
     ]
    }
   ],
   "source": [
    "# Interactive debug plot: speaker vector, emotion vector, and final (speaker + alpha * emotion)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "def _compute_and_plot(speaker_name, emotion_name, alpha, method, mode, sample_id_int):\n",
    "    with plot_out:\n",
    "        clear_output()\n",
    "        try:\n",
    "            # Resolve speaker path and ensure cleaned file\n",
    "            sp_path = PROJECT_ROOT / 'data' / 'speakers' / speaker_name\n",
    "            sp_clean = ensure_speaker_clean(sp_path, sr=SR_XTTS)\n",
    "\n",
    "            # Get speaker latent (numpy 1D)\n",
    "            sp_vec = get_xtts_speaker_latent(XTTS, sp_clean, load_sr=SR_XTTS)\n",
    "            sp_vec = np.asarray(sp_vec).ravel()\n",
    "\n",
    "            # Compute emotion vector (mapped to XTTS space)\n",
    "            emotion_dir = PROJECT_ROOT / 'data' / 'emotion_samples' / emotion_name\n",
    "            mapped = compute_emotion_vector_xtts_multi(emotion_dir, method=method, mode=mode, sample_id=sample_id_int,\n",
    "                                                      save_avg_dir=PROJECT_ROOT/'data'/'outputs'/'emotion_vectors'/'average')\n",
    "            ev_vec = np.asarray(mapped).ravel()\n",
    "\n",
    "            # Final vector\n",
    "            final_vec = sp_vec + float(alpha) * ev_vec\n",
    "\n",
    "            # Prepare 2D projection via PCA for local visualization\n",
    "            all_vecs = np.vstack([sp_vec, ev_vec, final_vec])\n",
    "            pca = PCA(n_components=2)\n",
    "            coords = pca.fit_transform(all_vecs)\n",
    "            sp2, ev2, final2 = coords[0], coords[1], coords[2]\n",
    "\n",
    "            # Plot\n",
    "            plt.figure(figsize=(7, 7))\n",
    "            plt.scatter(sp2[0], sp2[1], c='blue', s=120, label='Speaker (cloned)', edgecolors='k')\n",
    "            plt.scatter(ev2[0], ev2[1], c='red', s=120, label='Emotion vector (mapped)', edgecolors='k')\n",
    "            plt.scatter(final2[0], final2[1], c='green', s=120, label=f'Final (alpha={alpha:.2f})', edgecolors='k')\n",
    "\n",
    "            # Arrows: speaker -> final, origin -> emotion (dashed)\n",
    "            plt.arrow(sp2[0], sp2[1], final2[0]-sp2[0], final2[1]-sp2[1], head_width=0.03, color='green', linewidth=2)\n",
    "            plt.arrow(0, 0, ev2[0], ev2[1], head_width=0.03, color='red', linestyle='--', linewidth=1.5)\n",
    "\n",
    "            plt.legend()\n",
    "            plt.title(f'2D PCA: Speaker, Emotion, Final (alpha={alpha:.2f})')\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.gca().set_aspect('equal', 'box')\n",
    "            plt.show()\n",
    "\n",
    "            # Print numeric diagnostics\n",
    "            sp_norm = np.linalg.norm(sp_vec)\n",
    "            ev_norm = np.linalg.norm(ev_vec)\n",
    "            final_norm = np.linalg.norm(final_vec)\n",
    "            cos_sp_ev = float(np.dot(sp_vec, ev_vec) / (sp_norm * ev_norm + 1e-12))\n",
    "            print('\\nNumeric diagnostics:')\n",
    "            print(f'  Speaker norm: {sp_norm:.4f}')\n",
    "            print(f'  Emotion norm: {ev_norm:.4f}')\n",
    "            print(f'  Final norm   : {final_norm:.4f}')\n",
    "            print(f'  Cosine(sp,ev): {cos_sp_ev:.4f}')\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print('Error computing/plotting vectors:', e)\n",
    "\n",
    "\n",
    "# Widgets\n",
    "speaker_dd = widgets.Dropdown(options=list_speakers(), description='Speaker:')\n",
    "emotion_dd = widgets.Dropdown(options=list_emotions(), description='Emotion:')\n",
    "method_dd = widgets.Dropdown(options=['xtts_native', 'cca', 'pls'], value='xtts_native', description='Method:')\n",
    "mode_dd = widgets.Dropdown(options=['average', 'single'], value='average', description='Mode:')\n",
    "sample_id_w = widgets.IntText(value=1, description='Sample id:')\n",
    "alpha_w = widgets.FloatSlider(value=0.7, min=0.0, max=1.0, step=0.01, description='alpha:')\n",
    "plot_btn = widgets.Button(description='Plot Vectors', button_style='info')\n",
    "plot_out = widgets.Output()\n",
    "\n",
    "\n",
    "def _on_plot_clicked(b):\n",
    "    _compute_and_plot(speaker_dd.value, emotion_dd.value, alpha_w.value, method_dd.value, mode_dd.value, int(sample_id_w.value))\n",
    "\n",
    "plot_btn.on_click(_on_plot_clicked)\n",
    "\n",
    "ui = widgets.VBox([\n",
    "    widgets.HBox([speaker_dd, emotion_dd]),\n",
    "    widgets.HBox([method_dd, mode_dd, sample_id_w]),\n",
    "    alpha_w,\n",
    "    plot_btn,\n",
    "    plot_out\n",
    "])\n",
    "\n",
    "display(ui)\n",
    "print('Interactive vector plot ready. Choose speaker, emotion and alpha, then click Plot Vectors.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd68253",
   "metadata": {},
   "source": [
    "## TTS GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d75bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d1c4ee9b4044ff96f08cd6d6ff6a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Emotion:', options=('angry', 'happy', 'sad', 'singing'), value='angry'), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GUI ready. Select options and press Run.\n"
     ]
    }
   ],
   "source": [
    "emotion_dropdown = widgets.Dropdown(options=list_emotions(), description='Emotion:')\n",
    "method_dropdown = widgets.Dropdown(options=['xtts_native','cca','pls'], value='xtts_native', description='Method:')\n",
    "mode_dropdown = widgets.Dropdown(options=['average','single'], description='Mode:')\n",
    "sample_id_widget = widgets.IntText(value=1, description='Sample id:')\n",
    "# Toggle: whether to scale the emotion delta to speaker embedding norm (recommended ON for XTTS native)\n",
    "scale_checkbox = widgets.Checkbox(value=True, description='Scale emotion to speaker', indent=False)\n",
    "\n",
    "# NOTE: Raw emotion deltas have natural magnitude (~0.6 typical here);\n",
    "# We expose a GUI slider [0..1] and map it to a perceptual internal alpha (0.01-0.5) via `gui_alpha_to_internal`.\n",
    "alpha_slider = widgets.FloatSlider(value=0.1, min=0.0, max=1.0, step=0.01, description='alpha (GUI 0-1):')\n",
    "alpha_hint = widgets.HTML(value='<small>GUI alpha maps to internal alpha ‚âà 0.01‚Äì0.5 (use map function)</small>')\n",
    "text_in = widgets.Text(value='‡§Æ‡•à‡§Ç ‡§Ü‡§ú ‡§¨‡§π‡•Å‡§§ ‡§ñ‡•Å‡§∂ ‡§π‡•Ç‡§Å‡•§ ‡§Ü‡§ú ‡§ï‡§æ ‡§¶‡§ø‡§® ‡§¨‡§π‡•Å‡§§ ‡§∏‡•Å‡§π‡§æ‡§µ‡§®‡§æ ‡§π‡•à‡•§ ‡§™‡§¢‡§º‡§æ‡§à ‡§î‡§∞ ‡§Æ‡•á‡§π‡§®‡§§ ‡§∏‡•á ‡§∏‡§´‡§≤‡§§‡§æ ‡§Æ‡§ø‡§≤‡§§‡•Ä ‡§π‡•à‡•§ ‡§µ‡§æ‡§π! ‡§Ø‡§π ‡§§‡•ã ‡§ï‡§Æ‡§æ‡§≤ ‡§π‡•ã ‡§ó‡§Ø‡§æ‡•§', description='Text:')\n",
    "\n",
    "# Expose a convenience function to compute internal alpha and run synth\n",
    "run_button = widgets.Button(description='Synthesize', button_style='success')\n",
    "run_output = widgets.Output()\n",
    "\n",
    "def on_run_clicked(b):\n",
    "    with run_output:\n",
    "        run_output.clear_output()\n",
    "        if len(list_emotions())==0:\n",
    "            print('No emotions found. Run preprocessing first.')\n",
    "            return\n",
    "        emotion = emotion_dropdown.value\n",
    "        method = method_dropdown.value\n",
    "        mode = mode_dropdown.value\n",
    "        sid = int(sample_id_widget.value)\n",
    "        gui_alpha = float(alpha_slider.value)\n",
    "        internal_alpha = gui_alpha_to_internal(gui_alpha)\n",
    "        scale_flag = bool(scale_checkbox.value)\n",
    "        print(f'Running synth: emotion={emotion}, method={method}, mode={mode}, sample_id={sid}, gui_alpha={gui_alpha:.3f} -> internal_alpha={internal_alpha:.3f}, scale_to_speaker={scale_flag}')\n",
    "        try:\n",
    "            ed = compute_emotion_vector_xtts_multi(PROJECT_ROOT/'data'/'emotion_samples'/emotion, method=method, mode=mode, sample_id=sid,\n",
    "                                                   save_avg_dir=PROJECT_ROOT/'data'/'outputs'/'emotion_vectors'/'average')\n",
    "            # ensure speaker selection exists\n",
    "            speakers = [p for p in (PROJECT_ROOT / 'data' / 'speakers').glob('*') if p.is_file() and p.suffix.lower() in ['.wav', '.mp3']]\n",
    "            if not speakers:\n",
    "                print('No speaker files found in data/speakers/')\n",
    "                return\n",
    "            speaker_clean = ensure_speaker_clean(speakers[0], sr=SR_XTTS)\n",
    "            out = apply_emotion_and_synthesize(text_in.value, speaker_clean, ed, alpha=internal_alpha,\n",
    "                                              out_path=PROJECT_ROOT/'data'/'outputs'/'generated'/f'gui_synth_{emotion}_{gui_alpha:.2f}.wav',\n",
    "                                              language='hi', scale_to_speaker=scale_flag)\n",
    "            print('Output:', out)\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print('Synthesis failed:', e)\n",
    "\n",
    "run_button.on_click(on_run_clicked)\n",
    "\n",
    "ui_controls = widgets.VBox([\n",
    "    widgets.HBox([emotion_dropdown, method_dropdown]),\n",
    "    widgets.HBox([mode_dropdown, sample_id_widget, scale_checkbox]),\n",
    "    widgets.HBox([alpha_slider, alpha_hint]),\n",
    "    text_in,\n",
    "    run_button,\n",
    "    run_output\n",
    "])\n",
    "\n",
    "display(ui_controls)\n",
    "print('TTS GUI ready: choose emotion, adjust GUI alpha, and press Synthesize.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "457f0190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTTS inference() parameters:\n",
      "  - text\n",
      "  - language\n",
      "  - gpt_cond_latent\n",
      "  - speaker_embedding\n",
      "  - temperature\n",
      "  - length_penalty\n",
      "  - repetition_penalty\n",
      "  - top_k\n",
      "  - top_p\n",
      "  - do_sample\n",
      "  - num_beams\n",
      "  - speed\n",
      "  - enable_text_splitting\n",
      "  - hf_generate_kwargs\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check what parameters XTTS.inference() actually accepts\n",
    "import inspect\n",
    "\n",
    "model = resolve_xtts_internal_model(XTTS)\n",
    "sig = inspect.signature(model.inference)\n",
    "print(\"XTTS inference() parameters:\")\n",
    "for param_name in sig.parameters.keys():\n",
    "    print(f\"  - {param_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60c8f92",
   "metadata": {},
   "source": [
    "## Workflow: Decoupled Preprocessing\n",
    "\n",
    "The notebook now supports a **modular workflow** where audio preprocessing is independent from TTS synthesis:\n",
    "\n",
    "### Step-by-Step Process\n",
    "\n",
    "1. **Preprocess Audio Samples** (PREPROCESSING cell)\n",
    "   - Run `üéµ Clean Selected Emotions` to batch-clean emotion samples\n",
    "   - Run `üé§ Clean Speaker Samples` to clean speaker reference audio\n",
    "   - Creates `neutral_clean.wav` and `{emotion}_clean.wav` for each sample folder\n",
    "   - Creates `{speaker}_clean.wav` in `data/speakers`\n",
    "\n",
    "2. **View Sample Stats** (SAMPLE STATS cell)\n",
    "   - Shows inventory of cleaned files per emotion\n",
    "   - Displays ready-to-visualize samples (those with both neutral + emotion clean files)\n",
    "   - Use this to verify preprocessing was successful\n",
    "\n",
    "3. **Visualize Emotion Vectors** (VISUALIZATION DEMO cell)\n",
    "   - Run after preprocessing to visualize emotion shifts in 2D or 3D\n",
    "   - Uses PCA or t-SNE for dimensionality reduction\n",
    "   - Shows neutral samples (blue) ‚Üí emotion samples (red) with shift arrow\n",
    "   - Prints variance statistics and vector properties\n",
    "\n",
    "4. **Synthesize with Emotion** (TTS GUI cell)\n",
    "   - Select emotion, method, speaker, and text\n",
    "   - Press `Run` to generate emotional speech\n",
    "   - GUI will warn if clean files missing (can auto-clean as fallback)\n",
    "   - Output saved to `data/outputs/generated/`\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "‚úÖ **Faster Iteration**: Preprocess once, use many times\n",
    "‚úÖ **Visibility**: See cleaned files and stats before synthesis\n",
    "‚úÖ **Modularity**: Skip steps as needed (e.g., visualize without synthesizing)\n",
    "‚úÖ **Fallback Safety**: Synthesis still works if clean files missing (slightly slower)\n",
    "\n",
    "### File Structure After Preprocessing\n",
    "\n",
    "```\n",
    "data/\n",
    "‚îú‚îÄ‚îÄ emotion_samples/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ happy/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ sample001/\n",
    "‚îÇ           ‚îú‚îÄ‚îÄ neutral_clean.wav          (‚úì Created by preprocessing)\n",
    "‚îÇ           ‚îî‚îÄ‚îÄ happy_clean.wav            (‚úì Created by preprocessing)\n",
    "‚îú‚îÄ‚îÄ speakers/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ lata_mangeskar_clean.wav           (‚úì Created by preprocessing)\n",
    "‚îî‚îÄ‚îÄ outputs/\n",
    "    ‚îî‚îÄ‚îÄ emotion_vectors/\n",
    "        ‚îú‚îÄ‚îÄ average/\n",
    "        ‚îÇ   ‚îú‚îÄ‚îÄ happy_avg_xtts.npy\n",
    "        ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "        ‚îî‚îÄ‚îÄ single/\n",
    "            ‚îî‚îÄ‚îÄ happy/\n",
    "                ‚îî‚îÄ‚îÄ happy_sample001_xtts.npy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "736ac7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_emotion_vectors(emotion_dir, method='pca', dimension=2, figsize=(10, 8)):\n",
    "    \"\"\"Visualize neutral vs emotion vectors in 2D or 3D (PCA or t-SNE).\n",
    "    \n",
    "    ‚ö†Ô∏è  REQUIRES CLEANED FILES: This cell assumes neutral_clean.wav and {emotion}_clean.wav\n",
    "        files have been created. Run the preprocessing cell above to create them.\n",
    "    \n",
    "    Args:\n",
    "        emotion_dir: path to emotion folder (e.g., 'data/emotion_samples/happy')\n",
    "        method: 'pca' or 'tsne'\n",
    "        dimension: 2 or 3 (for 3D use 'pca' only)\n",
    "        figsize: matplotlib figure size\n",
    "    \"\"\"\n",
    "    emotion_dir = Path(emotion_dir)\n",
    "    emotion_name = emotion_dir.name\n",
    "    \n",
    "    neutral_vecs = []\n",
    "    emotion_vecs = []\n",
    "    sample_dirs = [d for d in sorted(emotion_dir.iterdir()) if d.is_dir()]\n",
    "    valid_count = 0\n",
    "    \n",
    "    print(f'\\nüéµ Visualizing emotion vectors for \"{emotion_name}\"...')\n",
    "    print(f'   Scanning {len(sample_dirs)} sample folders...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b090c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347712f265554b44b42b3bd56e04113a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Select emotion and click Plot:</b>'), HBox(children=(Dropdown(description='Emoti‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Interactive emotion sample plotter ready\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "519e5fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Testing emotion blending concept\n",
      "======================================================================\n",
      "\n",
      "‚ö†Ô∏è NOTE: Direct speaker_embedding injection has shape mismatch issues.\n",
      "Instead, we'll verify the concept is sound by showing:\n",
      "  1. Emotion vector is properly extracted (non-unit norm)\n",
      "  2. Scaling factor is applied correctly\n",
      "  3. Audio files can be generated with reference speaker\n",
      "\n",
      "Using speaker: amir_khan_clean.wav\n",
      "[sample001] Found existing clean files: neutral_clean.wav, happy_clean.wav\n",
      "[sample002] Found existing clean files: neutral_clean.wav, happy_clean.wav\n",
      "[sample002] Found existing clean files: neutral_clean.wav, happy_clean.wav\n",
      "[sample003] Found existing clean files: neutral_clean.wav, happy_clean.wav\n",
      "[sample003] Found existing clean files: neutral_clean.wav, happy_clean.wav\n",
      "[sample004] Found existing clean files: neutral_clean.wav, happy_clean.wav\n",
      "[sample004] Found existing clean files: neutral_clean.wav, happy_clean.wav\n",
      "\n",
      "  ‚úì Emotion delta computed from 4 samples\n",
      "    Shape: (512,)\n",
      "    Norm (difference of normalized embeddings): 0.641648\n",
      "    ‚ö†Ô∏è NOTE: XTTS returns L2-normalized embeddings (norm=1.0)\n",
      "    The emotion vector will be scaled up in apply_emotion_and_synthesize()\n",
      "    to compensate for normalized embeddings (scale factor ‚âà 1.56x)\n",
      "\n",
      "‚úì Emotion vector computed\n",
      "  Shape: (512,)\n",
      "  Norm (raw difference of L2-normalized embeddings): 0.641648\n",
      "\n",
      "  ‚úì Emotion delta computed from 4 samples\n",
      "    Shape: (512,)\n",
      "    Norm (difference of normalized embeddings): 0.641648\n",
      "    ‚ö†Ô∏è NOTE: XTTS returns L2-normalized embeddings (norm=1.0)\n",
      "    The emotion vector will be scaled up in apply_emotion_and_synthesize()\n",
      "    to compensate for normalized embeddings (scale factor ‚âà 1.56x)\n",
      "\n",
      "‚úì Emotion vector computed\n",
      "  Shape: (512,)\n",
      "  Norm (raw difference of L2-normalized embeddings): 0.641648\n",
      "\n",
      "üìä Scaling verification:\n",
      "  Speaker embedding norm: 1.000000\n",
      "  Emotion vector norm: 0.641648\n",
      "  Scaling factor (sp_norm / ev_norm): 1.5585x\n",
      "\n",
      "  After scaling, emotion vector would have norm: 1.000000\n",
      "  This matches speaker embedding norm (1.000000), allowing perceptible alpha changes\n",
      "\n",
      "üé§ Synthesizing reference audio (no emotion blending)...\n",
      " > Text splitted to sentences.\n",
      "['‡§∏‡•Ç‡§∞‡§ú ‡§ï‡•Ä ‡§∞‡•ã‡§∂‡§®‡•Ä ‡§ö‡§Æ‡§ï ‡§∞‡§π‡•Ä ‡§π‡•à, ‡§¨‡§ö‡•ç‡§ö‡•ã‡§Ç ‡§ï‡•Ä ‡§π‡§Å‡§∏‡•Ä ‡§ó‡•Ç‡§Å‡§ú ‡§∞‡§π‡•Ä ‡§π‡•à, ‡§î‡§∞ ‡§∏‡§¨‡§ï‡•á ‡§ö‡•á‡§π‡§∞‡•á ‡§™‡§∞ ‡§Æ‡•Å‡§∏‡•ç‡§ï‡§æ‡§® ‡§ñ‡§ø‡§≤‡•Ä ‡§π‡•Å‡§à ‡§π‡•à‡•§']\n",
      "\n",
      "üìä Scaling verification:\n",
      "  Speaker embedding norm: 1.000000\n",
      "  Emotion vector norm: 0.641648\n",
      "  Scaling factor (sp_norm / ev_norm): 1.5585x\n",
      "\n",
      "  After scaling, emotion vector would have norm: 1.000000\n",
      "  This matches speaker embedding norm (1.000000), allowing perceptible alpha changes\n",
      "\n",
      "üé§ Synthesizing reference audio (no emotion blending)...\n",
      " > Text splitted to sentences.\n",
      "['‡§∏‡•Ç‡§∞‡§ú ‡§ï‡•Ä ‡§∞‡•ã‡§∂‡§®‡•Ä ‡§ö‡§Æ‡§ï ‡§∞‡§π‡•Ä ‡§π‡•à, ‡§¨‡§ö‡•ç‡§ö‡•ã‡§Ç ‡§ï‡•Ä ‡§π‡§Å‡§∏‡•Ä ‡§ó‡•Ç‡§Å‡§ú ‡§∞‡§π‡•Ä ‡§π‡•à, ‡§î‡§∞ ‡§∏‡§¨‡§ï‡•á ‡§ö‡•á‡§π‡§∞‡•á ‡§™‡§∞ ‡§Æ‡•Å‡§∏‡•ç‡§ï‡§æ‡§® ‡§ñ‡§ø‡§≤‡•Ä ‡§π‡•Å‡§à ‡§π‡•à‡•§']\n",
      " > Processing time: 51.55499577522278\n",
      " > Real-time factor: 6.587781970582188\n",
      "  ‚úì Reference audio saved\n",
      "\n",
      "‚úÖ CONCEPT VERIFICATION COMPLETE\n",
      "\n",
      "üìã Summary:\n",
      "  ‚úì Emotion deltas extracted as non-unit-norm vectors (0.6416)\n",
      "  ‚úì Scaling factor computes correctly (1.56x)\n",
      "  ‚úì The architecture now supports emotion blending via scaled deltas\n",
      "\n",
      "üéØ Next steps to enable full audio synthesis with emotion:\n",
      "  ‚Ä¢ Resolve XTTS speaker_embedding parameter passing issue\n",
      "  ‚Ä¢ Implement wrapper at tts_to_file level instead of inference level\n",
      "  ‚Ä¢ Or: Use alternative TTS model with explicit embedding control\n",
      " > Processing time: 51.55499577522278\n",
      " > Real-time factor: 6.587781970582188\n",
      "  ‚úì Reference audio saved\n",
      "\n",
      "‚úÖ CONCEPT VERIFICATION COMPLETE\n",
      "\n",
      "üìã Summary:\n",
      "  ‚úì Emotion deltas extracted as non-unit-norm vectors (0.6416)\n",
      "  ‚úì Scaling factor computes correctly (1.56x)\n",
      "  ‚úì The architecture now supports emotion blending via scaled deltas\n",
      "\n",
      "üéØ Next steps to enable full audio synthesis with emotion:\n",
      "  ‚Ä¢ Resolve XTTS speaker_embedding parameter passing issue\n",
      "  ‚Ä¢ Implement wrapper at tts_to_file level instead of inference level\n",
      "  ‚Ä¢ Or: Use alternative TTS model with explicit embedding control\n"
     ]
    }
   ],
   "source": [
    "# Test emotion blending with higher alphas (0.5, 0.7, 1.0) - SIMPLE VERSION\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if happy emotion samples exist\n",
    "happy_dir = PROJECT_ROOT / 'data' / 'emotion_samples' / 'happy'\n",
    "if happy_dir.exists():\n",
    "    print(\"üîß Testing emotion blending concept\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n‚ö†Ô∏è NOTE: Direct speaker_embedding injection has shape mismatch issues.\")\n",
    "    print(\"Instead, we'll verify the concept is sound by showing:\")\n",
    "    print(\"  1. Emotion vector is properly extracted (non-unit norm)\")\n",
    "    print(\"  2. Scaling factor is applied correctly\")\n",
    "    print(\"  3. Audio files can be generated with reference speaker\\n\")\n",
    "    \n",
    "    # Get a speaker\n",
    "    speakers = [p for p in (PROJECT_ROOT / 'data' / 'speakers').glob('*') if p.is_file() and p.suffix.lower() in ['.wav', '.mp3']]\n",
    "    if speakers:\n",
    "        speaker_path = speakers[0]\n",
    "        print(f\"Using speaker: {speaker_path.name}\")\n",
    "        \n",
    "        # Clean the speaker\n",
    "        try:\n",
    "            speaker_clean = ensure_speaker_clean(speaker_path, sr=SR_XTTS)\n",
    "        except:\n",
    "            speaker_clean = speaker_path\n",
    "        \n",
    "        # Compute UNNORMALIZED emotion vector for happy\n",
    "        try:\n",
    "            emotion_vec = compute_emotion_vector_xtts_multi(happy_dir, method='xtts_native', mode='average',\n",
    "                                                           save_avg_dir=PROJECT_ROOT/'data'/'outputs'/'emotion_vectors'/'average')\n",
    "            emotion_norm = np.linalg.norm(emotion_vec)\n",
    "            print(f\"\\n‚úì Emotion vector computed\")\n",
    "            print(f\"  Shape: {emotion_vec.shape}\")\n",
    "            print(f\"  Norm (raw difference of L2-normalized embeddings): {emotion_norm:.6f}\")\n",
    "            \n",
    "            # Verify the scaling concept\n",
    "            sp = get_xtts_speaker_latent(XTTS, speaker_clean, load_sr=SR_XTTS)\n",
    "            sp_norm = np.linalg.norm(sp)\n",
    "            scale_factor = sp_norm / (emotion_norm + 1e-12)\n",
    "            \n",
    "            print(f\"\\nüìä Scaling verification:\")\n",
    "            print(f\"  Speaker embedding norm: {sp_norm:.6f}\")\n",
    "            print(f\"  Emotion vector norm: {emotion_norm:.6f}\")\n",
    "            print(f\"  Scaling factor (sp_norm / ev_norm): {scale_factor:.4f}x\")\n",
    "            print(f\"\\n  After scaling, emotion vector would have norm: {emotion_norm * scale_factor:.6f}\")\n",
    "            print(f\"  This matches speaker embedding norm ({sp_norm:.6f}), allowing perceptible alpha changes\")\n",
    "            \n",
    "            # Generate reference audio without emotion\n",
    "            test_text = \"‡§∏‡•Ç‡§∞‡§ú ‡§ï‡•Ä ‡§∞‡•ã‡§∂‡§®‡•Ä ‡§ö‡§Æ‡§ï ‡§∞‡§π‡•Ä ‡§π‡•à, ‡§¨‡§ö‡•ç‡§ö‡•ã‡§Ç ‡§ï‡•Ä ‡§π‡§Å‡§∏‡•Ä ‡§ó‡•Ç‡§Å‡§ú ‡§∞‡§π‡•Ä ‡§π‡•à, ‡§î‡§∞ ‡§∏‡§¨‡§ï‡•á ‡§ö‡•á‡§π‡§∞‡•á ‡§™‡§∞ ‡§Æ‡•Å‡§∏‡•ç‡§ï‡§æ‡§® ‡§ñ‡§ø‡§≤‡•Ä ‡§π‡•Å‡§à ‡§π‡•à‡•§\"\n",
    "            print(f\"\\nüé§ Synthesizing reference audio (no emotion blending)...\")\n",
    "            try:\n",
    "                ref_path = XTTS.tts_to_file(\n",
    "                    text=test_text,\n",
    "                    speaker_wav=str(speaker_clean),\n",
    "                    language='hi',\n",
    "                    file_path=str(PROJECT_ROOT/'data'/'outputs'/'generated'/'reference_no_emotion.wav')\n",
    "                )\n",
    "                print(f\"  ‚úì Reference audio saved\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Reference synthesis failed: {str(e)[:80]}\")\n",
    "            \n",
    "            print(f\"\\n‚úÖ CONCEPT VERIFICATION COMPLETE\")\n",
    "            print(f\"\\nüìã Summary:\")\n",
    "            print(f\"  ‚úì Emotion deltas extracted as non-unit-norm vectors ({emotion_norm:.4f})\")\n",
    "            print(f\"  ‚úì Scaling factor computes correctly ({scale_factor:.2f}x)\")\n",
    "            print(f\"  ‚úì The architecture now supports emotion blending via scaled deltas\")\n",
    "            print(f\"\\nüéØ Next steps to enable full audio synthesis with emotion:\")\n",
    "            print(f\"  ‚Ä¢ Resolve XTTS speaker_embedding parameter passing issue\")\n",
    "            print(f\"  ‚Ä¢ Implement wrapper at tts_to_file level instead of inference level\")\n",
    "            print(f\"  ‚Ä¢ Or: Use alternative TTS model with explicit embedding control\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(f\"Error: {e}\")\n",
    "    else:\n",
    "        print(\"No speaker files found in data/speakers/\")\n",
    "else:\n",
    "    print(\"Happy emotion samples not found. Run preprocessing first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ae81a4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DIAGNOSTIC: Checking raw XTTS speaker latent norms\n",
      "======================================================================\n",
      "‚úì Found clean files: neutral_clean.wav, happy_clean.wav\n",
      "\n",
      "yi (neutral XTTS latent):\n",
      "  Shape: (512,)\n",
      "  Norm: 1.000000\n",
      "  First 5 values: [ 0.07747481 -0.05574609  0.0368368   0.01727467  0.10866245]\n",
      "\n",
      "ye (emotion XTTS latent):\n",
      "  Shape: (512,)\n",
      "  Norm: 1.000000\n",
      "  First 5 values: [ 0.01586454 -0.0983865   0.02240454  0.062475    0.08436754]\n",
      "\n",
      "(ye - yi) raw difference:\n",
      "  Shape: (512,)\n",
      "  Norm: 0.762460\n",
      "  First 5 values: [-0.06161027 -0.04264041 -0.01443226  0.04520033 -0.02429491]\n",
      "\n",
      "üìä Multi-sample averaging (4 samples):\n",
      "  Average (ye - yi) across 4 samples:\n",
      "    Norm: 0.641648\n",
      "    First 5 values: [-0.01102235 -0.0353942  -0.02338244  0.03386448 -0.01073863]\n",
      "\n",
      "üîç Now calling compute_emotion_vector_xtts_multi()...\n",
      "[sample001] Found existing clean files: neutral_clean.wav, happy_clean.wav\n",
      "[sample002] Found existing clean files: neutral_clean.wav, happy_clean.wav\n",
      "[sample003] Found existing clean files: neutral_clean.wav, happy_clean.wav\n",
      "[sample004] Found existing clean files: neutral_clean.wav, happy_clean.wav\n",
      "\n",
      "  ‚úì Emotion delta computed from 4 samples\n",
      "    Shape: (512,)\n",
      "    Norm (difference of normalized embeddings): 0.641648\n",
      "    ‚ö†Ô∏è NOTE: XTTS returns L2-normalized embeddings (norm=1.0)\n",
      "    The emotion vector will be scaled up in apply_emotion_and_synthesize()\n",
      "    to compensate for normalized embeddings (scale factor ‚âà 1.56x)\n",
      "\n",
      "  Returned vector norm: 0.641648\n",
      "  Returned vector first 5 values: [-0.01102235 -0.0353942  -0.02338244  0.03386448 -0.01073863]\n",
      "  Do they match? avg_diff norm=0.641648, returned norm=0.641648\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Check XTTS embedding norms BEFORE any averaging or normalization\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç DIAGNOSTIC: Checking raw XTTS speaker latent norms\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "happy_dir = PROJECT_ROOT / 'data' / 'emotion_samples' / 'happy'\n",
    "if happy_dir.exists():\n",
    "    sample_dir = happy_dir / 'sample001'\n",
    "    if sample_dir.exists():\n",
    "        n_clean = sample_dir / 'neutral_clean.wav'\n",
    "        e_clean = sample_dir / 'happy_clean.wav'\n",
    "        \n",
    "        if n_clean.exists() and e_clean.exists():\n",
    "            print(f\"‚úì Found clean files: {n_clean.name}, {e_clean.name}\\n\")\n",
    "            \n",
    "            # Get XTTS speaker latents DIRECTLY\n",
    "            yi = get_xtts_speaker_latent(XTTS, n_clean, load_sr=SR_XTTS)\n",
    "            ye = get_xtts_speaker_latent(XTTS, e_clean, load_sr=SR_XTTS)\n",
    "            \n",
    "            print(f\"yi (neutral XTTS latent):\")\n",
    "            print(f\"  Shape: {yi.shape}\")\n",
    "            print(f\"  Norm: {np.linalg.norm(yi):.6f}\")\n",
    "            print(f\"  First 5 values: {yi[:5]}\")\n",
    "            \n",
    "            print(f\"\\nye (emotion XTTS latent):\")\n",
    "            print(f\"  Shape: {ye.shape}\")\n",
    "            print(f\"  Norm: {np.linalg.norm(ye):.6f}\")\n",
    "            print(f\"  First 5 values: {ye[:5]}\")\n",
    "            \n",
    "            # Check the difference\n",
    "            diff = ye - yi\n",
    "            print(f\"\\n(ye - yi) raw difference:\")\n",
    "            print(f\"  Shape: {diff.shape}\")\n",
    "            print(f\"  Norm: {np.linalg.norm(diff):.6f}\")\n",
    "            print(f\"  First 5 values: {diff[:5]}\")\n",
    "            \n",
    "            # Now average across multiple samples if available\n",
    "            sample_dirs = [d for d in sorted(happy_dir.iterdir()) if d.is_dir()]\n",
    "            if len(sample_dirs) > 1:\n",
    "                print(f\"\\nüìä Multi-sample averaging ({len(sample_dirs)} samples):\")\n",
    "                all_diffs = []\n",
    "                for sd in sample_dirs:\n",
    "                    n_clean = sd / 'neutral_clean.wav'\n",
    "                    e_clean = sd / 'happy_clean.wav'\n",
    "                    if n_clean.exists() and e_clean.exists():\n",
    "                        yi = get_xtts_speaker_latent(XTTS, n_clean, load_sr=SR_XTTS)\n",
    "                        ye = get_xtts_speaker_latent(XTTS, e_clean, load_sr=SR_XTTS)\n",
    "                        all_diffs.append(ye - yi)\n",
    "                \n",
    "                avg_diff = np.mean(all_diffs, axis=0)\n",
    "                print(f\"  Average (ye - yi) across {len(all_diffs)} samples:\")\n",
    "                print(f\"    Norm: {np.linalg.norm(avg_diff):.6f}\")\n",
    "                print(f\"    First 5 values: {avg_diff[:5]}\")\n",
    "                \n",
    "                # Now call the function and compare\n",
    "                print(f\"\\nüîç Now calling compute_emotion_vector_xtts_multi()...\")\n",
    "                returned_vec = compute_emotion_vector_xtts_multi(happy_dir, method='xtts_native', mode='average')\n",
    "                returned_norm = np.linalg.norm(returned_vec)\n",
    "                print(f\"\\n  Returned vector norm: {returned_norm:.6f}\")\n",
    "                print(f\"  Returned vector first 5 values: {returned_vec[:5]}\")\n",
    "                print(f\"  Do they match? avg_diff norm={np.linalg.norm(avg_diff):.6f}, returned norm={returned_norm:.6f}\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"‚úó Clean files not found in {sample_dir}\")\n",
    "    else:\n",
    "        print(f\"‚úó sample001 not found in {happy_dir}\")\n",
    "else:\n",
    "    print(f\"‚úó happy folder not found in {PROJECT_ROOT / 'data' / 'emotion_samples'}\")\n",
    "\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "461dd8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved model: <class 'TTS.tts.models.xtts.Xtts'>\n",
      "\n",
      "Inference signature:\n",
      "(text, language, gpt_cond_latent, speaker_embedding, temperature=0.75, length_penalty=1.0, repetition_penalty=10.0, top_k=50, top_p=0.85, do_sample=True, num_beams=1, speed=1.0, enable_text_splitting=False, **hf_generate_kwargs)\n",
      "\n",
      "Parameters:\n",
      " - text text\n",
      " - language language\n",
      " - gpt_cond_latent gpt_cond_latent\n",
      " - speaker_embedding speaker_embedding\n",
      " - temperature temperature=0.75\n",
      " - length_penalty length_penalty=1.0\n",
      " - repetition_penalty repetition_penalty=10.0\n",
      " - top_k top_k=50\n",
      " - top_p top_p=0.85\n",
      " - do_sample do_sample=True\n",
      " - num_beams num_beams=1\n",
      " - speed speed=1.0\n",
      " - enable_text_splitting enable_text_splitting=False\n",
      " - hf_generate_kwargs **hf_generate_kwargs\n",
      "\n",
      "Using speaker file: data\\speakers\\amir_khan_clean.wav\n",
      "get_conditioning_latents returned type: <class 'tuple'>\n",
      "  item[0] type: <class 'torch.Tensor'>\n",
      "    torch tensor shape/dtype: torch.Size([1, 32, 1024]) torch.float32\n",
      "  item[1] type: <class 'torch.Tensor'>\n",
      "    torch tensor shape/dtype: torch.Size([1, 512, 1]) torch.float32\n",
      "get_conditioning_latents returned type: <class 'tuple'>\n",
      "  item[0] type: <class 'torch.Tensor'>\n",
      "    torch tensor shape/dtype: torch.Size([1, 32, 1024]) torch.float32\n",
      "  item[1] type: <class 'torch.Tensor'>\n",
      "    torch tensor shape/dtype: torch.Size([1, 512, 1]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: inspect XTTS model.inference signature and conditioning latents shapes\n",
    "import inspect, torch, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    model = resolve_xtts_internal_model(XTTS)\n",
    "    print('Resolved model:', type(model))\n",
    "    sig = inspect.signature(model.inference)\n",
    "    print('\\nInference signature:')\n",
    "    print(sig)\n",
    "    print('\\nParameters:')\n",
    "    for n,p in sig.parameters.items():\n",
    "        print(' -', n, p)\n",
    "\n",
    "    sp_dir = Path('data') / 'speakers'\n",
    "    sp_files = [p for p in sp_dir.glob('*') if p.suffix.lower() in ['.wav','.mp3','.m4a','.flac']]\n",
    "    if not sp_files:\n",
    "        print('\\nNo speaker files found in data/speakers')\n",
    "    else:\n",
    "        sp = sp_files[0]\n",
    "        print('\\nUsing speaker file:', sp)\n",
    "        res = model.get_conditioning_latents(str(sp), load_sr=SR_XTTS)\n",
    "        print('get_conditioning_latents returned type:', type(res))\n",
    "        if isinstance(res, (list, tuple)):\n",
    "            for i, r in enumerate(res):\n",
    "                print(f'  item[{i}] type: {type(r)}')\n",
    "                if isinstance(r, torch.Tensor):\n",
    "                    print('    torch tensor shape/dtype:', r.shape, r.dtype)\n",
    "                else:\n",
    "                    try:\n",
    "                        arr = np.asarray(r)\n",
    "                        print('    numpy shape:', arr.shape, 'dtype:', arr.dtype)\n",
    "                    except Exception as e:\n",
    "                        print('    Could not convert to numpy:', e)\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print('Diagnostic failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "69b11c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct inference test: attempt to synthesize with emotion-blended speaker_embedding\n",
      "Using speaker: D:\\Downloads\\Bengali_EmoKnob\\data\\speakers\\amir_khan_clean.wav\n",
      "[sample001] Found existing clean files: neutral_clean.wav, happy_clean.wav\n",
      "[sample002] Found existing clean files: neutral_clean.wav, happy_clean.wav\n",
      "[sample003] Found existing clean files: neutral_clean.wav, happy_clean.wav\n",
      "[sample004] Found existing clean files: neutral_clean.wav, happy_clean.wav\n",
      "\n",
      "  ‚úì Emotion delta computed from 4 samples\n",
      "    Shape: (512,)\n",
      "    Norm (difference of normalized embeddings): 0.641648\n",
      "    ‚ö†Ô∏è NOTE: XTTS returns L2-normalized embeddings (norm=1.0)\n",
      "    The emotion vector will be scaled up in apply_emotion_and_synthesize()\n",
      "    to compensate for normalized embeddings (scale factor ‚âà 1.56x)\n",
      "Emotion vector norm: 0.64164793\n",
      "gpt_cond_latent shape: torch.Size([1, 32, 1024])\n",
      "\n",
      "--- Synthesizing alpha= 0.1\n",
      "sp_tensor shape: torch.Size([1, 512, 1])\n",
      "Saved -> D:\\Downloads\\Bengali_EmoKnob\\data\\outputs\\generated\\direct_alpha_0.10.wav\n",
      "\n",
      "--- Synthesizing alpha= 0.3\n",
      "sp_tensor shape: torch.Size([1, 512, 1])\n",
      "Saved -> D:\\Downloads\\Bengali_EmoKnob\\data\\outputs\\generated\\direct_alpha_0.30.wav\n",
      "\n",
      "--- Synthesizing alpha= 0.6\n",
      "sp_tensor shape: torch.Size([1, 512, 1])\n",
      "Saved -> D:\\Downloads\\Bengali_EmoKnob\\data\\outputs\\generated\\direct_alpha_0.60.wav\n",
      "\n",
      "Direct inference test complete. Output files:\n",
      " - D:\\Downloads\\Bengali_EmoKnob\\data\\outputs\\generated\\direct_alpha_0.10.wav\n",
      " - D:\\Downloads\\Bengali_EmoKnob\\data\\outputs\\generated\\direct_alpha_0.30.wav\n",
      " - D:\\Downloads\\Bengali_EmoKnob\\data\\outputs\\generated\\direct_alpha_0.60.wav\n"
     ]
    }
   ],
   "source": [
    "# Direct synthesis test: call model.inference with correctly-shaped tensors\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "print('Direct inference test: attempt to synthesize with emotion-blended speaker_embedding')\n",
    "\n",
    "model = resolve_xtts_internal_model(XTTS)\n",
    "sp = next((p for p in (PROJECT_ROOT / 'data' / 'speakers').glob('*') if p.suffix.lower() in ['.wav','.mp3','.m4a','.flac']), None)\n",
    "if sp is None:\n",
    "    print('No speaker files found in data/speakers; aborting test')\n",
    "else:\n",
    "    print('Using speaker:', sp)\n",
    "    happy_dir = PROJECT_ROOT / 'data' / 'emotion_samples' / 'happy'\n",
    "    ev = compute_emotion_vector_xtts_multi(happy_dir, method='xtts_native', mode='average')\n",
    "    ev = np.asarray(ev).astype(np.float32)\n",
    "    print('Emotion vector norm:', np.linalg.norm(ev))\n",
    "\n",
    "    # Ensure gpt_cond_latent shape and speaker_embedding shape\n",
    "    cond = model.get_conditioning_latents(str(sp), load_sr=SR_XTTS)\n",
    "    if isinstance(cond, (list, tuple)) and len(cond) >= 2:\n",
    "        gpt_cond_latent = cond[0]\n",
    "        print('gpt_cond_latent shape:', getattr(gpt_cond_latent, 'shape', None))\n",
    "    else:\n",
    "        raise RuntimeError('Could not obtain gpt_cond_latent from get_conditioning_latents')\n",
    "\n",
    "    out_paths = []\n",
    "    for alpha in [0.1, 0.3, 0.6]:\n",
    "        print('\\n--- Synthesizing alpha=', alpha)\n",
    "        # base speaker embedding\n",
    "        sp_base = get_xtts_speaker_latent(XTTS, sp, load_sr=SR_XTTS)\n",
    "        sp_base = np.asarray(sp_base).astype(np.float32)\n",
    "        # scale emotion to speaker magnitude\n",
    "        ev_norm = np.linalg.norm(ev)\n",
    "        sp_norm = np.linalg.norm(sp_base)\n",
    "        if ev_norm > 1e-12:\n",
    "            scale = sp_norm / ev_norm\n",
    "        else:\n",
    "            scale = 1.0\n",
    "        ev_scaled = ev * scale\n",
    "\n",
    "        new_sp = sp_base + alpha * ev_scaled\n",
    "        # ensure shape [1, channels, 1]\n",
    "        sp_tensor = torch.tensor(new_sp).unsqueeze(0).unsqueeze(-1).float()\n",
    "        print('sp_tensor shape:', sp_tensor.shape)\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                res = model.inference(text='‡§Ø‡§π ‡§è‡§ï ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ ‡§µ‡§æ‡§ï‡•ç‡§Ø ‡§π‡•à‡•§', language='hi', gpt_cond_latent=gpt_cond_latent, speaker_embedding=sp_tensor)\n",
    "            if isinstance(res, dict) and 'wav' in res:\n",
    "                wav = res['wav']\n",
    "            else:\n",
    "                wav = res\n",
    "            if isinstance(wav, torch.Tensor):\n",
    "                wav = wav.cpu().numpy()\n",
    "            wav = np.asarray(wav, dtype=np.float32)\n",
    "            out_path = unique_path(PROJECT_ROOT / 'data' / 'outputs' / 'generated' / f'direct_alpha_{alpha:.2f}.wav')\n",
    "            import soundfile as sf\n",
    "            sf.write(str(out_path), wav, SR_XTTS)\n",
    "            print('Saved ->', out_path)\n",
    "            out_paths.append(str(out_path))\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print('Synthesis failed for alpha', alpha, ':', e)\n",
    "\n",
    "    print('\\nDirect inference test complete. Output files:')\n",
    "    for p in out_paths:\n",
    "        print(' -', p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
