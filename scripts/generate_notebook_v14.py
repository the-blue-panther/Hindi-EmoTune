#!/usr/bin/env python3
'''
generate_notebook_v14.py

Generates hindi_emoknob_demo_v14.ipynb for the Bengali_EmoKnob project.

- Uses local model folders under PROJECT_ROOT/models/
- Implements robust XTTS resolver and correct get_conditioning_latents usage
- Handles Indic resampling (16000) vs XTTS sample rate (22050)
- GUI (ipywidgets) with visible status logs and dropdowns

Author: ChatGPT (prepared for Arnab)
'''
import json
from pathlib import Path
import nbformat
from nbformat.v4 import new_notebook, new_code_cell, new_markdown_cell

# ---------- EDIT/CONFIRM THESE ----------
PROJECT_ROOT = Path(r"D:\Downloads\Bengali_EmoKnob")  # confirmed by user
SCRIPTS_DIR = PROJECT_ROOT / "scripts"
MODELS_DIR = PROJECT_ROOT / "models"
NOTEBOOK_PATH = PROJECT_ROOT / "hindi_emoknob_demo_v14.ipynb"
# ---------------------------------------

def make_cells():
    cells = []

    # 0: Title + short description
    cells.append(new_markdown_cell(
        "# Hindi EmoKnob — Demo (v14)\n\n"
        "Safe, local-model-first notebook. Fixes XTTS wrapper mismatch and adds robust helpers.\n\n"
        "Run cells in order (Environment → XTTS download/load → Indic load → Helpers → GUI)."
    ))

    # 1: Environment / paths / safe threads (do not call set_num_interop_threads)
    cell_env = (
        "# Environment & paths (run first)\n"
        "import os, sys, shutil, traceback\n"
        "from pathlib import Path\n"
        "import torch\n\n"
        f"PROJECT_ROOT = Path(r\"{PROJECT_ROOT}\")\n"
        "MODELS_DIR = PROJECT_ROOT / \"models\"\n"
        "XTTS_LOCAL_DIR = MODELS_DIR / \"xtts_v2\"   # local XTTS folder\n"
        "INDIC_LOCAL_DIR = MODELS_DIR / \"ai4bharat_indicwav2vec_hindi\"  # local Indic wav2vec\n\n"
        "for p in [MODELS_DIR, XTTS_LOCAL_DIR, INDIC_LOCAL_DIR]:\n"
        "    p.mkdir(parents=True, exist_ok=True)\n\n"
        "# Safe thread config: try set_num_threads, avoid set_num_interop_threads after parallel work started\n"
        cell_helpers = (
            "# Embedding helpers: safe XTTS resolver and embedding extraction\n"
            "import numpy as np\n"
            "import torch\n"
            "import librosa\n"
            "from pathlib import Path\n\n"
            "def resolve_xtts_internal_model(tts_obj):\n"
            "    '''Return the internal XTTS model used by TTS wrapper.''\'\'\'\n"
            "    if tts_obj is None:\n"
            "        raise RuntimeError('Provided tts_obj is None')\n"
            "    if hasattr(tts_obj, 'synthesizer') and hasattr(tts_obj.synthesizer, 'tts_model'):\n"
            "        return tts_obj.synthesizer.tts_model\n"
            "    if hasattr(tts_obj, 'tts_model'):\n"
            "        return tts_obj.tts_model\n"
            "    raise RuntimeError('Could not resolve internal XTTS model. Ensure you loaded native XTTS-v2 via TTS API.')\n\n"
            "def get_indic_embedding(wav_path, sr_source=SR_XTTS, sr_indic=SR_INDIC):\n"
            "    '''Load wav, resample to sr_indic if needed, and return 1D numpy embedding (mean of last_hidden_state).''\'\'\'\n"
            "    global processor, indic_enc\n"
            "    if 'processor' not in globals() or 'indic_enc' not in globals():\n"
            "        raise RuntimeError('Indic encoder not loaded. Run the Indic load cell.')\n"
            "    y, sr = librosa.load(str(wav_path), sr=sr_source, mono=True)\n"
            "    if sr != sr_indic:\n"
            "        y = librosa.resample(y, orig_sr=sr, target_sr=sr_indic)\n"
            "    inp = processor(y, sampling_rate=sr_indic, return_tensors='pt', padding=True)\n"
            "    with torch.no_grad():\n"
            "        out = indic_enc(**inp).last_hidden_state\n"
            "    emb = out.mean(dim=1).squeeze().detach().cpu().numpy()\n"
            "    return emb\n\n"
            "def get_xtts_speaker_latent(tts_obj, wav_path, load_sr=SR_XTTS):\n"
            "    '''Extract speaker latent from XTTS. Try to call internal speaker encoder with l2_norm=False\n"
            "    to obtain raw (non-normalized) embeddings. Falls back to model.get_conditioning_latents.\n"
            "    Returns 1D numpy array.''\'\'\'\n"
            "    model = resolve_xtts_internal_model(tts_obj)\n"
            "    # Try to load audio into a torch tensor (batch, samples)\n"
            "    try:\n"
            "        wav, sr = librosa.load(str(wav_path), sr=load_sr, mono=True)\n"
            "        audio_t = torch.tensor(wav).unsqueeze(0).float()\n"
            "    except Exception:\n"
            "        audio_t = None\n\n"
            "    def _walk_attr(obj, path_list):\n"
            "        cur = obj\n"
            "        for p in path_list:\n"
            "            if hasattr(cur, p):\n"
            "                cur = getattr(cur, p)\n"
            "            else:\n"
            "                return None\n"
            "        return cur\n\n"
            "    tried = []\n"
            "    encoder_paths = [\n"
            "        ['hifigan_decoder', 'speaker_encoder'],\n"
            "        ['speaker_manager', 'encoder'],\n"
            "        ['speaker_encoder'],\n"
            "        ['synthesizer', 'speaker_encoder'],\n"
            "        ['encoder', 'speaker_encoder'],\n"
            "    ]\n\n"
            "    for p in encoder_paths:\n"
            "        enc = _walk_attr(model, p)\n"
            "        if enc is None:\n"
            "            tried.append((p, False))\n"
            "            continue\n"
            "        tried.append((p, True))\n"
            "        try:\n"
            "            with torch.no_grad():\n"
            "                out = None\n"
            "                if audio_t is not None:\n"
            "                    try:\n"
            "                        out = enc.forward(audio_t, l2_norm=False)\n"
            "                    except TypeError:\n"
            "                        try:\n"
            "                            out = enc.forward(audio_t)\n"
            "                        except Exception:\n"
            "                            out = None\n"
            "                else:\n"
            "                    out = None\n\n"
            "                if out is None:\n"
            "                    try:\n"
            "                        out = enc(str(wav_path))\n"
            "                    except Exception:\n"
            "                        out = None\n\n"
            "                if out is not None:\n"
            "                    sp = out.squeeze()\n"
            "                    return sp.detach().cpu().numpy() if hasattr(sp, 'detach') else np.array(sp)\n"
            "        except Exception:\n"
            "            continue\n\n"
            "    # Fallback to legacy API\n"
            "    try:\n"
            "        res = model.get_conditioning_latents(str(wav_path), load_sr=load_sr)\n"
            "    except TypeError:\n"
            "        res = model.get_conditioning_latents(str(wav_path))\n"
            "    if isinstance(res, (list, tuple)) and len(res) >= 2:\n"
            "        speaker_lat = res[1]\n"
            "    else:\n"
            "        speaker_lat = res\n"
            "    try:\n"
            "        sp = speaker_lat.squeeze()\n"
            "        return sp.detach().cpu().numpy() if hasattr(sp, 'detach') else np.array(sp)\n"
            "    except Exception as e:\n"
            "        raise RuntimeError('Failed to convert speaker latent to numpy: ' + str(e) + '\\nTried encoders: ' + str(tried))\n\n"
            "print('Helpers ready: resolve_xtts_internal_model, get_indic_embedding, get_xtts_speaker_latent')\n"
        )
        "    return out_wav_path\n\n"
        "def unique_path(path: Path):\n"
        "    path = Path(path)\n"
        "    if not path.exists():\n"
        "        return path\n"
        "    base = path.stem\n"
        "    suf = path.suffix\n"
        "    parent = path.parent\n"
        "    i = 1\n"
        "    while True:\n"
        "        candidate = parent / f\"{base}_{i}{suf}\"\n"
        "        if not candidate.exists():\n"
        "            return candidate\n"
        "        i += 1\n"
    )
    cells.append(new_code_cell(cell_utils))

    # 3: XTTS download/load cell (local-first, fallback to model_name)
    cell_xtts = (
        "# XTTS local download & loader (local-first)\n"
        "from TTS.api import TTS\n"
        "from huggingface_hub import snapshot_download\n"
        "import shutil, os, traceback\n"
        "from pathlib import Path\n\n"
        "def ensure_xtts_local(target_dir: Path):\n"
        "    target_dir.mkdir(parents=True, exist_ok=True)\n"
        "    ck = target_dir / 'model.pth'\n"
        "    cfg = target_dir / 'config.json'\n"
        "    if ck.exists() and cfg.exists():\n"
        "        print('XTTS local present:', target_dir)\n"
        "        return True\n"
        "    print('Attempting snapshot_download of coqui/xtts-v2 into models folder (best-effort)...')\n"
        "    try:\n"
        "        tmp = snapshot_download(repo_id='coqui/xtts-v2', cache_dir=str(target_dir), repo_type='model', allow_patterns=['*'])\n"
        "        print('snapshot_download result:', tmp)\n"
        "    except Exception as e:\n"
        "        print('snapshot_download failed (this is OK if huggingface auth required). Error:', e)\n"
        "        traceback.print_exc()\n"
        "    ck = target_dir / 'model.pth'\n"
        "    cfg = target_dir / 'config.json'\n"
        "    if ck.exists() and cfg.exists():\n"
        "        return True\n"
        "    print('XTTS not available locally. You can allow TTS to download to cache once, then move folder to models/xtts_v2.')\n"
        "    return False\n\n"
        "def load_xtts_local_or_remote(gpu=False):\n"
        "    ok = ensure_xtts_local(Path('models') / 'xtts_v2')\n"
        "    try:\n"
        "        if ok:\n"
        "            print('Trying to load XTTS from local models/xtts_v2 ...')\n"
        "            t = TTS(model_path=str(Path('models') / 'xtts_v2' / 'model.pth'),\n"
        "                    config_path=str(Path('models') / 'xtts_v2' / 'config.json'),\n"
        "                    gpu=gpu)\n"
        "            print('Loaded XTTS from local files.')\n"
        "            return t\n"
        "    except Exception as e:\n"
        "        print('Failed to load local XTTS (will try model_name). Error:', e)\n"
        "        traceback.print_exc()\n"
        "    print('Loading XTTS via model_name (this will download to user cache if not present)...')\n"
        "    t = TTS(model_name='tts_models/multilingual/multi-dataset/xtts_v2', gpu=gpu)\n"
        "    print('XTTS loaded via model_name.')\n"
        "    return t\n\n"
        "# Load XTTS (CPU first)\n"
        "XTTS = None\n"
        "try:\n"
        "    XTTS = load_xtts_local_or_remote(gpu=False)\n"
        "except Exception as e:\n"
        "    print('XTTS load error:', e)\n"
        "    import traceback; traceback.print_exc()\n"
    )
    cells.append(new_code_cell(cell_xtts))

    # 4: Indic load cell (local)
    cell_indic = (
        "# Load ai4bharat/indicwav2vec-hindi from local models folder\n"
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n"
        "import torch\n"
        "from pathlib import Path\n\n"
        "INDIC_LOCAL = Path('models') / 'ai4bharat_indicwav2vec_hindi'\n"
        "if not INDIC_LOCAL.exists():\n"
        "    print('Warning: Indic model folder not found at:', INDIC_LOCAL)\n"
        "else:\n"
        "    print('Loading Indic wav2vec from:', INDIC_LOCAL)\n"
        "    processor = Wav2Vec2Processor.from_pretrained(str(INDIC_LOCAL))\n"
        "    indic_enc = Wav2Vec2Model.from_pretrained(str(INDIC_LOCAL))\n"
        "    indic_enc.eval()\n"
        "    print('Indic encoder loaded.')\n"
    )
    cells.append(new_code_cell(cell_indic))

    # 5: Embedding helpers with XTTS resolver + correct usage
    cell_helpers = (
        "# Embedding helpers: safe XTTS resolver and embedding extraction\n"
        "import numpy as np\n"
        "import torch\n"
        "import librosa\n"
        "from pathlib import Path\n\n"
        "def resolve_xtts_internal_model(tts_obj):\n"
        "    '''Return the internal XTTS model used by TTS wrapper.'''\n"
        "    if tts_obj is None:\n"
        "        raise RuntimeError('Provided tts_obj is None')\n"
        "    if hasattr(tts_obj, 'synthesizer') and hasattr(tts_obj.synthesizer, 'tts_model'):\n"
        "        return tts_obj.synthesizer.tts_model\n"
        "    if hasattr(tts_obj, 'tts_model'):\n"
        "        return tts_obj.tts_model\n"
        "    raise RuntimeError('Could not resolve internal XTTS model. Ensure you loaded native XTTS-v2 via TTS API.')\n\n"
        "def get_indic_embedding(wav_path, sr_source=SR_XTTS, sr_indic=SR_INDIC):\n"
        "    '''Load wav, resample to sr_indic if needed, and return 1D numpy embedding (mean of last_hidden_state).'''\n"
        "    global processor, indic_enc\n"
        "    if 'processor' not in globals() or 'indic_enc' not in globals():\n"
        "        raise RuntimeError('Indic encoder not loaded. Run the Indic load cell.')\n"
        "    y, sr = librosa.load(str(wav_path), sr=sr_source, mono=True)\n"
        "    if sr != sr_indic:\n"
        "        y = librosa.resample(y, orig_sr=sr, target_sr=sr_indic)\n"
        "    inp = processor(y, sampling_rate=sr_indic, return_tensors='pt', padding=True)\n"
        "    with torch.no_grad():\n"
        "        out = indic_enc(**inp).last_hidden_state\n"
        "    emb = out.mean(dim=1).squeeze().detach().cpu().numpy()\n"
        "    return emb\n\n"
        "def get_xtts_speaker_latent(tts_obj, wav_path, load_sr=SR_XTTS):\n"
        "    '''Extract speaker latent from XTTS by calling internal.get_conditioning_latents(audio_path, load_sr=...).\n"
        "    Returns 1D numpy array.'''\n"
        "    model = resolve_xtts_internal_model(tts_obj)\n"
        "    try:\n"
        "        res = model.get_conditioning_latents(str(wav_path), load_sr=load_sr)\n"
        "    except TypeError:\n"
        "        res = model.get_conditioning_latents(str(wav_path))\n"
        "    if isinstance(res, (list, tuple)) and len(res) >= 2:\n"
        "        speaker_lat = res[1]\n"
        "    else:\n"
        "        speaker_lat = res\n"
        "    try:\n"
        "        sp = speaker_lat.squeeze()\n"
        "        return sp.detach().cpu().numpy() if hasattr(sp, 'detach') else np.array(sp)\n"
        "    except Exception as e:\n"
        "        raise RuntimeError('Failed to convert speaker latent to numpy: ' + str(e))\n\n"
        "print('Helpers ready: resolve_xtts_internal_model, get_indic_embedding, get_xtts_speaker_latent')\n"
    )
    cells.append(new_code_cell(cell_helpers))

    # 6: Projection methods (CCA/PLS + mapping)
    cell_projections = (
        "# Projection methods: CCA and PLS mapping utilities\n"
        "import numpy as np\n"
        "from sklearn.cross_decomposition import CCA, PLSRegression\n\n"
        "def fit_cca_or_pls(X_indic, Y_xtts, method='cca', n_comp=32):\n"
        "    if method == 'cca':\n"
        "        cca = CCA(n_components=n_comp, max_iter=500)\n"
        "        cca.fit(X_indic, Y_xtts)\n"
        "        return cca\n"
        "    elif method == 'pls':\n"
        "        pls = PLSRegression(n_components=n_comp, max_iter=500)\n"
        "        pls.fit(X_indic, Y_xtts)\n"
        "        return pls\n"
        "    else:\n"
        "        raise ValueError('Unknown mapping method: ' + str(method))\n\n"
        "def map_indic_vector_to_xtts(model, v_indic):\n"
        "    v_indic = np.asarray(v_indic).reshape(1, -1)\n"
        "    if isinstance(model, CCA):\n"
        "        u, v = model.transform(v_indic, np.zeros((1, model.n_components)))\n"
        "        return v.ravel()\n"
        "    elif isinstance(model, PLSRegression):\n"
        "        out = model.predict(v_indic)\n"
        "        return out.ravel()\n"
        "    else:\n"
        "        return v_indic.ravel()\n"
    )
    cells.append(new_code_cell(cell_projections))

    # 7: Emotion vector extraction (multi-sample average or single)
    cell_emovec = (
        "# Compute emotion vectors from emotion_samples folder (multi-sample average or single)\n"
        "import numpy as np\n"
        "from pathlib import Path\n\n"
        "EMOTION_SAMPLES_DIR = PROJECT_ROOT / 'data' / 'emotion_samples'\n"
        "OUTPUT_SINGLE_DIR = PROJECT_ROOT / 'data' / 'outputs' / 'emotion_vectors' / 'single'\n"
        "OUTPUT_AVG_DIR = PROJECT_ROOT / 'data' / 'outputs' / 'emotion_vectors' / 'average'\n"
        "for p in [OUTPUT_SINGLE_DIR, OUTPUT_AVG_DIR]:\n"
        "    Path(p).mkdir(parents=True, exist_ok=True)\n\n"
        "def compute_emotion_vector_xtts_multi(emotion_dir, method='cca', n_comp=32, mode='average', sample_id=1,\n"
        "                                      save_single_dir=None, save_avg_dir=None):\n"
        "    emotion_dir = Path(emotion_dir)\n"
        "    sample_dirs = [d for d in sorted(emotion_dir.iterdir()) if d.is_dir()]\n"
        "    if len(sample_dirs) == 0:\n"
        "        raise ValueError('No sample subfolders found in: ' + str(emotion_dir))\n\n"
        "    X = []\n"
        "    Y = []\n"
        "    single_vectors = []\n\n"
        "    for sd in sample_dirs:\n"
        "        wavs = [f for f in sorted(sd.iterdir()) if f.suffix.lower() in ['.wav','.mp3','.m4a','.flac']]\n"
        "        if len(wavs) < 2:\n"
        "            print('Skipping sample (not enough files):', sd)\n"
        "            continue\n"
        "        neutral = wavs[0]; emot = wavs[1]\n\n"
        "        n_clean = sd / (neutral.stem + '_clean.wav')\n"
        "        e_clean = sd / (emot.stem + '_clean.wav')\n"
        "        if not n_clean.exists():\n"
        "            preprocess_audio(neutral, n_clean, sr=SR_XTTS)\n"
        "        if not e_clean.exists():\n"
        "            preprocess_audio(emot, e_clean, sr=SR_XTTS)\n\n"
        "        xi = get_indic_embedding(n_clean, sr_source=SR_XTTS, sr_indic=SR_INDIC)\n"
        "        xe = get_indic_embedding(e_clean, sr_source=SR_XTTS, sr_indic=SR_INDIC)\n"
        "        yi = get_xtts_speaker_latent(XTTS, n_clean, load_sr=SR_XTTS)\n"
        "        ye = get_xtts_speaker_latent(XTTS, e_clean, load_sr=SR_XTTS)\n\n"
        "        X.append(xe - xi)\n"
        "        Y.append(ye - yi)\n"
        "        single_vectors.append((sd.name, xe - xi, ye - yi))\n\n"
        "        if save_single_dir:\n"
        "            Path(save_single_dir).mkdir(parents=True, exist_ok=True)\n"
        "            np.save(Path(save_single_dir) / f\"{emotion_dir.name}_{sd.name}_indic.npy\", xe - xi)\n"
        "            np.save(Path(save_single_dir) / f\"{emotion_dir.name}_{sd.name}_xtts.npy\", ye - yi)\n\n"
        "    if len(X) == 0:\n"
        "        raise ValueError('No matched pairs extracted for emotion: ' + str(emotion_dir))\n\n"
        "    X = np.stack(X)\n"
        "    Y = np.stack(Y)\n\n"
        "    if method == 'xtts_native':\n"
        "        if mode == 'single':\n"
        "            idx = sample_id - 1\n"
        "            return single_vectors[idx][2] / (np.linalg.norm(single_vectors[idx][2]) + 1e-12)\n"
        "        avg = np.mean([v for (_,_,v) in single_vectors], axis=0)\n"
        "        avg = avg / (np.linalg.norm(avg) + 1e-12)\n"
        "        if save_avg_dir:\n"
        "            Path(save_avg_dir).mkdir(parents=True, exist_ok=True)\n"
        "            np.save(Path(save_avg_dir) / f\"{emotion_dir.name}_avg_xtts.npy\", avg)\n"
        "        return avg\n\n"
        "    mapper = fit_cca_or_pls(X, Y, method=method, n_comp=min(n_comp, X.shape[1], Y.shape[1]))\n\n"
        "    if mode == 'single':\n"
        "        idx = sample_id - 1\n"
        "        v_indic = single_vectors[idx][1]\n"
        "    else:\n"
        "        v_indic = np.mean([xi for (_,xi,_) in single_vectors], axis=0)\n\n"
        "    v_indic = v_indic / (np.linalg.norm(v_indic) + 1e-12)\n"
        "    mapped = map_indic_vector_to_xtts(mapper, v_indic)\n"
        "    mapped = mapped / (np.linalg.norm(mapped) + 1e-12)\n\n"
        "    if save_avg_dir:\n"
        "        Path(save_avg_dir).mkdir(parents=True, exist_ok=True)\n"
        "        np.save(Path(save_avg_dir) / f\"{emotion_dir.name}_avg_mapped.npy\", mapped)\n\n"
        "    return mapped\n"
    )
    cells.append(new_code_cell(cell_emovec))

    # 8: Synthesis cell (apply emotion vector and synthesize)
    cell_synth = (
        "# Apply emotion vector and synthesize via XTTS\n"
        "import numpy as np\n"
        "import torch\n"
        "from pathlib import Path\n"
        "import soundfile as sf\n\n"
        "OUTPUT_GEN_DIR = PROJECT_ROOT / 'data' / 'outputs' / 'generated'\n"
        "OUTPUT_GEN_DIR.mkdir(parents=True, exist_ok=True)\n\n"
        "def apply_emotion_and_synthesize(text, speaker_wav, emotion_vec, alpha=0.7, out_path=None, language='hi'):\n"
        "    if out_path is None:\n"
        "        out_path = OUTPUT_GEN_DIR / 'test_hindi_emotional.wav'\n"
        "    out_path = unique_path(Path(out_path))\n\n"
        "    sp = get_xtts_speaker_latent(XTTS, speaker_wav, load_sr=SR_XTTS)\n"
        "    sp = np.asarray(sp)\n"
        "    ev = np.asarray(emotion_vec)\n\n"
        "    if ev.shape[0] != sp.shape[0]:\n"
        "        if ev.shape[0] > sp.shape[0]:\n"
        "            ev = ev[: sp.shape[0]]\n"
        "        else:\n"
        "            ev = np.pad(ev, (0, sp.shape[0]-ev.shape[0]))\n\n"
        "    new_sp = sp + alpha * ev\n"
        "    sp_tensor = torch.tensor(new_sp).unsqueeze(0).float()\n\n"
        "    try:\n"
        "        XTTS.tts_to_file(text=text, speaker_wav=str(speaker_wav), language=language,\n"
        "                         speaker_embedding=sp_tensor, file_path=str(out_path))\n"
        "        print('Synthesis complete ->', out_path)\n"
        "    except Exception as e:\n"
        "        print('Primary synth failed, trying low-level inference. Error:', e)\n"
        "        try:\n"
        "            with torch.no_grad():\n"
        "                res = XTTS.synthesizer.tts_model.inference(text=text, language=language, speaker_embedding=sp_tensor)\n"
        "            if isinstance(res, dict) and 'wav' in res:\n"
        "                wav = res['wav']\n"
        "            else:\n"
        "                wav = res\n"
        "            sf.write(str(out_path), wav.astype(np.float32), SR_XTTS)\n"
        "            print('Fallback synthesis complete ->', out_path)\n"
        "        except Exception as e2:\n"
        "            raise RuntimeError('Synthesis failed: ' + str(e) + ' // fallback: ' + str(e2))\n\n"
        "    return out_path\n"
    )
    cells.append(new_code_cell(cell_synth))

    # 9: GUI cell (ipywidgets) with dropdowns and progress prints
    cell_gui = (
        "# Simple GUI (ipywidgets)\n"
        "import ipywidgets as widgets\n"
        "from IPython.display import display, clear_output\n\n"
        "def list_speakers():\n"
        "    sp_dir = PROJECT_ROOT / 'data' / 'speakers'\n"
        "    sp_dir.mkdir(parents=True, exist_ok=True)\n"
        "    items = [p.name for p in sp_dir.iterdir() if p.suffix.lower() in ['.wav','.mp3','.m4a','.flac']]\n"
        "    return items if items else ['(no speakers found)']\n\n"
        "def list_emotions():\n"
        "    ed = PROJECT_ROOT / 'data' / 'emotion_samples'\n"
        "    ed.mkdir(parents=True, exist_ok=True)\n"
        "    return [p.name for p in ed.iterdir() if p.is_dir()] or ['(no emotions found)']\n\n"
        "emotion_dropdown = widgets.Dropdown(options=list_emotions(), description='Emotion:')\n"
        "method_dropdown = widgets.Dropdown(options=['cca','pls','xtts_native'], description='Method:')\n"
        "mode_dropdown = widgets.Dropdown(options=['average','single'], description='Mode:')\n"
        "sample_id_widget = widgets.IntText(value=1, description='Sample id:')\n"
        "alpha_slider = widgets.FloatSlider(value=0.7, min=0.0, max=1.0, step=0.01, description='alpha:')\n"
        "text_in = widgets.Text(value='मैं आज बहुत खुश हूँ।', description='Text:')\n"
        "speaker_dropdown = widgets.Dropdown(options=list_speakers(), description='Speaker:')\n"
        "run_button = widgets.Button(description='Run', button_style='success')\n"
        "log_out = widgets.Output(layout={'border': '1px solid black'})\n\n"
        "display(widgets.VBox([emotion_dropdown, method_dropdown, mode_dropdown, sample_id_widget, alpha_slider, speaker_dropdown, text_in, run_button, log_out]))\n\n"
        "def on_run_clicked(b):\n"
        "    with log_out:\n"
        "        clear_output()\n"
        "        print('[RUN] Starting pipeline...')\n"
        "        emotion = emotion_dropdown.value\n"
        "        method = method_dropdown.value\n"
        "        mode = mode_dropdown.value\n"
        "        sid = int(sample_id_widget.value)\n"
        "        alpha = float(alpha_slider.value)\n"
        "        sp = speaker_dropdown.value\n"
        "        txt = text_in.value\n"
        "        print('Options -> emotion:', emotion, 'method:', method, 'mode:', mode, 'sample:', sid, 'alpha:', alpha, 'speaker:', sp)\n"
        "        try:\n"
        "            emotion_dir = PROJECT_ROOT / 'data' / 'emotion_samples' / emotion\n"
        "            print('Computing emotion vector... (this may take a few seconds)')\n"
        "            vec = compute_emotion_vector_xtts_multi(emotion_dir, method=method, n_comp=32, mode=mode, sample_id=sid,\n"
        "                                                   save_single_dir=PROJECT_ROOT/'data'/'outputs'/'emotion_vectors'/'single'/emotion,\n"
        "                                                   save_avg_dir=PROJECT_ROOT/'data'/'outputs'/'emotion_vectors'/'average')\n"
        "            print('Emotion vector shape:', getattr(vec, 'shape', None))\n"
        "            speaker_wav = PROJECT_ROOT / 'data' / 'speakers' / sp\n"
        "            print('Synthesizing...')\n"
        "            outp = apply_emotion_and_synthesize(txt, speaker_wav, vec, alpha=alpha)\n"
        "            print('Done. Output saved at:', outp)\n"
        "        except Exception as e:\n"
        "            import traceback\n"
        "            traceback.print_exc()\n\n"
        "run_button.on_click(on_run_clicked)\n"
        "print('GUI ready. Select options and press Run.')\n"
    )
    cells.append(new_code_cell(cell_gui))

    # 10: Final usage notes
    cells.append(new_markdown_cell(
        "## Usage notes\n\n"
        "- Run the **Environment** cell first.\n"
        "- Run **XTTS** load cell (cell 3). If XTTS downloads to cache, copy the model folder into `models/xtts_v2` to avoid repeated downloads.\n"
        "- Make sure `ffmpeg` is installed and in PATH.\n"
        "- Populate `data/speakers` and `data/emotion_samples/<emotion>/sample_xx/` with your audio files.\n"
        "- The GUI will list available speakers and emotions automatically."
    ))

    return cells

def write_notebook(path: Path):
    nb = new_notebook()
    nb['cells'] = make_cells()
    nb['metadata'] = {
        "kernelspec": {"display_name": "Python 3 (venv)", "language": "python", "name": "python3"},
        "language_info": {"name": "python"}
    }
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf8") as f:
        nbformat.write(nb, f)
    print("Notebook written to:", path)

if __name__ == "__main__":
    NOTEBOOK_PATH = Path(NOTEBOOK_PATH)  # ensure Path object
    write_notebook(NOTEBOOK_PATH)
    print("Generation complete. Open the notebook and run cells inside your virtual environment.")
