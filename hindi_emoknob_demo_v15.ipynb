{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ca370a",
   "metadata": {},
   "source": [
    "# Hindi EmoKnob ‚Äî Demo (v15)\n",
    "\n",
    "Safe, local-model-first notebook. Fixes XTTS wrapper mismatch and adds robust helpers.\n",
    "\n",
    "Run cells in order (Environment ‚Üí XTTS download/load ‚Üí Indic load ‚Üí Helpers ‚Üí GUI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b0ff3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: D:\\Downloads\\Projects\\Hindi Emotion controlled TTS with Voice cloning\n",
      "MODELS_DIR: D:\\Downloads\\Projects\\Hindi Emotion controlled TTS with Voice cloning\\models\n",
      "XTTS_LOCAL_DIR: D:\\Downloads\\Projects\\Hindi Emotion controlled TTS with Voice cloning\\models\\xtts_v2\n",
      "INDIC_LOCAL_DIR: D:\\Downloads\\Projects\\Hindi Emotion controlled TTS with Voice cloning\\models\\ai4bharat_indicwav2vec_hindi\n",
      "SR_XTTS: 22050 SR_INDIC: 16000\n"
     ]
    }
   ],
   "source": [
    "# Environment & paths (run first)\n",
    "import os, sys, shutil, traceback\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "XTTS_LOCAL_DIR = MODELS_DIR / \"xtts_v2\"   # local XTTS folder\n",
    "INDIC_LOCAL_DIR = MODELS_DIR / \"ai4bharat_indicwav2vec_hindi\"  # local Indic wav2vec\n",
    "\n",
    "for p in [MODELS_DIR, XTTS_LOCAL_DIR, INDIC_LOCAL_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Safe thread config: try set_num_threads, avoid set_num_interop_threads after parallel work started\n",
    "try:\n",
    "    torch.set_num_threads(2)\n",
    "except Exception as e:\n",
    "    print(\"Warning: torch.set_num_threads failed:\", e)\n",
    "\n",
    "# Sampling rates\n",
    "SR_XTTS = 22050   # XTTS default sample rate used by get_conditioning_latents\n",
    "SR_INDIC = 16000  # ai4bharat/indicwav2vec-hindi expects 16k\n",
    "\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "print('MODELS_DIR:', MODELS_DIR)\n",
    "print('XTTS_LOCAL_DIR:', XTTS_LOCAL_DIR)\n",
    "print('INDIC_LOCAL_DIR:', INDIC_LOCAL_DIR)\n",
    "print('SR_XTTS:', SR_XTTS, 'SR_INDIC:', SR_INDIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bb592b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Utilities & Helpers Ready: list_emotions, list_speakers, ensure_speaker_clean, gui_alpha_to_internal defined.\n"
     ]
    }
   ],
   "source": [
    "# Utilities: ffmpeg-based conversion + normalization + unique path\n",
    "import subprocess\n",
    "import librosa, soundfile as sf\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def run_ffmpeg_convert_to_wav(in_path, out_path, sr):\n",
    "    in_path = str(in_path)\n",
    "    out_path = str(out_path)\n",
    "    cmd = [\"ffmpeg\", \"-y\", \"-i\", in_path, \"-ac\", \"1\", \"-ar\", str(sr), \"-vn\",\n",
    "           \"-hide_banner\", \"-loglevel\", \"error\", out_path]\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"ffmpeg conversion failed for {in_path}: {e}\")\n",
    "\n",
    "def preprocess_audio(in_path, out_wav_path, sr, normalize=True):\n",
    "    '''Convert any audio file to mono WAV with sample-rate 'sr', do simple amplitude normalization.'''\n",
    "    in_path = Path(in_path)\n",
    "    out_wav_path = Path(out_wav_path)\n",
    "    out_wav_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = out_wav_path.with_suffix('.tmp.wav')\n",
    "    run_ffmpeg_convert_to_wav(in_path, tmp, sr)\n",
    "    y, _ = librosa.load(str(tmp), sr=sr, mono=True)\n",
    "    if normalize:\n",
    "        peak = max(1e-9, max(abs(float(y.max())), abs(float(y.min()))))\n",
    "        y = y / peak * 0.95\n",
    "    sf.write(str(out_wav_path), y.astype(np.float32), sr)\n",
    "    try:\n",
    "        tmp.unlink()\n",
    "    except:\n",
    "        pass\n",
    "    return out_wav_path\n",
    "\n",
    "def unique_path(path: Path):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        return path\n",
    "    base = path.stem\n",
    "    suf = path.suffix\n",
    "    parent = path.parent\n",
    "    i = 1\n",
    "    while True:\n",
    "        candidate = parent / f\"{base}_{i}{suf}\"\n",
    "        if not candidate.exists():\n",
    "            return candidate\n",
    "        i += 1\n",
    "\n",
    "# --- MISSING HELPERS ADDED BY FIX ---\n",
    "def list_emotions():\n",
    "    emotion_dir = PROJECT_ROOT / 'data' / 'emotion_samples'\n",
    "    if not emotion_dir.exists(): return []\n",
    "    return sorted([d.name for d in emotion_dir.iterdir() if d.is_dir()])\n",
    "\n",
    "def list_speakers():\n",
    "    speaker_dir = PROJECT_ROOT / 'data' / 'speakers'\n",
    "    if not speaker_dir.exists(): return []\n",
    "    return sorted([f.name for f in speaker_dir.iterdir() if f.is_file() and f.suffix.lower() in ['.wav', '.mp3', '.m4a', '.flac']])\n",
    "\n",
    "def ensure_speaker_clean(speaker_path, sr=SR_XTTS):\n",
    "    speaker_path = Path(speaker_path)\n",
    "    if speaker_path.stem.endswith('_clean'):\n",
    "         return speaker_path\n",
    "    \n",
    "    clean_path = speaker_path.with_name(speaker_path.stem + '_clean.wav')\n",
    "    if clean_path.exists():\n",
    "        return clean_path\n",
    "        \n",
    "    preprocess_audio(speaker_path, clean_path, sr=sr)\n",
    "    return clean_path\n",
    "\n",
    "def gui_alpha_to_internal(gui_val):\n",
    "    # Map 0.0-1.0 to 0.0-0.5 roughly\n",
    "    return float(gui_val) * 0.5\n",
    "\n",
    "print('‚úì Utilities & Helpers Ready: list_emotions, list_speakers, ensure_speaker_clean, gui_alpha_to_internal defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "111e93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # XTTS local download & loader (local-first)\n",
    "# from TTS.api import TTS\n",
    "# from huggingface_hub import snapshot_download\n",
    "# import shutil, os, traceback\n",
    "# from pathlib import Path\n",
    "\n",
    "# def ensure_xtts_local(target_dir: Path):\n",
    "#     target_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     ck = target_dir / 'model.pth'\n",
    "#     cfg = target_dir / 'config.json'\n",
    "#     if ck.exists() and cfg.exists():\n",
    "#         print('XTTS local present:', target_dir)\n",
    "#         return True\n",
    "#     print('Attempting snapshot_download of coqui/xtts-v2 into models folder (best-effort)...')\n",
    "#     try:\n",
    "#         tmp = snapshot_download(repo_id='coqui/xtts-v2', cache_dir=str(target_dir), repo_type='model', allow_patterns=['*'])\n",
    "#         print('snapshot_download result:', tmp)\n",
    "#     except Exception as e:\n",
    "#         print('snapshot_download failed (this is OK if huggingface auth required). Error:', e)\n",
    "#         traceback.print_exc()\n",
    "#     ck = target_dir / 'model.pth'\n",
    "#     cfg = target_dir / 'config.json'\n",
    "#     if ck.exists() and cfg.exists():\n",
    "#         return True\n",
    "#     print('XTTS not available locally. You can allow TTS to download to cache once, then move folder to models/xtts_v2.')\n",
    "#     return False\n",
    "\n",
    "# def load_xtts_local_or_remote(gpu=False):\n",
    "#     ok = ensure_xtts_local(Path('models') / 'xtts_v2')\n",
    "#     try:\n",
    "#         if ok:\n",
    "#             print('Trying to load XTTS from local models/xtts_v2 ...')\n",
    "#             t = TTS(model_path=str(Path('models') / 'xtts_v2' / 'model.pth'),\n",
    "#                     config_path=str(Path('models') / 'xtts_v2' / 'config.json'),\n",
    "#                     gpu=gpu)\n",
    "#             print('Loaded XTTS from local files.')\n",
    "#             return t\n",
    "#     except Exception as e:\n",
    "#         print('Failed to load local XTTS (will try model_name). Error:', e)\n",
    "#         traceback.print_exc()\n",
    "#     print('Loading XTTS via model_name (this will download to user cache if not present)...')\n",
    "#     t = TTS(model_name='tts_models/multilingual/multi-dataset/xtts_v2', gpu=gpu)\n",
    "#     print('XTTS loaded via model_name.')\n",
    "#     return t\n",
    "\n",
    "# # Load XTTS (CPU first)\n",
    "# XTTS = None\n",
    "# try:\n",
    "#     XTTS = load_xtts_local_or_remote(gpu=False)\n",
    "# except Exception as e:\n",
    "#     print('XTTS load error:', e)\n",
    "#     import traceback; traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c26e46af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading XTTS via model_name (this will download to user cache if not present)...\n",
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n",
      "XTTS loaded via model_name.\n"
     ]
    }
   ],
   "source": [
    "# XTTS local download & loader (local-first)\n",
    "from TTS.api import TTS\n",
    "from huggingface_hub import snapshot_download\n",
    "import shutil, os, traceback\n",
    "from pathlib import Path\n",
    "def ensure_xtts_local(target_dir: Path):\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ck = target_dir / 'model.pth'\n",
    "    cfg = target_dir / 'config.json'\n",
    "    if ck.exists() and cfg.exists():\n",
    "        print('XTTS local present:', target_dir)\n",
    "        return True\n",
    "    print('Attempting snapshot_download of coqui/xtts-v2 into models folder (best-effort)...')\n",
    "    try:\n",
    "        tmp = snapshot_download(repo_id='coqui/xtts-v2', cache_dir=str(target_dir), repo_type='model', allow_patterns=['*'])\n",
    "        print('snapshot_download result:', tmp)\n",
    "    except Exception as e:\n",
    "        print('snapshot_download failed (this is OK if huggingface auth required). Error:', e)\n",
    "        traceback.print_exc()\n",
    "    ck = target_dir / 'model.pth'\n",
    "    cfg = target_dir / 'config.json'\n",
    "    if ck.exists() and cfg.exists():\n",
    "        return True\n",
    "    print('XTTS not available locally. You can allow TTS to download to cache once, then move folder to models/xtts_v2.')\n",
    "    return False\n",
    "def load_xtts_local_or_remote(gpu=False):\n",
    "    # DIRECT LOAD: Skipping local check and going straight to model_name\n",
    "    print('Loading XTTS via model_name (this will download to user cache if not present)...')\n",
    "    t = TTS(model_name='tts_models/multilingual/multi-dataset/xtts_v2', gpu=gpu)\n",
    "    print('XTTS loaded via model_name.')\n",
    "    return t\n",
    "# Load XTTS (CPU first)\n",
    "XTTS = None\n",
    "try:\n",
    "    XTTS = load_xtts_local_or_remote(gpu=False)\n",
    "except Exception as e:\n",
    "    print('XTTS load error:', e)\n",
    "    import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeb58b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Indic wav2vec from: models\\ai4bharat_indicwav2vec_hindi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at models\\ai4bharat_indicwav2vec_hindi and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indic encoder loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load ai4bharat/indicwav2vec-hindi from local models folder\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "INDIC_LOCAL = Path('models') / 'ai4bharat_indicwav2vec_hindi'\n",
    "if not INDIC_LOCAL.exists():\n",
    "    print('Warning: Indic model folder not found at:', INDIC_LOCAL)\n",
    "else:\n",
    "    print('Loading Indic wav2vec from:', INDIC_LOCAL)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(str(INDIC_LOCAL))\n",
    "    indic_enc = Wav2Vec2Model.from_pretrained(str(INDIC_LOCAL))\n",
    "    indic_enc.eval()\n",
    "    print('Indic encoder loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04271c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers ready (UPDATED): resolve_xtts_internal_model, get_indic_embedding, get_xtts_speaker_latent (GPT target)\n"
     ]
    }
   ],
   "source": [
    "# Embedding helpers: safe XTTS resolver and embedding extraction (UPDATED for GPT Latent)\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_xtts_internal_model(tts_obj):\n",
    "    '''Return the internal XTTS model used by TTS wrapper.'''\n",
    "    if tts_obj is None:\n",
    "        raise RuntimeError('Provided tts_obj is None')\n",
    "    if hasattr(tts_obj, 'synthesizer') and hasattr(tts_obj.synthesizer, 'tts_model'):\n",
    "        return tts_obj.synthesizer.tts_model\n",
    "    if hasattr(tts_obj, 'tts_model'):\n",
    "        return tts_obj.tts_model\n",
    "    raise RuntimeError('Could not resolve internal XTTS model. Ensure you loaded native XTTS-v2 via TTS API.')\n",
    "\n",
    "def get_indic_embedding(wav_path, sr_source=SR_XTTS, sr_indic=SR_INDIC):\n",
    "    '''Load wav, resample to sr_indic if needed, and return 1D numpy embedding (mean of last_hidden_state).'''\n",
    "    global processor, indic_enc\n",
    "    if 'processor' not in globals() or 'indic_enc' not in globals():\n",
    "        raise RuntimeError('Indic encoder not loaded. Run the Indic load cell.')\n",
    "    y, sr = librosa.load(str(wav_path), sr=sr_source, mono=True)\n",
    "    if sr != sr_indic:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=sr_indic)\n",
    "    inp = processor(y, sampling_rate=sr_indic, return_tensors='pt', padding=True)\n",
    "    with torch.no_grad():\n",
    "        out = indic_enc(**inp).last_hidden_state\n",
    "    emb = out.mean(dim=1).squeeze().detach().cpu().numpy()\n",
    "    return emb\n",
    "\n",
    "def get_xtts_speaker_latent(tts_obj, wav_path, load_sr=SR_XTTS):\n",
    "    '''Extract GPT conditioning latent (prosody) from XTTS.\n",
    "    Returns flattened numpy array (approx 32k dims).'''\n",
    "    model = resolve_xtts_internal_model(tts_obj)\n",
    "    try:\n",
    "        res = model.get_conditioning_latents(str(wav_path), load_sr=load_sr)\n",
    "    except TypeError:\n",
    "        res = model.get_conditioning_latents(str(wav_path))\n",
    "        \n",
    "    if isinstance(res, (list, tuple)) and len(res) >= 2:\n",
    "        # res[0] is gpt_cond_latent (1, 32, 1024) -> flatten to (32768,)\n",
    "        # res[1] is speaker_embedding (1, 512, 1)\n",
    "        gpt_lat = res[0]\n",
    "    else:\n",
    "        # Fallback if structure is different\n",
    "        gpt_lat = res\n",
    "        \n",
    "    try:\n",
    "        sp = gpt_lat.cpu().numpy() if hasattr(gpt_lat, 'cpu') else np.array(gpt_lat)\n",
    "        return sp.ravel() # Flatten to 1D (~32768)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError('Failed to convert GPT latent to numpy: ' + str(e))\n",
    "\n",
    "print('Helpers ready (UPDATED): resolve_xtts_internal_model, get_indic_embedding, get_xtts_speaker_latent (GPT target)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "659b0997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function patched in memory. Try synthesizing now!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell once to fix the function in memory!\n",
    "def compute_emotion_vector_xtts_multi(emotion_dir, method='cca', n_comp=32, mode='average', sample_id=1,\n",
    "                                      save_single_dir=None, save_avg_dir=None):\n",
    "    # FIX: Import numpy specifically at the top to avoid UnboundLocalError scopes\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    \n",
    "    emotion_dir = Path(emotion_dir)\n",
    "    emotion_name = emotion_dir.name\n",
    "    \n",
    "    # CACHE CHECK: If mode is average and save_avg_dir is provided\n",
    "    if mode == 'average' and save_avg_dir:\n",
    "        save_avg_dir = Path(save_avg_dir)\n",
    "        save_avg_dir.mkdir(parents=True, exist_ok=True)\n",
    "        avg_file_name = f\"{emotion_name}_avg_{method}.npy\"\n",
    "        avg_file = save_avg_dir / avg_file_name\n",
    "        \n",
    "        if avg_file.exists():\n",
    "            print(f'‚ö° Loading cached average vector for {emotion_name}...')\n",
    "            return np.load(avg_file)\n",
    "            \n",
    "    sample_dirs = [d for d in sorted(emotion_dir.iterdir()) if d.is_dir()]\n",
    "    if len(sample_dirs) == 0:\n",
    "        raise ValueError('No sample subfolders found in: ' + str(emotion_dir))\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    single_vectors = []\n",
    "\n",
    "    for sd in sample_dirs:\n",
    "        n_clean = sd / 'neutral_clean.wav'\n",
    "        e_clean = sd / f'{emotion_name}_clean.wav'\n",
    "        \n",
    "        if not (n_clean.exists() and e_clean.exists()):\n",
    "             continue\n",
    "\n",
    "        xi = get_indic_embedding(n_clean, sr_source=SR_XTTS, sr_indic=SR_INDIC)\n",
    "        xe = get_indic_embedding(e_clean, sr_source=SR_XTTS, sr_indic=SR_INDIC)\n",
    "        yi = get_xtts_speaker_latent(XTTS, n_clean, load_sr=SR_XTTS)\n",
    "        ye = get_xtts_speaker_latent(XTTS, e_clean, load_sr=SR_XTTS)\n",
    "        \n",
    "        # Store deltas\n",
    "        X.append(xe - xi)\n",
    "        Y.append(ye - yi)\n",
    "        single_vectors.append((sd.name, xe - xi, ye - yi))\n",
    "        \n",
    "    if len(X) == 0:\n",
    "        raise ValueError('No matched pairs extracted for emotion: ' + str(emotion_dir))\n",
    "\n",
    "    X = np.stack(X)\n",
    "    Y = np.stack(Y)\n",
    "    \n",
    "    # Check dimensions\n",
    "    dim_y = Y.shape[1] \n",
    "    if dim_y > 1024 and method != 'xtts_native':\n",
    "        print(f'‚ö†Ô∏è Detected high-dimensional latent ({dim_y} dims). Forcing method=\"xtts_native\".')\n",
    "        method = 'xtts_native'\n",
    "\n",
    "    if len(X) < 5 and method != 'xtts_native':\n",
    "        print(f'‚ö†Ô∏è Only {len(X)} samples; Switch to \"xtts_native\" for stability.')\n",
    "        method = 'xtts_native'\n",
    "\n",
    "    result_vec = None\n",
    "    \n",
    "    if method == 'xtts_native':\n",
    "        if mode == 'single':\n",
    "            idx = sample_id - 1\n",
    "            result_vec = single_vectors[idx][2]\n",
    "        else:\n",
    "            # Average raw emotion deltas\n",
    "            result_vec = np.mean([v for (_,_,v) in single_vectors], axis=0)\n",
    "            \n",
    "    else:\n",
    "        # CCA/PLS logic\n",
    "        max_comp = min(X.shape[0], X.shape[1], Y.shape[1])\n",
    "        actual_n_comp = min(n_comp, max_comp)\n",
    "        mapper = fit_cca_or_pls(X, Y, method=method, n_comp=actual_n_comp)\n",
    "        v_indic = np.mean([xi for (_,xi,_) in single_vectors], axis=0)\n",
    "        v_indic = v_indic / (np.linalg.norm(v_indic) + 1e-12)\n",
    "        result_vec = map_indic_vector_to_xtts(mapper, v_indic)\n",
    "\n",
    "    # CACHE SAVE: If mode is average and save_avg_dir is provided\n",
    "    if mode == 'average' and save_avg_dir and result_vec is not None:\n",
    "        save_avg_dir = Path(save_avg_dir)\n",
    "        save_avg_dir.mkdir(parents=True, exist_ok=True)\n",
    "        avg_file_name = f\"{emotion_name}_avg_{method}.npy\"\n",
    "        print(f'üíæ Saving average vector to {save_avg_dir / avg_file_name}...')\n",
    "        np.save(save_avg_dir / avg_file_name, result_vec)\n",
    "\n",
    "    return result_vec\n",
    "print(\"Function patched in memory. Try synthesizing now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0332e85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì apply_emotion_and_synthesize() FIXED v2 (Direct + LangPatch)\n"
     ]
    }
   ],
   "source": [
    "# [FIXED v2] Apply emotion vector using DIRECT INFERENCE + LANGUAGE FIX\n",
    "# Handles KeyError: 'hi' by patching char_limits or disabling splitting.\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "\n",
    "OUTPUT_GEN_DIR = PROJECT_ROOT / 'data' / 'outputs' / 'generated'\n",
    "OUTPUT_GEN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def apply_emotion_and_synthesize(text, speaker_wav, emotion_vec, alpha=0.1, out_path=None, language='hi', scale_to_speaker=False):\n",
    "    if out_path is None:\n",
    "        out_path = OUTPUT_GEN_DIR / 'test_hindi_emotional.wav'\n",
    "    out_path = unique_path(Path(out_path))\n",
    "\n",
    "    # 1. Resolve Model\n",
    "    model = resolve_xtts_internal_model(XTTS)\n",
    "    \n",
    "    # 2. Get Base Latents\n",
    "    print(f'Extracting base latents from {Path(speaker_wav).name}...')\n",
    "    latents = model.get_conditioning_latents(str(speaker_wav), load_sr=SR_XTTS)\n",
    "    gpt_cond_latent = latents[0]\n",
    "    speaker_embedding = latents[1]\n",
    "    \n",
    "    # 3. Prepare Emotion Vector\n",
    "    ev = np.asarray(emotion_vec).astype(np.float32)\n",
    "    \n",
    "    # 4. Modify GPT Latent (High-Dim)\n",
    "    if ev.size > 2000:\n",
    "        ev_tensor = torch.tensor(ev).float().reshape(1, 32, 1024).to(gpt_cond_latent.device)\n",
    "        \n",
    "        # Diagnostic output\n",
    "        base_norm = torch.norm(gpt_cond_latent).item()\n",
    "        delta_norm = torch.norm(ev_tensor).item()\n",
    "        ratio = delta_norm / (base_norm + 1e-9)\n",
    "        print(f\"[Direct] Base Norm: {base_norm:.2f}, Delta Norm: {delta_norm:.2f}, Ratio: {ratio:.2%}\")\n",
    "        \n",
    "        # Apply modification\n",
    "        new_gpt_cond = gpt_cond_latent + alpha * ev_tensor\n",
    "        print(f\" -> Modified GPT latent with alpha={alpha}\")\n",
    "    else:\n",
    "        print(\"Warning: Low-dim vector ignored in this fixed version.\")\n",
    "        new_gpt_cond = gpt_cond_latent\n",
    "\n",
    "    # 5. LANGUAGE SUPPORT FIX\n",
    "    # Ensure tokenizer has limits for the language to avoid KeyError\n",
    "    if hasattr(model, 'tokenizer') and hasattr(model.tokenizer, 'char_limits'):\n",
    "        if language not in model.tokenizer.char_limits:\n",
    "            print(f\"Patching missing char_limit for '{language}'...\")\n",
    "            model.tokenizer.char_limits[language] = 200 # Default safe limit\n",
    "\n",
    "    # 6. DIRECT INFERENCE\n",
    "    print(\"Synthesizing via direct model.inference()...\")\n",
    "    try:\n",
    "        out = model.inference(\n",
    "            text=text,\n",
    "            language=language,\n",
    "            gpt_cond_latent=new_gpt_cond,\n",
    "            speaker_embedding=speaker_embedding,\n",
    "            temperature=0.7,\n",
    "            length_penalty=1.0,\n",
    "            repetition_penalty=2.0,\n",
    "            top_k=50,\n",
    "            top_p=0.8,\n",
    "            enable_text_splitting=True\n",
    "        )\n",
    "        \n",
    "        # 7. Save Output\n",
    "        wav = out['wav']\n",
    "        sf.write(str(out_path), wav, 24000)\n",
    "        print(f'‚úì Synthesis complete -> {out_path}')\n",
    "        return out_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Direct synthesis failed: {e}')\n",
    "        raise RuntimeError(str(e))\n",
    "\n",
    "print('‚úì apply_emotion_and_synthesize() FIXED v2 (Direct + LangPatch)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade7f77",
   "metadata": {},
   "source": [
    "## Samples Batch Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7efc1b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing functions ready: preprocess_all(emotion_names=[...])\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: Batch clean all emotion and speaker samples\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "AUDIO_EXTS = ['.wav', '.mp3', '.m4a', '.flac']\n",
    "\n",
    "def clean_emotion_samples(emotion_dir, sr=SR_XTTS):\n",
    "    \"\"\"Clean all audio files in emotion sample folders.\n",
    "    \n",
    "    For each sample folder under emotion_dir:\n",
    "    - Finds raw audio files (neutral and emotion)\n",
    "    - Creates *_clean.wav versions using preprocess_audio\n",
    "    - Skips if cleaned version already exists\n",
    "    \n",
    "    Returns count of cleaned files.\n",
    "    \"\"\"\n",
    "    emotion_dir = Path(emotion_dir)\n",
    "    emotion_name = emotion_dir.name\n",
    "    sample_dirs = sorted([d for d in emotion_dir.iterdir() if d.is_dir()])\n",
    "    \n",
    "    if not sample_dirs:\n",
    "        print(f'No sample folders found in {emotion_dir}')\n",
    "        return 0\n",
    "    \n",
    "    cleaned_count = 0\n",
    "    print(f'\\nüéµ Cleaning emotion samples for \"{emotion_name}\" ({len(sample_dirs)} samples)')\n",
    "    print('‚îÄ' * 70)\n",
    "    \n",
    "    for sd in sample_dirs:\n",
    "        # Find raw audio files\n",
    "        raw_files = sorted([f for f in sd.iterdir() if f.suffix.lower() in AUDIO_EXTS])\n",
    "        if not raw_files:\n",
    "            print(f'  {sd.name}: ‚ö†Ô∏è No audio files found')\n",
    "            continue\n",
    "        \n",
    "        # Match neutral and emotion files\n",
    "        neutral_raw = None\n",
    "        emotion_raw = None\n",
    "        \n",
    "        for f in raw_files:\n",
    "            f_lower = f.stem.lower()\n",
    "            if 'neutral' in f_lower:\n",
    "                neutral_raw = f\n",
    "            elif emotion_name.lower() in f_lower:\n",
    "                emotion_raw = f\n",
    "        \n",
    "        # Fallback to first two files if no match\n",
    "        if neutral_raw is None or emotion_raw is None:\n",
    "            neutral_raw = raw_files[0]\n",
    "            emotion_raw = raw_files[1] if len(raw_files) > 1 else raw_files[0]\n",
    "        \n",
    "        # Clean neutral file\n",
    "        n_clean = sd / 'neutral_clean.wav'\n",
    "        if not n_clean.exists():\n",
    "            try:\n",
    "                preprocess_audio(neutral_raw, n_clean, sr=sr)\n",
    "                cleaned_count += 1\n",
    "                print(f'  {sd.name}: ‚úì neutral_clean.wav')\n",
    "            except Exception as e:\n",
    "                print(f'  {sd.name}: ‚úó neutral failed - {str(e)[:40]}')\n",
    "        else:\n",
    "            print(f'  {sd.name}: ‚äò neutral_clean.wav (exists)')\n",
    "        \n",
    "        # Clean emotion file\n",
    "        e_clean = sd / f'{emotion_name}_clean.wav'\n",
    "        if not e_clean.exists():\n",
    "            try:\n",
    "                preprocess_audio(emotion_raw, e_clean, sr=sr)\n",
    "                cleaned_count += 1\n",
    "                print(f'  {sd.name}: ‚úì {emotion_name}_clean.wav')\n",
    "            except Exception as e:\n",
    "                print(f'  {sd.name}: ‚úó {emotion_name} failed - {str(e)[:40]}')\n",
    "        else:\n",
    "            print(f'  {sd.name}: ‚äò {emotion_name}_clean.wav (exists)')\n",
    "    \n",
    "    print('‚îÄ' * 70)\n",
    "    print(f'‚úì Emotion cleaning complete: {cleaned_count} new files created\\n')\n",
    "    return cleaned_count\n",
    "\n",
    "\n",
    "def clean_speaker_samples(sr=SR_XTTS):\n",
    "    \"\"\"Clean all speaker audio files in data/speakers.\n",
    "    \n",
    "    For each supported audio file:\n",
    "    - Creates {stem}_clean.wav if it doesn't exist\n",
    "    - Skips if already cleaned\n",
    "    \n",
    "    Returns count of cleaned files.\n",
    "    \"\"\"\n",
    "    sp_dir = PROJECT_ROOT / 'data' / 'speakers'\n",
    "    sp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    raw_files = sorted([f for f in sp_dir.iterdir() if f.suffix.lower() in AUDIO_EXTS and f.is_file()])\n",
    "    \n",
    "    if not raw_files:\n",
    "        print('No speaker files found in data/speakers')\n",
    "        return 0\n",
    "    \n",
    "    cleaned_count = 0\n",
    "    print(f'\\nüé§ Cleaning speaker samples ({len(raw_files)} files)')\n",
    "    print('‚îÄ' * 70)\n",
    "    \n",
    "    for f in raw_files:\n",
    "        # Skip if already a clean file\n",
    "        if f.stem.endswith('_clean'):\n",
    "            print(f'  {f.name}: ‚äò (already clean)')\n",
    "            continue\n",
    "        \n",
    "        clean_name = f.with_name(f.stem + '_clean.wav')\n",
    "        if clean_name.exists():\n",
    "            print(f'  {f.name}: ‚äò {clean_name.name} (exists)')\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            preprocess_audio(f, clean_name, sr=sr)\n",
    "            cleaned_count += 1\n",
    "            print(f'  {f.name}: ‚úì {clean_name.name}')\n",
    "        except Exception as e:\n",
    "            print(f'  {f.name}: ‚úó failed - {str(e)[:40]}')\n",
    "    \n",
    "    print('‚îÄ' * 70)\n",
    "    print(f'‚úì Speaker cleaning complete: {cleaned_count} new files created\\n')\n",
    "    return cleaned_count\n",
    "\n",
    "\n",
    "def preprocess_all(emotion_names=None):\n",
    "    \"\"\"Batch clean all emotion and speaker samples.\n",
    "    \n",
    "    Args:\n",
    "        emotion_names: list of emotion folder names to clean. If None, clean all.\n",
    "    \n",
    "    Returns: dict with cleaning stats.\n",
    "    \"\"\"\n",
    "    emotion_dir = PROJECT_ROOT / 'data' / 'emotion_samples'\n",
    "    emotion_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if emotion_names is None:\n",
    "        emotion_names = [d.name for d in emotion_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    stats = {'emotion': {}, 'speaker': 0}\n",
    "    \n",
    "    print('\\n' + '=' * 70)\n",
    "    print('BATCH PREPROCESSING: EMOTION + SPEAKER SAMPLES')\n",
    "    print('=' * 70)\n",
    "    \n",
    "    for emotion in emotion_names:\n",
    "        ed = emotion_dir / emotion\n",
    "        if ed.exists():\n",
    "            count = clean_emotion_samples(ed)\n",
    "            stats['emotion'][emotion] = count\n",
    "    \n",
    "    stats['speaker'] = clean_speaker_samples()\n",
    "    \n",
    "    print('=' * 70)\n",
    "    print('SUMMARY:')\n",
    "    for emotion, count in stats['emotion'].items():\n",
    "        print(f'  {emotion}: {count} files cleaned')\n",
    "    print(f'  speakers: {stats[\"speaker\"]} files cleaned')\n",
    "    print('=' * 70 + '\\n')\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "print('Preprocessing functions ready: preprocess_all(emotion_names=[...])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe45fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fcb0a950625405e9767e688f5bd28d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<b>Select Emotions to Clean:</b>'), Checkbox(value=True, description‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Preprocessing GUI ready. Use buttons above to clean samples.\n"
     ]
    }
   ],
   "source": [
    "# Interactive Preprocessing GUI\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Get available emotions\n",
    "emotion_dir = PROJECT_ROOT / 'data' / 'emotion_samples'\n",
    "available_emotions = sorted([d.name for d in emotion_dir.iterdir() if d.is_dir()])\n",
    "\n",
    "# Create checkboxes for emotion selection\n",
    "emotion_checkboxes = {\n",
    "    emotion: widgets.Checkbox(value=True, description=emotion, indent=False)\n",
    "    for emotion in available_emotions\n",
    "}\n",
    "\n",
    "# Output area\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_clean_emotions(button):\n",
    "    \"\"\"Clean selected emotion samples.\"\"\"\n",
    "    with output:\n",
    "        clear_output()\n",
    "        selected = [e for e, cb in emotion_checkboxes.items() if cb.value]\n",
    "        if not selected:\n",
    "            print('‚ö†Ô∏è No emotions selected')\n",
    "            return\n",
    "        preprocess_all(emotion_names=selected)\n",
    "\n",
    "def on_clean_speakers(button):\n",
    "    \"\"\"Clean speaker samples.\"\"\"\n",
    "    with output:\n",
    "        clear_output()\n",
    "        clean_speaker_samples()\n",
    "\n",
    "def on_clean_all(button):\n",
    "    \"\"\"Clean all emotion and speaker samples.\"\"\"\n",
    "    with output:\n",
    "        clear_output()\n",
    "        preprocess_all()\n",
    "\n",
    "# Create buttons\n",
    "btn_emotions = widgets.Button(description='üéµ Clean Selected Emotions', button_style='info', tooltip='Clean checked emotion samples')\n",
    "btn_speakers = widgets.Button(description='üé§ Clean Speaker Samples', button_style='warning', tooltip='Clean speaker audio files')\n",
    "btn_all = widgets.Button(description='üîÑ Clean All Samples', button_style='danger', tooltip='Clean all emotions and speakers')\n",
    "\n",
    "# Attach callbacks\n",
    "btn_emotions.on_click(on_clean_emotions)\n",
    "btn_speakers.on_click(on_clean_speakers)\n",
    "btn_all.on_click(on_clean_all)\n",
    "\n",
    "# Layout\n",
    "emotion_box = widgets.VBox(\n",
    "    [widgets.HTML('<b>Select Emotions to Clean:</b>')] + \n",
    "    [emotion_checkboxes[e] for e in available_emotions],\n",
    "    layout=widgets.Layout(border='1px solid #ccc', padding='10px', margin='10px 0')\n",
    ")\n",
    "\n",
    "button_box = widgets.HBox([btn_emotions, btn_speakers, btn_all], layout=widgets.Layout(margin='10px 0'))\n",
    "\n",
    "panel = widgets.VBox([emotion_box, button_box, output])\n",
    "display(panel)\n",
    "\n",
    "print('‚úì Preprocessing GUI ready. Use buttons above to clean samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6c93326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä CLEANED SAMPLES INVENTORY\n",
      "======================================================================\n",
      "\n",
      "‚úì ANGRY:\n",
      "   Ready for visualization: 4 samples (with both neutral + emotion clean files)\n",
      "\n",
      "‚úì DEMO:\n",
      "   Ready for visualization: 2 samples (with both neutral + emotion clean files)\n",
      "\n",
      "‚úì HAPPY:\n",
      "   Ready for visualization: 4 samples (with both neutral + emotion clean files)\n",
      "\n",
      "‚úì SAD:\n",
      "   Ready for visualization: 4 samples (with both neutral + emotion clean files)\n",
      "\n",
      "‚úì SINGING:\n",
      "   Ready for visualization: 1 samples (with both neutral + emotion clean files)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üé§ SPEAKER SAMPLES:\n",
      "   Total files: 3\n",
      "   Cleaned files: 3\n",
      "      ‚úì amir_khan_clean.wav (687.4 KB)\n",
      "      ‚úì character_1_clean.wav (281.4 KB)\n",
      "      ‚úì lata_mangeskar_clean.wav (626.6 KB)\n",
      "\n",
      "======================================================================\n",
      "Summary: 15 emotion samples ready\n",
      "======================================================================\n",
      "\n",
      "Tip: Run \"Clean All Samples\" or \"Clean Selected Emotions\" above to prepare files.\n"
     ]
    }
   ],
   "source": [
    "# Sample Stats: View cleaned files and their properties\n",
    "from pathlib import Path\n",
    "\n",
    "def show_sample_stats():\n",
    "    \"\"\"Display stats about cleaned samples and speakers.\"\"\"\n",
    "    emotion_dir = PROJECT_ROOT / 'data' / 'emotion_samples'\n",
    "    speaker_dir = PROJECT_ROOT / 'data' / 'speakers'\n",
    "    \n",
    "    print('\\n' + '=' * 70)\n",
    "    print('üìä CLEANED SAMPLES INVENTORY')\n",
    "    print('=' * 70)\n",
    "    \n",
    "    # Check emotions\n",
    "    total_emotion_samples = 0\n",
    "    total_emotion_files = 0\n",
    "    for emotion_folder in sorted(emotion_dir.iterdir()):\n",
    "        if not emotion_folder.is_dir():\n",
    "            continue\n",
    "        emotion_name = emotion_folder.name\n",
    "        samples_with_both = 0\n",
    "        samples_with_partial = 0\n",
    "        \n",
    "        for sample_folder in sorted(emotion_folder.iterdir()):\n",
    "            if not sample_folder.is_dir():\n",
    "                continue\n",
    "            n_clean = sample_folder / 'neutral_clean.wav'\n",
    "            e_clean = sample_folder / f'{emotion_name}_clean.wav'\n",
    "            \n",
    "            if n_clean.exists() and e_clean.exists():\n",
    "                samples_with_both += 1\n",
    "                total_emotion_files += 2\n",
    "            elif n_clean.exists() or e_clean.exists():\n",
    "                samples_with_partial += 1\n",
    "        \n",
    "        total_emotion_samples += samples_with_both\n",
    "        \n",
    "        status = '‚úì' if samples_with_both > 0 else '‚úó'\n",
    "        print(f'\\n{status} {emotion_name.upper()}:')\n",
    "        print(f'   Ready for visualization: {samples_with_both} samples (with both neutral + emotion clean files)')\n",
    "        if samples_with_partial > 0:\n",
    "            print(f'   Partial: {samples_with_partial} samples (only one file cleaned)')\n",
    "    \n",
    "    # Check speakers\n",
    "    print(f'\\n{\"‚îÄ\" * 70}')\n",
    "    print('\\nüé§ SPEAKER SAMPLES:')\n",
    "    if speaker_dir.exists():\n",
    "        all_files = list(speaker_dir.glob('*.wav')) + list(speaker_dir.glob('*.mp3')) + list(speaker_dir.glob('*.m4a')) + list(speaker_dir.glob('*.flac'))\n",
    "        clean_files = list(speaker_dir.glob('*_clean.wav'))\n",
    "        if all_files:\n",
    "            print(f'   Total files: {len(all_files)}')\n",
    "            print(f'   Cleaned files: {len(clean_files)}')\n",
    "            for cf in sorted(clean_files):\n",
    "                size_kb = cf.stat().st_size / 1024\n",
    "                print(f'      ‚úì {cf.name} ({size_kb:.1f} KB)')\n",
    "        else:\n",
    "            print('   No speaker files found')\n",
    "    else:\n",
    "        print('   Directory not found')\n",
    "    \n",
    "    print('\\n' + '=' * 70)\n",
    "    print(f'Summary: {total_emotion_samples} emotion samples ready')\n",
    "    print('=' * 70 + '\\n')\n",
    "\n",
    "\n",
    "# Display stats\n",
    "show_sample_stats()\n",
    "print('Tip: Run \"Clean All Samples\" or \"Clean Selected Emotions\" above to prepare files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f02992b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ece3a56f28414d87ec1f37d9c3c10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Select emotion and click Plot:</b>'), HBox(children=(Dropdown(description='Emoti‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Interactive emotion sample plotter ready\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_emotion_samples_pca(emotion_name):\n",
    "    \"\"\"Plot emotion samples in 2D PCA space showing neutral vs emotion shifts.\n",
    "    \n",
    "    Args:\n",
    "        emotion_name: name of emotion folder (e.g., 'happy', 'sad')\n",
    "    \"\"\"\n",
    "    emotion_dir = PROJECT_ROOT / 'data' / 'emotion_samples' / emotion_name\n",
    "    \n",
    "    if not emotion_dir.exists():\n",
    "        print(f'Emotion folder not found: {emotion_dir}')\n",
    "        return\n",
    "    \n",
    "    neutral_vecs = []\n",
    "    emotion_vecs = []\n",
    "    sample_names = []\n",
    "    \n",
    "    sample_dirs = sorted([d for d in emotion_dir.iterdir() if d.is_dir()])\n",
    "    \n",
    "    print(f'üéµ Loading {emotion_name} samples...')\n",
    "    \n",
    "    for sd in sample_dirs:\n",
    "        n_clean = sd / 'neutral_clean.wav'\n",
    "        e_clean = sd / f'{emotion_name}_clean.wav'\n",
    "        \n",
    "        # Skip if clean files don't exist\n",
    "        if not (n_clean.exists() and e_clean.exists()):\n",
    "            print(f'  ‚ö†Ô∏è {sd.name}: missing clean files, skipping')\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Extract speaker latents\n",
    "            n_emb = get_xtts_speaker_latent(XTTS, n_clean, load_sr=SR_XTTS)\n",
    "            e_emb = get_xtts_speaker_latent(XTTS, e_clean, load_sr=SR_XTTS)\n",
    "            \n",
    "            neutral_vecs.append(n_emb)\n",
    "            emotion_vecs.append(e_emb)\n",
    "            sample_names.append(sd.name)\n",
    "            print(f'  ‚úì {sd.name}')\n",
    "        except Exception as e:\n",
    "            print(f'  ‚úó {sd.name}: {str(e)[:40]}')\n",
    "    \n",
    "    if len(neutral_vecs) == 0:\n",
    "        print(f'No valid samples found in {emotion_dir}')\n",
    "        return\n",
    "    \n",
    "    neutral_vecs = np.array(neutral_vecs)  # shape: (n_samples, 512)\n",
    "    emotion_vecs = np.array(emotion_vecs)  # shape: (n_samples, 512)\n",
    "    \n",
    "    # Compute averages\n",
    "    neutral_avg = neutral_vecs.mean(axis=0)  # (512,)\n",
    "    emotion_avg = emotion_vecs.mean(axis=0)  # (512,)\n",
    "    \n",
    "    # Stack all vectors for PCA\n",
    "    all_vecs = np.vstack([neutral_vecs, emotion_vecs, neutral_avg.reshape(1, -1), emotion_avg.reshape(1, -1)])\n",
    "    \n",
    "    # Apply PCA to 2D\n",
    "    pca = PCA(n_components=2)\n",
    "    vecs_2d = pca.fit_transform(all_vecs)\n",
    "    \n",
    "    # Split back\n",
    "    n_samples = len(neutral_vecs)\n",
    "    neutral_2d = vecs_2d[:n_samples]\n",
    "    emotion_2d = vecs_2d[n_samples:2*n_samples]\n",
    "    neutral_avg_2d = vecs_2d[2*n_samples]\n",
    "    emotion_avg_2d = vecs_2d[2*n_samples + 1]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot individual samples\n",
    "    plt.scatter(neutral_2d[:, 0], neutral_2d[:, 1], c='blue', s=100, alpha=0.6, \n",
    "               label='Neutral samples', edgecolors='darkblue', linewidth=1.5)\n",
    "    plt.scatter(emotion_2d[:, 0], emotion_2d[:, 1], c='red', s=100, alpha=0.6, \n",
    "               label=f'{emotion_name.capitalize()} samples', edgecolors='darkred', linewidth=1.5)\n",
    "    \n",
    "    # Annotate sample names\n",
    "    for i, name in enumerate(sample_names):\n",
    "        plt.annotate(name, (neutral_2d[i, 0], neutral_2d[i, 1]), \n",
    "                    fontsize=8, alpha=0.7, xytext=(5, 5), textcoords='offset points')\n",
    "        plt.annotate(name, (emotion_2d[i, 0], emotion_2d[i, 1]), \n",
    "                    fontsize=8, alpha=0.7, xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    # Plot averages (larger markers)\n",
    "    plt.scatter(neutral_avg_2d[0], neutral_avg_2d[1], c='blue', s=400, marker='X', \n",
    "               edgecolors='darkblue', linewidth=2, label='Neutral avg', zorder=10)\n",
    "    plt.scatter(emotion_avg_2d[0], emotion_avg_2d[1], c='red', s=400, marker='X', \n",
    "               edgecolors='darkred', linewidth=2, label=f'{emotion_name.capitalize()} avg', zorder=10)\n",
    "    \n",
    "    # Draw arrow from neutral to emotion average (emotion shift)\n",
    "    plt.arrow(neutral_avg_2d[0], neutral_avg_2d[1], \n",
    "             emotion_avg_2d[0] - neutral_avg_2d[0], \n",
    "             emotion_avg_2d[1] - neutral_avg_2d[1],\n",
    "             head_width=0.2, head_length=0.15, fc='green', ec='green', alpha=0.7, linewidth=2.5, zorder=5)\n",
    "    \n",
    "    # Labels and formatting\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    cumsum_var = np.cumsum(explained_var)\n",
    "    \n",
    "    plt.xlabel(f'PC1 ({explained_var[0]:.1%})', fontsize=12)\n",
    "    plt.ylabel(f'PC2 ({explained_var[1]:.1%})', fontsize=12)\n",
    "    plt.title(f'Emotion Vectors: {emotion_name.capitalize()}\\nPCA 2D Projection (Cumulative: {cumsum_var[1]:.1%})',\n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11, loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f'\\nüìä PCA Statistics:')\n",
    "    print(f'   PC1 variance: {explained_var[0]:.2%}')\n",
    "    print(f'   PC2 variance: {explained_var[1]:.2%}')\n",
    "    print(f'   Cumulative: {cumsum_var[1]:.2%}')\n",
    "    \n",
    "    print(f'\\nüìà Vector Statistics ({n_samples} samples):')\n",
    "    print(f'   Neutral avg norm: {np.linalg.norm(neutral_avg):.3f}')\n",
    "    print(f'   {emotion_name.capitalize()} avg norm: {np.linalg.norm(emotion_avg):.3f}')\n",
    "    emotion_diff = emotion_avg - neutral_avg\n",
    "    print(f'   Difference norm: {np.linalg.norm(emotion_diff):.3f}')\n",
    "    cosine_sim = np.dot(neutral_avg, emotion_avg) / (np.linalg.norm(neutral_avg) * np.linalg.norm(emotion_avg) + 1e-12)\n",
    "    print(f'   Cosine similarity: {cosine_sim:.3f}')\n",
    "\n",
    "\n",
    "# Interactive dropdown to select emotion and plot\n",
    "emotion_plot_dropdown = widgets.Dropdown(\n",
    "    options=list_emotions(),\n",
    "    description='Emotion:',\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "plot_button = widgets.Button(description='Plot Samples', button_style='info')\n",
    "plot_output = widgets.Output()\n",
    "\n",
    "def on_plot_clicked(b):\n",
    "    with plot_output:\n",
    "        clear_output()\n",
    "        emotion = emotion_plot_dropdown.value\n",
    "        if emotion == '(no emotions found)':\n",
    "            print('No emotions available. Run preprocessing first.')\n",
    "            return\n",
    "        plot_emotion_samples_pca(emotion)\n",
    "\n",
    "plot_button.on_click(on_plot_clicked)\n",
    "\n",
    "plot_panel = widgets.VBox([\n",
    "    widgets.HTML('<b>Select emotion and click Plot:</b>'),\n",
    "    widgets.HBox([emotion_plot_dropdown, plot_button]),\n",
    "    plot_output\n",
    "])\n",
    "\n",
    "display(plot_panel)\n",
    "print('‚úì Interactive emotion sample plotter ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6c53e",
   "metadata": {},
   "source": [
    "## Caching Latents to speed up repeated runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e1b1dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì apply_emotion_and_synthesize() updated with CACHING.\n"
     ]
    }
   ],
   "source": [
    "# [OPTIMIZED] Caching Latents to speed up inference\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "\n",
    "# Global Cache: Maps speaker_wav_path (str) -> (gpt_cond_latent, speaker_embedding)\n",
    "SPEAKER_CACHE = {}\n",
    "\n",
    "def get_cached_latents(model, speaker_wav):\n",
    "    \"\"\"Retrieve latents from cache or compute them once.\"\"\"\n",
    "    speaker_wav = str(speaker_wav)\n",
    "    \n",
    "    if speaker_wav in SPEAKER_CACHE:\n",
    "        # print(f\"DEBUG: Using cached latents for {Path(speaker_wav).name}\")\n",
    "        return SPEAKER_CACHE[speaker_wav]\n",
    "    \n",
    "    # Compute if not in cache\n",
    "    print(f\"Computing new latents for {Path(speaker_wav).name} ...\")\n",
    "    latents = model.get_conditioning_latents(speaker_wav, load_sr=SR_XTTS)\n",
    "    \n",
    "    # Store in cache\n",
    "    SPEAKER_CACHE[speaker_wav] = latents\n",
    "    return latents\n",
    "\n",
    "def apply_emotion_and_synthesize(text, speaker_wav, emotion_vec, alpha=0.1, out_path=None, language='hi'):\n",
    "    if out_path is None:\n",
    "        out_path = OUTPUT_GEN_DIR / 'test_hindi_emotional.wav'\n",
    "    out_path = unique_path(Path(out_path))\n",
    "\n",
    "    # 1. Resolve Model\n",
    "    model = resolve_xtts_internal_model(XTTS)\n",
    "    \n",
    "    # 2. Get Base Latents (FROM CACHE)\n",
    "    # This is the optimization!\n",
    "    latents = get_cached_latents(model, speaker_wav)\n",
    "    \n",
    "    # Clone to avoid modifying the cached version in place (very important!)\n",
    "    gpt_cond_latent = latents[0].clone().detach() \n",
    "    speaker_embedding = latents[1].clone().detach()\n",
    "    \n",
    "    # 3. Prepare Emotion Vector\n",
    "    ev = np.asarray(emotion_vec).astype(np.float32)\n",
    "    \n",
    "    # 4. Modify GPT Latent (High-Dim)\n",
    "    if ev.size > 2000:\n",
    "        ev_tensor = torch.tensor(ev).float().reshape(1, 32, 1024).to(gpt_cond_latent.device)\n",
    "        \n",
    "        # Apply modification\n",
    "        # We modify the clone, so the cache remains pure \"Neutral/Original\"\n",
    "        new_gpt_cond = gpt_cond_latent + alpha * ev_tensor\n",
    "    else:\n",
    "        # Fallback (shouldn't happen with xtts_native)\n",
    "        new_gpt_cond = gpt_cond_latent\n",
    "\n",
    "    # 5. LANGUAGE SUPPORT FIX\n",
    "    if hasattr(model, 'tokenizer') and hasattr(model.tokenizer, 'char_limits'):\n",
    "        if language not in model.tokenizer.char_limits:\n",
    "            model.tokenizer.char_limits[language] = 200 \n",
    "\n",
    "    # 6. DIRECT INFERENCE\n",
    "    # print(\"Synthesizing...\")\n",
    "    try:\n",
    "        out = model.inference(\n",
    "            text=text,\n",
    "            language=language,\n",
    "            gpt_cond_latent=new_gpt_cond,\n",
    "            speaker_embedding=speaker_embedding,\n",
    "            temperature=0.7,\n",
    "            length_penalty=1.0,\n",
    "            repetition_penalty=2.0,\n",
    "            top_k=50,\n",
    "            top_p=0.8,\n",
    "            enable_text_splitting=True\n",
    "        )\n",
    "        \n",
    "        # 7. Save Output\n",
    "        wav = out['wav']\n",
    "        sf.write(str(out_path), wav, 24000)\n",
    "        return out_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Direct synthesis failed: {e}')\n",
    "        raise RuntimeError(str(e))\n",
    "\n",
    "print(\"‚úì apply_emotion_and_synthesize() updated with CACHING.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd68253",
   "metadata": {},
   "source": [
    "## TTS GUI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131f85da",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608de72f",
   "metadata": {},
   "source": [
    "## 1. Shared State (Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b7e5fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Shared state initialized.\n"
     ]
    }
   ],
   "source": [
    "# [1] SHARED STATE INITIALIZATION\n",
    "LAST_GEN_STATE = {\n",
    "    \"audio_path\": None,\n",
    "    \"ref_speaker\": None,\n",
    "    \"text\": \"\",\n",
    "    \"emotion\": \"\",\n",
    "    \"mode\": \"\",\n",
    "    \"alpha\": 0.0,\n",
    "    \"speaker_name\": \"\",\n",
    "    \"timestamp\": \"\"\n",
    "}\n",
    "print(\"üîó Shared state initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e90e7",
   "metadata": {},
   "source": [
    "## 2. Helper Functions (Metric Logics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e53d7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading Whisper (tiny) for WER...\n",
      "‚úì Whisper loaded.\n",
      "‚úì Metric helpers ready.\n"
     ]
    }
   ],
   "source": [
    "# [2] METRIC HELPERS\n",
    "import torch\n",
    "import numpy as np\n",
    "import whisper\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import time\n",
    "\n",
    "# Attempt to load Whisper for WER (ignore if on CPU and it's too slow)\n",
    "try:\n",
    "    if 'whisper_model' not in globals():\n",
    "        print(\"‚è≥ Loading Whisper (tiny) for WER...\")\n",
    "        whisper_model = whisper.load_model(\"tiny\")\n",
    "        print(\"‚úì Whisper loaded.\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Whisper load failed/skipped. WER will be disabled.\")\n",
    "\n",
    "def get_cosine_sim(path1, path2):\n",
    "    \"\"\"Calculate Speaker Similarity\"\"\"\n",
    "    try:\n",
    "        model = resolve_xtts_internal_model(XTTS)\n",
    "        lat1 = model.get_conditioning_latents(str(path1), load_sr=SR_XTTS)[1]\n",
    "        lat2 = model.get_conditioning_latents(str(path2), load_sr=SR_XTTS)[1]\n",
    "        emb1 = torch.tensor(np.array(lat1)).flatten().float().unsqueeze(0)\n",
    "        emb2 = torch.tensor(np.array(lat2)).flatten().float().unsqueeze(0)\n",
    "        return cosine_similarity(emb1, emb2).item()\n",
    "    except Exception as e:\n",
    "        return 0.0\n",
    "\n",
    "def get_wer(ref_text, audio_path):\n",
    "    \"\"\"Calculate Word Error Rate (Approx)\"\"\"\n",
    "    if 'whisper_model' not in globals(): return -1.0\n",
    "    try:\n",
    "        res = whisper_model.transcribe(str(audio_path), language='hi')\n",
    "        hyp = res['text']\n",
    "        # Simple ratio metric (Length Diff) for speed\n",
    "        # If you have 'jiwer' installed, use jiwer.wer(ref, hyp)\n",
    "        return abs(len(hyp) - len(ref_text)) / len(ref_text)\n",
    "    except:\n",
    "        return -1.0\n",
    "print(\"‚úì Metric helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2140e4c5",
   "metadata": {},
   "source": [
    "## 3. Synthesis GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efb055ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448d96a49206483f9263aeb192c946c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Emotion:', options=(), value=None), Dropdown(description='‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [3] SYNTHESIS GUI\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Audio\n",
    "import traceback\n",
    "\n",
    "# --- Widgets ---\n",
    "emotion_dropdown = widgets.Dropdown(description='Emotion:')\n",
    "speaker_dropdown = widgets.Dropdown(description='Speaker:')\n",
    "mode_dropdown = widgets.Dropdown(options=['average','single'], value='average', description='Mode:')\n",
    "sample_dropdown = widgets.Dropdown(description='Sample:')\n",
    "alpha_input = widgets.FloatText(value=1.5, step=0.1, description='Alpha:')\n",
    "text_in = widgets.Text(value='‡§Ü‡§ú ‡§ï‡§æ ‡§¶‡§ø‡§® ‡§®‡§à ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§®‡§æ‡§ì‡§Ç ‡§∏‡•á ‡§≠‡§∞‡§æ ‡§π‡•Å‡§Ü ‡§π‡•à, ‡§ú‡•à‡§∏‡•á ‡§ï‡•ã‡§à ‡§∂‡§æ‡§®‡§¶‡§æ‡§∞ ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•ã! ‡§π‡§∞ ‡§™‡§≤ ‡§ú‡•Ä‡§µ‡§® ‡§ï‡•á ‡§∞‡§Ç‡§ó‡•ã‡§Ç ‡§î‡§∞ ‡§Ü‡§∂‡•ç‡§ö‡§∞‡•ç‡§Ø‡•ã‡§Ç ‡§ï‡§æ ‡§â‡§§‡•ç‡§∏‡§µ ‡§π‡•à‡•§', description='Text:') \n",
    "run_button = widgets.Button(description='Synthesize', button_style='success')\n",
    "run_output = widgets.Output()\n",
    "\n",
    "# --- Helpers ---\n",
    "def safe_get_samples(emotion_name):\n",
    "    try:\n",
    "        if not emotion_name: return []\n",
    "        emotion_dir = PROJECT_ROOT / 'data' / 'emotion_samples' / emotion_name\n",
    "        if not emotion_dir.exists(): return []\n",
    "        return sorted([d.name for d in emotion_dir.iterdir() if d.is_dir()])\n",
    "    except: return []\n",
    "\n",
    "# --- Layout ---\n",
    "ui_syn = widgets.VBox([\n",
    "    widgets.HBox([emotion_dropdown, mode_dropdown]),\n",
    "    widgets.HBox([speaker_dropdown, sample_dropdown]),\n",
    "    widgets.HBox([alpha_input, text_in]),\n",
    "    run_button,\n",
    "    run_output\n",
    "])\n",
    "display(ui_syn)\n",
    "\n",
    "# --- Init Data ---\n",
    "try:\n",
    "    eml = list_emotions()\n",
    "    emotion_dropdown.options = eml\n",
    "    if eml: emotion_dropdown.value = eml[0]\n",
    "    spl = list_speakers()\n",
    "    speaker_dropdown.options = spl\n",
    "    if spl: speaker_dropdown.value = spl[0]\n",
    "except: pass\n",
    "\n",
    "def on_emotion_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        new_samples = safe_get_samples(change['new'])\n",
    "        sample_dropdown.options = new_samples\n",
    "        if new_samples: sample_dropdown.value = new_samples[0]\n",
    "emotion_dropdown.observe(on_emotion_change)\n",
    "\n",
    "def on_run_clicked(b):\n",
    "    with run_output:\n",
    "        run_output.clear_output()\n",
    "        try:\n",
    "            emotion = emotion_dropdown.value\n",
    "            sample_name = sample_dropdown.value\n",
    "            speaker_name = speaker_dropdown.value\n",
    "            mode = mode_dropdown.value\n",
    "            alpha = float(alpha_input.value)\n",
    "            txt = text_in.value\n",
    "            \n",
    "            print(f\"Synthesizing... (Alpha={alpha}, Mode={mode})\")\n",
    "            \n",
    "            # 1. Compute Vector\n",
    "            sid = 1\n",
    "            all_samples = safe_get_samples(emotion)\n",
    "            if sample_name in all_samples: sid = all_samples.index(sample_name) + 1\n",
    "            \n",
    "            ed = compute_emotion_vector_xtts_multi(\n",
    "                PROJECT_ROOT/'data'/'emotion_samples'/emotion, \n",
    "                method='xtts_native', mode=mode, sample_id=sid,\n",
    "                save_avg_dir=PROJECT_ROOT/'data'/'outputs'/'emotion_vectors'/'average'\n",
    "            )\n",
    "            \n",
    "            # 2. Get Speaker\n",
    "            speaker_path = PROJECT_ROOT/'data'/'speakers'/speaker_name\n",
    "            speaker_clean = ensure_speaker_clean(speaker_path, sr=SR_XTTS)\n",
    "            \n",
    "            # 3. Synthesize\n",
    "            out_name = f'gen_{emotion}_{mode}_{sample_name}_a{alpha:.1f}.wav'\n",
    "            out_file = PROJECT_ROOT/'data'/'outputs'/'generated'/out_name\n",
    "            \n",
    "            out = apply_emotion_and_synthesize(\n",
    "                txt, speaker_clean, ed, alpha=alpha, out_path=out_file, language='hi'\n",
    "            )\n",
    "            display(Audio(str(out), rate=24000))\n",
    "            \n",
    "            # UPDATE STATE\n",
    "            LAST_GEN_STATE.update({\n",
    "                \"audio_path\": str(out),\n",
    "                \"ref_speaker\": str(speaker_clean),\n",
    "                \"text\": txt,\n",
    "                \"emotion\": emotion,\n",
    "                \"mode\": mode,\n",
    "                \"alpha\": alpha,\n",
    "                \"speaker_name\": speaker_name,\n",
    "                \"sample\": sample_name,\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })\n",
    "            print(\"‚úÖ Ready for Evaluation below.\")\n",
    "            \n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "\n",
    "run_button.on_click(on_run_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24318fef",
   "metadata": {},
   "source": [
    "## 4. Metric GUI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa0a1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a175902f76d64ce79718858bb121cf79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Evaluation Panel</h3>Waiting for Synthesis...'), Button(button_style='warning',‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [4] METRICS & LOGGING GUI\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Log File ---\n",
    "LOG_FILE = PROJECT_ROOT / \"data\" / \"outputs\" / \"comprehensive_metrics_log.csv\"\n",
    "\n",
    "# --- Widgets ---\n",
    "lbl_info = widgets.HTML(\"<h3>Evaluation Panel</h3>Waiting for Synthesis...\")\n",
    "btn_load = widgets.Button(description=\"1. Load Audio\", button_style='warning')\n",
    "btn_calc = widgets.Button(description=\"2. Calc Objective\", button_style='info', disabled=True)\n",
    "\n",
    "# Subjective Sliders\n",
    "slider_intensity = widgets.IntSlider(value=3, min=1, max=5, description='Emotion Strength:', style={'description_width': 'initial'})\n",
    "slider_natural = widgets.IntSlider(value=3, min=1, max=5, description='Naturalness:', style={'description_width': 'initial'})\n",
    "\n",
    "btn_save = widgets.Button(description=\"3. Save to Log\", button_style='success', disabled=True)\n",
    "out_met = widgets.Output()\n",
    "\n",
    "# Holding logic\n",
    "current_obj_metrics = {}\n",
    "\n",
    "def on_load_click(b):\n",
    "    with out_met:\n",
    "        out_met.clear_output()\n",
    "        if not LAST_GEN_STATE[\"audio_path\"]:\n",
    "            print(\"‚ö†Ô∏è Synthesis not run yet.\")\n",
    "            return\n",
    "        \n",
    "        fname = Path(LAST_GEN_STATE[\"audio_path\"]).name\n",
    "        lbl_info.value = f\"<h3>Evaluating: {fname}</h3>(Alpha: {LAST_GEN_STATE['alpha']} | Emotion: {LAST_GEN_STATE['emotion']})\"\n",
    "        btn_calc.disabled = False\n",
    "        print(f\"Loaded: {fname}\")\n",
    "\n",
    "def on_calc_click(b):\n",
    "    with out_met:\n",
    "        print(\"‚è≥ Calculating... (Sim + WER)\")\n",
    "        sim = get_cosine_sim(LAST_GEN_STATE[\"ref_speaker\"], LAST_GEN_STATE[\"audio_path\"])\n",
    "        wer = get_wer(LAST_GEN_STATE[\"text\"], LAST_GEN_STATE[\"audio_path\"])\n",
    "        \n",
    "        global current_obj_metrics\n",
    "        current_obj_metrics = {\"sim\": sim, \"wer\": wer}\n",
    "        \n",
    "        print(f\"Speaker Sim: {sim:.3f}\")\n",
    "        print(f\"WER Score:   {wer:.3f}\")\n",
    "        \n",
    "        # Enable save now that we have data\n",
    "        btn_save.disabled = False\n",
    "\n",
    "def on_save_click(b):\n",
    "    with out_met:\n",
    "        # Combine everything\n",
    "        entry = {\n",
    "            \"Timestamp\": LAST_GEN_STATE[\"timestamp\"],\n",
    "            \"Speaker\": LAST_GEN_STATE[\"speaker_name\"],\n",
    "            \"Emotion\": LAST_GEN_STATE[\"emotion\"],\n",
    "            \"Alpha\": LAST_GEN_STATE[\"alpha\"],\n",
    "            \"Mode\": LAST_GEN_STATE[\"mode\"],\n",
    "            \"Audio_File\": Path(LAST_GEN_STATE[\"audio_path\"]).name,\n",
    "            # Objective\n",
    "            \"SIM_Score\": round(current_obj_metrics.get(\"sim\", 0), 4),\n",
    "            \"WER_Score\": round(current_obj_metrics.get(\"wer\", 0), 4),\n",
    "            # Subjective\n",
    "            \"Subj_Intensity\": slider_intensity.value,\n",
    "            \"Subj_Naturalness\": slider_natural.value\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame([entry])\n",
    "        header = not LOG_FILE.exists()\n",
    "        df.to_csv(LOG_FILE, mode='a', header=header, index=False)\n",
    "        print(f\"‚úÖ Data Saved to {LOG_FILE.name}\")\n",
    "        btn_save.disabled = True\n",
    "\n",
    "btn_load.on_click(on_load_click)\n",
    "btn_calc.on_click(on_calc_click)\n",
    "btn_save.on_click(on_save_click)\n",
    "\n",
    "ui_met = widgets.VBox([\n",
    "    lbl_info,\n",
    "    btn_load,\n",
    "    btn_calc,\n",
    "    widgets.HTML(\"<b>Subjective Ratings (1-5):</b>\"),\n",
    "    slider_intensity,\n",
    "    slider_natural,\n",
    "    btn_save,\n",
    "    out_met\n",
    "])\n",
    "display(ui_met)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}